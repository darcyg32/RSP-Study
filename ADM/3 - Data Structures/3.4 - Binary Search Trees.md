#Chapter_3
- We have seen data structures that allow for fast search or for flexible update, but not fast search *and* flexible update.
	- Unsorted, doubly linked lists supported insertion and deletion in O(1) time, but search took linear time in the worst case.
	- Sorted arrays support binary search and logarithmic query times, but at the cost of linear-time update.
- Binary search requires that we have fast access to *two* elements - specifically the median elements above and below the given node.
	- To combine these ideas, we need a "linked list" with two pointers per node.
	- This is the basic idea behind Binary Search Trees.
- A *rooted binary tree* is recursively defined as either being:
	- 1. Empty, or
	- 2. Consisting of a node called the *root*, together with two rooted binary trees called the left and right subtrees, respectively.
- The order among "sibling" nodes matters in rooted trees. That is, left is different from right.
- Figure 3.3 gives the shapes of the five distinct binary trees that can be formed on three nodes:
![[Pasted image 20230924010408.png]]
- A binary *search* tree labels each node in a binary tree with a single key such that for any node labeled x, all nodes in the left subtree of x have keys < x, while all nodes in the right subtree of x have keys > x. (Allowing duplicate keys in a binary search tree is bad practice).
	- This search tree labeling scheme is very special.
	- For any binary tree on n nodes, and any set of n keys, there is *exactly one* labeling that makes it a binary search tree.
	- Allowable labelings for three-node binary search trees are given in Figure 3.3.
### 3.4.1 - Implementing Binary Search Trees
- Binary tree nodes have *left* and *right* pointer fields, an (optional) *parent* pointer, and a data field.
	- These relationships are shown in Figure 3.4:
![[Pasted image 20230924010717.png]]
- A type declaration for the tree structure is given below:
```C
typedef struct tree {
	item_type item;        // data item
	struct tree *parent;   // Pointer to parent
	struct tree *left;     // Pointer to left child
	struct tree *right;    // Pointer to right child
} tree;
```
- The basic operations supported by binary trees are searching, traversal, insertion, and deletion.
##### Searching in a Tree
- The binary search tree labeling uniquely identifies where each key is located.
- Start at the root.
	- Unless it contains the query key x, proceed either left or right depending upon whether x occurs before or after the root key.
- This algorithm works because both the left and right subtrees of a binary search tree are themselves binary search trees.
- This recursive structure yields the recursive search algorithm below:
```C
tree *search_tree(tree *l, item_type x) {
	if (l == NULL) {
		return(NULL);
	}

	if (l->item == x) {
		return(l);
	}

	if (x < l->item) {
		return(search_tree(l->left, x));
	} else {
		return(search_tree(l->right, x));
	}
}
```
- This search algorithm runs in O(h) time, where h is the height of the tree.
##### Finding Minimum and Maximum Elements in a Tree.
- By definition, the smallest key must reside in the left subtree of the root, since all keys in the left subtree have values less than that of the root.
	- Therefore, as shown in Figure 3.4 (center), the minimum element must be the left-most descendant of the root.
	- Similarly, the maximum element must be the right-most descendant of the root.
```C
tree *find_minimum(tree *t) {
	tree *min;    // Pointer to minimum

	if (t == NULL) {
		return(NULL);
	}

	min = t;
	while (min->left != NULL) {
		min = min->left;
	}
	return(min);
}
```
##### Traversal in a Tree
- Visiting all the nodes in a rooted binary tree proves to be an important component of many algorithms.
	- It is a special case of traversing all the nodes and edges in a graph, which will be the foundation of Chapter 7.
- A prime application of tree traversal is listing the labels of the tree nodes.
- Binary search trees make it easy to report the labels in *sorted* order.
	- By definition, all the keys smaller than the root must lie in the left subtree of the root, and all the keys bigger than the root in the right subtree.
	- Thus, visiting all the nodes recursively, in accords with such a policy, produces an *in-order* traversal of the search tree:
```C
void traverse_tree(tree *l) {
	if (l != NULL) {
		traverse_tree(l->left);
		process_item(l->item);
		traverse_tree(l->right);
	}
}
```
- Each item is processed only once during the course of traversal, so it runs in O(n) time, where n is the number of nodes in the tree.
- Different traversal orders come from changing the position of *process_item* relative to the traversals of the left and right subtrees.
	- Processing the item first yields a *pre-order* traversal.
	- Processing the item last gives a *post-order* traversal.
- These make relatively little sense with search trees, but prove useful when the rooted tree represents arithmetic or logical expressions.
##### Insertion in a Tree
- There is exactly one place to insert an item x into a binary search tree T so that we can be certain where to find it again.
- We must replace the NULL pointer found in T after an unsuccessful query for the key of x.
- This implementation uses recursion to combine the search and node insertion stages of key insertion.
	- The three arguments to *insert_tree* are:
		- A pointer l to the pointer linking the search subtree to the rest of the tree.
		- The key x to be inserted.
		- A *parent* pointer to the parent node containing l.
	- The node is allocated and linked in after hitting the NULL pointer.
	- Note that we pass the *pointer* to the appropriate left/right pointer in the node during the search, so the assignment `*l = p;` links the new node into the tree:
```C
void insert_tree(tree **l, item_type x, tree *parent) {
	tree *p;    // temp pointer

	if (*l == NULL) {
		p = malloc(sizeof(tree));
		p->item = x;
		p->left = p->right = NULL;
		p->parent = parent;
		*l = p;
		return;
	}

	if (x < (*l)->item) {
		insert_tree(&((*l)->left), x, *l);
	} else {
		insert_tree(&((*l)->right), x, *l);
	}
}
```
- Allocating the node and linking it into the tree is a constant-time operation, after the search has been performed in O(h) time, where again h denotes the height of the search tree.
##### Deletion from a Tree
- Deletion is somewhat tricker than insertion, because removing a node means appropriately linking its two descendant subtrees back into the tree somewhere else.
- There are three cases, illustrated in Figure 3.5:
![[Pasted image 20230924012330.png]]
- Leaf nodes have no children, and so may be deleted simply by clearing the pointer to the given node.
- The case of a doomed node having one child is also straightforward.
	- We can link this child to the deleted node's parent without violating the in-order labeling property of the tree.
- But what about a node with two children?
	- Our solution is to relabel this node with the key of its immediate successor in sorted order.
	- This successor must be the smallest value in the right subtree, specifically the left-most descendant in the right subtree p.
	- Moving this descendant to the point of deletion results in a properly labeled binary search tree, and reduces our deletion problem to physically removing a node with at most one child (which we just covered above).
- The full implementation looks a little ghastly, but simply follows the above description.
- The worst-case complexity analysis is as follows:
	- Every deletion requires the cost of at most two search operations, each taking O(h) time (where h is the height of the tree), plus a constant amount of pointer manipulation.
	- Therefore it has a worst-case complexity of O(h).

### 3.4.2 - How Good are Binary Search Trees?
- When implemented using binary search trees, all three [[3.3 - Dictionaries|dictionary]] operations take O(h) time, where h is the height of the tree.
	- The smallest height we can hope for occurs when the tree is perfectly balanced, meaning that $h = [\log n]$.
	- This is very good, but the tree must be perfectly balanced.
- Our insertion algorithm puts each new item at a leaf node where it should have been found.
	- This makes the shape (and more importantly the height) of the tree determined by the order in which we insert the keys.
- Unfortunately, bad things can happen when building trees through insertion.
	- The data structure has no control over the order of insertion.
- Consider what happens if the user inserts the keys in sorted order.
	- The operations insert(a), insert(b), ..., would result in a skinny, linear height tree where only right pointers are used (basically a linked list).
- Thus, binary trees can have heights ranging from lg n to n.
	- But how tall are they on average?
	- The average case analysis of algorithms can be tricky because we must carefully specify what we mean by *average*.
		- The question is well defined if we assume each of the *n!* possible insertion orderings to be equally likely, and average over those.
			- If this assumption is valid, then we are in luck, because with high probability the resulting tree will have $\Theta (\log n)$ height.
			- This will be shown in Section 4.6.
- This argument is an important example of the power of *randomization*.
	- We can often develop simple algorithms that offer good performance with high probability.
	- We will see that a similar idea underlies the fastest known sorting algorithm, quicksort.

### 3.4.3 - Balanced Search Trees
- Random search trees are *usually* good.
	- But if we get unlucky with our order of insertion, we can end up with a linear-height tree in the worst case.
	- This worst case is outside of our direct control, since we must build the tree in response to the requests given by our potentially nasty user.
- What would be better is an insertion/deletion procedure that *adjusts* the tree a little after each insertion, keeping it close enough to be balanced that the maximum height is logarithmic.
- Sophisticated *balanced* binary search tree data structures have been developed that guarantee the height of the tree always to be O(log n).
	- Therefore, all dictionary operations (insert, delete, query) take O(log n) time each.
	- Implementations of balanced tree data structures such as red-black trees and splay trees are discussed in Section 15.1.
- From an algorithmic design viewpoint, it is important to know that these trees exist, and that they can be used as black boxes to provide an efficient dictionary implementation.
	- When figuring the costs of dictionary operations for algorithm analysis, we can assume the worst-case complexities of balanced binary trees to be a fair measure.
- ***Take-Home Lesson:***
	- **Picking the wrong data structure for the job can be disastrous in terms of performance.**
	- **Identifying the very best data structure is usually not as critical, because there can be several choices that perform in a similar manner.**
##### Stock and Think: Exploiting Balanced Search Trees
- *Problem*:
	- You are given the task of reading n numbers and then printing them out in sorted order.
	- Suppose you have access to a balanced dictionary data structure, which supports the operations search, insert, delete, minimum, maximum, successor, and predecessor each in O(log n) time.
	- How can you sort in O(n log n) time using only:
		1. Insert and in-order traversal?
		2. Minimum, successor, and insert?
		3. Minimum, insert, and delete?
- *Solution*:
	- Every algorithm for sorting items using a binary search tree *has* to start by building the actual tree.
		- This involves initializing the tree (basically setting the pointer t to NULL), and then reading/inserting each of the n items into t.
		- This costs O(n log n), since each insertion takes at most O(log n) time.
		- Curiously, just building the data structure is a rate-limiting step for each of out sorting algorithms!
	- The first problem allows us to do insertion and in-order traversal.
		- We can build a search tree by inserting all n elements, then do a traversal to access the items in sorted order.
	- The second problem allows us to us the minimum and successor operations after constructing the tree.
		- We can start from the minimum element, then repeatedly find the successor to traverse the elements in sorted order.
	- The third problem does not give us successor, but now allow us delete.
		- We can repeatedly find the delete the minimum element to once again traverse all the elements in sorted order.
	- In summary, the solutions to the three problems are:
![[Pasted image 20230924014314.png]]
- Each of these algorithms does a linear number of logarithmic-time operations, and hence runs in O(n log n) time.
- The key to exploiting balanced binary search trees is using them as black boxes.