#Chapter_3
- Geometric models used in computer graphics are commonly represented by a triangulated surface, as shown in Figure 3.6 (1).
![[Pasted image 20230926002430.png]]
- High performance rendering engines have special hardware for rendering and shading triangles.
	- This rendering hardware is so fast that the computational bottleneck becomes the cost of feeding the triangulation structure into the hardware engine.
- Although each triangle can be described by specifying its three endpoints, an alternative representation proves more efficient.
	- Instead of specifying each triangle in isolation, suppose that we partition the triangles into *strips* of adjacency triangles, and walk along the strip.
		- Since each triangle shares two vertices in common with its neighbors, we save the cost of retransmitting the two extra vertices and any associated information.
		- To make the description of the triangles unambiguous, the *OpenGL* triangular-mesh renderer assumes that all turns alternate left and right.
		- The strip in Figure 3.7 (left) completely describes fives triangles of the mesh with the vertex sequence $[1,2,3,4,5,7,6]$, and the implied left/right order.
		- The strip on the right describes all seven triangles with specified turns: $[1,2,3,l-4,r-5,l-7,r-6,r-1,r-3]$.
![[Pasted image 20230926002849.png]]
- The task was to find a small number of strips that together cover all the triangle in a mesh, without overlap.
	- This can be thought of as a graph problem.
		- The graph of interest has a vertex for every *triangle* of the mesh, and an edge between every pair of vertices representing adjacent triangles.
		- This *dual graph* representation captures all the information about triangulation needed to partition it into strips.
- Once we had the dual graph, the project could begin.
	- We sought to partition the dual graph's vertices into as few paths or strips as possible.
	- Partitioning it into one path implied that we had discovered a Hamiltonian path, which by definition visits each vertex exactly once.
		- Since finding a Hamiltonian path is NP-complete, we knew not to look for an optimal algorithm, but concentrate instead on heuristics.
- The simplest approach for a strip cover would start from an arbitrary triangle and then do a left-right walk until the walk ends, either by hitting the boundary of the object or a previously visited triangle.
	- This heuristic had the advantage that it would be fast and simple, although there is no reason to expect that is must find the smallest possible set of left-right strips for a given triangulation.
- The *greedy* heuristic should result in a relatively small number of strips, however.
	- Greedy heuristics always try to grab the best possible thing first.
	- In the case of the triangulation, the natural greedy heuristic would be to identify the starting triangle that yields the longest left-right strip, and peel that one off first.
- However, being greedy doesn't guarantee you the best possible solution overall, since the first strip you peel off might break apart a lot of potential strips we would have wanted to use later.
	- Still, being greedy is a good rule of thumb if you want to get rich (aka it's usually better than nothing).
	- Removing the longest strip leaves the fewest number of triangles remaining for later strips, so greedy should outperform the naive heuristic of pick anything.
- But how much time does it take to find the largest strip to peel off next?
	- Let k be the length of the walk possible from an average vertex.
	- Using the simplest possible implementation, we could walk from each of the n vertices to find the largest remaining strip to report in O(kn) time.
		- Repeating this for each of the roughly n/k strips we extract yields an $O(n^2)$-time implementation, which is way too slow.
- How could we speed this up?
	- It seems wasteful to re-walk from each triangle after deleting a single strip.
	- We could maintain the lengths of all the possible future strips in a data structure.
	- However, whenever we peel off a strip, we must update the lengths of all affected strips.
	- These strips will be shorted because they walked through a triangle that now no longer exists.
	- There are two aspects of such a data structure:
		- *[[3.5 - Priority Queues|Priority Queue]]*:
			- Since we were repeatedly identifying the longest remaining strip, we needed a priority queue to store the strips ordered according to length.
			- The next strip to peel always sits at the top of the queue.
			- Out priority queue had to permit reducing the priority of arbitrary elements of the queue whenever we updated the strip lengths to reflect what triangles were peeled away.
			- Because all of the strip lengths were bounded by a fairly small integer (hardware constraints prevent any strip from having > 256 vertices), we used a bounded-height priority queue (an array of buckets, shown in Figure 3.8, described in Section 15.2).
			- ![[Pasted image 20230926004047.png]]
				- An ordinary heap would have also worked.
			- To update the queue entry associated with each triangle, we needed to quickly find where it was.
			- This meant that we also needed a ...
		- **[[3.3 - Dictionaries|Dictionary]]*:
			- For each triangle in the mesh, we had to find where it was in the queue.
			- This meant storing a pointer to each triangle in a dictionary.
			- By integrating this dictionary with the priority queue, we built a data structure capable of a wide range of operations.
- Although there were various other complications, such as quickly recalculating the length of the strips affected by the peeling, the key idea needed to obtain better performance was to use the priority queue.
	- Run time improved by several orders of magnitude after employing this data structure.
- How much better did the greedy heuristic do than the naive heuristic?
	- Study the table in Figure 3.9:
	- ![[Pasted image 20230926004513.png]]
	- In all cases, the greedy heuristic led to a set of strips that cost less, as measured by the total number of vertex occurrences in the strips.
		- This saving ranged from about 10% - 50%, which is quite remarkable since the greatest possible improvement (going from three vertices per triangle down to one) yields a savings of only 66.6%.
- After implementing the greedy heuristic with our priority queue data structure, the program ran in $O(n*k)$ time, where n is the number of triangles and k is the length of the average strip.
	- Thus. the torus, which consists of a small number of very long strips, took longer than the jaw, even though the latter contained over 3 times as many triangles.
- There are several lessons to be gleaned from this story.
	- First, when working with a large enough data set, only linear or near-linear algorithms are likely to be fast enough.
	- Second, choosing the right data structure is often the key to getting the time complexity down.
	- Finally, using the greedy heuristic can significantly improve performance over the naive approach.
		- How much this improvement will be can only be determined by experimentation.