#Chapter_3
- Hash tables are a very practical way to maintain a [[3.3 - Dictionaries|dictionary]].
	- They exploit the fact that looking an item up in an array takes constant time once you have its index.
- A hash function is a mathematical function that maps keys to integers.
	- We will use the value of our hash function as an index into an array, and store our item at that position.

- The first step of the has function is usually to map each key (here the string *S*) to a big integer.
	- Let $\alpha$ be the size of the alphabet on which *S* is written.
	- Let *char(c)* be a function that maps each symbol of the alphabet to a unique integer from 0 to $\alpha - 1$.
- The function:
$H(S)=\alpha^{|S|} + \sum^{|S|-1}_{i=0} \alpha^{|S|-(i+1)} * char(s_i)$
- maps each string to a unique (but large) integer by treating the characters of the string as "digits" in a base-$\alpha$ number system.

- The creates unique identifier numbers, but they are so large that they will quickly exceed the number of desired slots in our has table (denoted by *m*).
	- We must reduce this number to an integer between 0 and $m-1$, by taking the remainder $H'(S)=H(S)$ mod $m$.
	- This works on the same principle as a roulette wheel. The ball travels a long distance, around and around the circumference-*m* wheel \[*H(S)/m*] times before settling down to a random bin.
	- If the table size is selected with enough finesse (ideally *m* is a large prime not too close to $2^i-1$), the resulting hash values should be fairly uniformly distributed.

### 3.7.1 - Collision Resolution
- No matter how good our has function is, we had better be prepared for collisions, because two distinct keys will at least occasionally hash to the same value.

- There are two different approaches for maintaining a hash table:
	- *Chaining*: ![[Pasted image 20231113213410.png]]
		- Represents a hash table as an array of *m* linked lists ("buckets"), as shown in Figure 3.10.
			- The *i*th list will contain all the items that hash to the value of *i*.
			- Search, insertion, and deletion thus reduce to the corresponding problem in linked lists.
			- If the *n* keys are distributed uniformly in a table, each list will contain roughly *n/m* elements, making them a constant size when $m\approx n$.
		- Chaining is very natural, but devotes a considerable amount of memory to pointers.
			- This is space that could be used to make the table larger, which reduces the likelihood of collisions.
			- In fact, the highest-performing hash tables generally rely on open addressing.
		

	- *Open Addressing*: ![[Pasted image 20231113214045.png]]
		- Maintains the hash table as a simple array of elements (not buckets).
			- Each cell is initialized to null, as shown in Figure 3.11.
			- On each insertion, we check to see whether the desired cell is empty; 
				- If so, we insert the item there.
				- But if the cell is already occupied, we must find some other place to put the item.
					- The simplest possibility (called *sequential probing*) inserts the item into the next open cell in the table.
			- Provided the table is not too full, the contiguous runs of non-empty cells should be fairly short, hence this location *should* be only a few cells away from its intended position.
		- Searching for a given key now involves going to the appropriate hash value and checking to see if the item there is the one we want.
			- If so, return it. Otherwise we must keep checking through the length of the run.
		- Deletion in an open addressing scheme can get ugly, since removing one element might break a chain of insertions, making some elements inaccessible.
			- We have no alternative but to reinsert all the items in the run that follows the new hole.

- Chaining and open addressing both cost $O(m)$ to initialize an *m*-element hash table to null elements prior to the first insertion.

- When using chaining with doubly linked lists to resolve collisions in an *m*-element hash table, the dictionary operations for *n* items can be implemented in the following expected and worst-case times:
![[Pasted image 20231113214218.png]]
- Traversing all the elements in the table takes $O(n+m)$ time for chaining, since we have to scan all *m* buckets looking for elements, even if the actual number of inserted items is small.
	- This reduces to $O(m)$ time for open addressing, since n must be at most *m*.

- Pragmatically, a hash table often is the best data structure to maintain a dictionary.
- The applications of hashing go far beyond dictionaries, however, as we will see below.

### 3.7.2 - Duplicate Detection via Hashing
- The key idea of hashing is to represent a large object (e.g. a key, string, substring, etc) by a single number.
	- We get a representation of the large object by a value that can be manipulated in constant time, such that is is relatively unlikely that two different large objects map to the same value.
- Hashing has a variety of clever applications beyond just speeding up search.

- Consider the following problems with nice hashing solutions:
	- *Is a given document unique within a large corpus?*
		- Explicitly comparing the new document *D* against all *n* previous documents is hopelessly inefficient for a large corpus.
		- However, we can hash *D* to an integer, and compare *H(D)* to the hash codes of the rest of the corpus.
		- Only if there is a collision *might* *D* be a possible duplicate.
		- Since we expect few spurious collisions, we can explicitly compare the few documents sharing a particular hash code with little total effort.
	- *Is part of this document plagiarized?*
		- This is a more difficult problem that the previous application.
			- Adding, deleting, or changing even one character from a document will completely change its hash code.
			- The hash codes produced in the previous application thus cannot help for this more general problem.
		- However, we *could* build a hash table of all overlapping windows (substrings) of length *w* in all the documents in the corpus.
			- Whenever there is a match of hash codes, there is likely a common substring of length *w* between the two documents, which can then be further investigated.
			- We should choose *w* to be long enough so such a co-occurrence is very unlikely to happen by chance.
		- The biggest downside to this scheme is that the size of the hash table becomes as large as the document corpus itself.
			- Retaining a small but well-chosen subset of these hash codes is exactly the goal of min-wise hashing, discussed in Section 6.6.
	- *How can I convince you that a file isn't changed?*
		- In a closed-big auction, each part submits their bid in secret before the announced deadline.
			- If you knew what the other parties were bidding, you could bid $1 more than the highest opponent and walk off with the prize as cheaply as possible.
			- The "right" strategy would thus be to hack into the computer containing the bids just prior to the deadline, read the bids, and then magically emerge as the winner.
		- How can this be prevented?
			- What is everyone submits a hash code of their actual bid prior to the deadline, and then submits the full bid only after the deadline?
			- The auctioneer will pick the largest full bid, but checks to make sure the hash code matches what was submitted prior to the deadline.
		- Such *cryptographic hashing* methods provide a way to ensure that the file you give me today is the same as the original, because any change to the file will change the hash code.

- Although the worst-case bounds on anything involving hashing are dismal, with a proper hash function we can confidently expect good behaviour.
- Hashing is a fundamental ideal in randomized algorithms, yielding linear expected-time algorithms for problems that are $\Theta (n\log n)$ or $\Theta (n^2)$ in the worst case.

### 3.7.3 - Other Hashing Tricks
- Hash functions provide useful tools for many things beyond powering hash tables.
- The fundamental idea is of many-to-one mappings, where *many* is controlled so it is very unlikely to be *too many*.

### 3.7.4 - Canonicalization
- Consider a word game that gives you a set of letters *S*, and asks you to find all dictionary words that can be made by reordering them.
	- For example, I can make three words from the four letters in $S=(a,e,k,l)$, namely *kale*, *lake*, and *leak*.

- Think how you might write a program to find the matching words for *S*, given a dictionary *D* of *n* words.
	- Perhaps the most straightforward approach is to test each word $d \in D$ against the characters of *S*.
		- This takes time linear in *n* for each *S*, and the test is somewhat tricky to program.

- What if we instead hash every word in *D* to a string, by sorting the word's letters.
	- Now *kale* goes to *aekl*, as do *lake* and *leak*.
- By building a hash table with the sorted strings as keys, all words with the same letter distribution get hashed to the same bucket.
	- Once you have built this hash table, you can use it for different query sets *S*.
	- The time for each query will be proportional to the number of matching words in *D*, which is a lot smaller than *n*.

- Which set of *k* letters can be used to make the most dictionary words?
	- This seems like a much harder problem, because there are $\alpha^k$ possible letter sets, where $\alpha$ is the size of the alphabet.
	- But observe that the answer is simply the hash code with the largest number of collisions.
	- Sweeping over a sorted array of hash codes (or walking through each bucket in a chained hash table) makes this fast and easy.

- This is a good example of the power of *canonicalization*, reducing complicated objects to a standard (i.e. "canonical") form.
	- String transformations like reducing letters to lower case or stemming (removing suffixes like -ed, -s, or -ing) result in increased matches, because multiple strings collide on the same code.
	- *Soundex* is a canonicalization scheme for names, so spelling variants of "Skiena" like "Skina", "Skinnia" and "Schiena" all get hashed to the same Soundex code *S25*.

- For hash tables, collisions are very bad. But for pattern matching problems like these, collisions are exactly what we want.

### 3.7.5 Compaction
- Suppose you wanted to sort all *n* books in the library, not by their titles but by the contents of the actual text.
	- Bulwer-Lytton's \[BL30] "It was a dark and stormy night..." would appear before this book's "What is an algorithm?..."
- Assuming the average book is $m \approx 100,000$ words long, doing this sort seems an expensive and clumsy job, since each comparison involves two books.

- But suppose we instead represent each book by the first, say, 100 characters, and sort these strings.
	- There will be collisions involving duplicates of the same prefix, involving multiple editions or perhaps plagiarism, but these will be quite rare.
	- After sorting the prefixes, we can then then resolve the collisions by comparing the full texts.
	- The world's fastest sorting programs use this idea.

- This is an example of hashing for compaction, also called *fingerprinting*, where we represent large objects by small hash codes.
	- It is easier to work with small objects than large ones, and the hash code generally preserves the identity of each item.
- The hash function here is trivial (just take the prefix) but it is designed to accomplish a specific goal - not to maintain a hash table.
- More sophisticated hash functions can make the probability of collisions between even slightly different objects vanishingly low.