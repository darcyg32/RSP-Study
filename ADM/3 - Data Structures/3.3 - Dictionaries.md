#Chapter_3
- The *dictionary* data type permits access to data items by content.
- You stick an item into a dictionary so you can find it when you need it.
- The primary operations dictionaries support are:
	- *Search(D, k)*:
		- Given a search key k, return a pointer to the element in dictionary D whose key value is k, if one exists.
	- *Insert(D, x)*:
		- Given a data item x, add it to the dictionary D.
	- *Delete(D, x)*:
		- Given a pointer x to a given data item in the dictionary D, remove it from D.
- Certain dictionary data structures also efficiently support other useful operations:
	- *Max(D)* or *Min(D)*:
		- Retrieve the item with the largest (or smallest) key from D.
		- This enables the dictionary to also serve as a priority queue, as will be discussed in Section 3.5.
	- *Predecessor(D, x)* or *Successor(D, x)*:
		- Retrieve the item from D whose key is immediately before (or after) item x in *sorted* order.
		- These enable us to iterate through the elements of the data structure in sorted order.
- Many common data processing tasks can be handled using these dictionary operations.
	- For example, suppose we want to remove all duplicate names from a mailing list, and print the results in sorted order.
		- Initialize an empty dictionary D, whose search key will be the record name.
		- Now, read through the mailing list, and for each record *search* to see if the name is already in D.
			- If not, *insert* it into D.
		- After reading through the mailing list, we print the names in the dictionary.
		- By starting from the first item Min(D) and repeatedly calling *Successor* until we obtain Max(D), we traverse all elements in sorted order.
- By defining such problems in terms of abstract dictionary operations, we can ignore the details of the data structure's representation, and focus on the task at hand.
- In the rest of this section, we'll investigate simple dictionary implementations based on arrays and linked lists.
	- More powerful dictionary implementations, such as Binary Search Trees and Hash Tables, are also attractive options in practice.
### Comparing Dictionary Implementations (I)
- Problem:
	- What are the asymptotic worst-case running times for all 7 fundamental dictionary operations (search, insert, delete, successor, predecessor, minimum, and maximum) when the data structure is implemented as:
		- An unsorted array?
		- A sorted array?
- Solution:
	- This problem reveals some of the inherent trade-offs of data structure design.
		- A given data representation may permit efficient implementation of certain operations at the cost of other operations' expense.
	- In addition to the array in question, we'll assume access to a few extra variables such as n, the number of elements currently in the array.
		- Note that we must *maintain* the value of these variables in the operations where they change (e.g. insert and delete), and charge these operations the cost of this maintenance.
	- The basic dictionary operations can be implemented with the following costs on unsorted and sorted arrays. The starred element indicates "cleverness".
![[Pasted image 20230922111507.png]]
- We must understand the implementation of each operation to see the reasoning between each answer.
- First, let's discuss the operations when maintaining an ***unsorted* array** A.
	- Search:
		- Implemented by testing the search key k against (in the worst case) each element of an unsorted array.
		- Thus, search takes linear time in the worst case, which is when key k is not found in A.
	- Insertion:
		- Implemented by incrementing n and then copying item x to the nth cell in the array, A\[n].
		- The bulk of the array is untouched, so this operation takes constant time.
	- Deletion:
		- Is somewhat trickier, hence the asterisk above.
		- The definition states that we are given a pointer x to the element to delete, so we need not spend any time searching for the element.
		- However, removing the x-th element from the array A leaves a hole that must be filled.
		- We could fill this hold by moving each of the elements from A\[x+1] to A\[n] down one position.
			- But this requires $\Theta (n)$ time when the first element is deleted. 
		- Instead, just overwrite A\[x] with A\[n], and decrement n.
			- This only takes constant time.
	- Predecessor and Successor:
		- The definitions of these operations refer to the item appearing before/after x *in sorted order*.
		- Thus, the answer is not simply A\[x-1] (or A\[x+1]), because in an unsorted array an element's *physical* predecessor (or successor) is not necessarily the *logical* predecessor (or successor).
			- Instead, the predecessor of A\[x] is the biggest element smaller than A\[x].
			- Similarly, the successor of A\[x] is the smallest element larger than A\[x].
			- Both require a sweep through all n elements of A to determine the winner, hence the linear time.
	- Minimum and Maximum:
		- These operations are similarly defined with respect to sorted order.
		- Therefore they also require linear-cost sweeps to identify in an unsorted array.
		- You might think we could set aside extra variables containing the current minimum and maximum values, so we can report them in O(1) time.
			- However, this is incompatible with constant-time deletion, as deleting the minimum or maximum valued item mandates a linear-time search to find the new minimum or maximum.
- Now, implementing a dictionary using a ***sorted* array** completely reverses our notions of what is easy and what is hard.
	- Searches can now be done in O(log n) time, using binary search, because we know the median element sits in A\[n/2].
		- Since the upper and lower portions of the array are also sorted, the search can continue recursively on the appropriate portion.
		- The number of halvings of n until we get to a single element is \[lg n].
	- The minimum and maximum elements now sit in A\[1] and A\[n], respectively.
	- The predecessor and successor to A\[x] are now A\[x-1] and A\[x+1], respectively.
	- However, insertion and deletion become more expensive.
		- Making room for a new item or filling a hole may require moving many items arbitrarily.
		- Thus, both become linear-time operations.
- **Take-Home Lesson:**
	- **Data structure design must balance all the different operations it supports.**
	- **The fastest data structure to support *both* operations A and B may well not be the fastest structure to support just operation A *or* B**.

### Comparing Dictionary Implementations (II)
- Problem:
	- What are the asymptotic worst-case running times for each of the seven fundamental dictionary operations when the data structure is implemented as:
		- A singly linked unsorted list.
		- A doubly linked unsorted list.
		- A singly linked sorted list.
		- A doubly linked sorted list.
- Solution:
	- Two different issues must be considered in evaluating these implementations:
		- Singly vs doubly linked lists, and sorted vs. unsorted order.
	- Operations with subtle implementations are denoted with an asterisk:
![[Pasted image 20230922230902.png]]
- As with unsorted arrays, search operations are destined to be slow, while maintenance operations are fast.
	- Insertion/Deletion:
		- The complication here is deletion from a singly linked list.
		- The definition of the Delete operation states we are given a pointer x to the item to be deleted.
			- But what we *really* need is a pointer to the element pointing to x in the list, because that is the node that needs to be changed.
		- We can do nothing without this list predecessor, and so must spend linear time searching for it on a singly linked list.
		- Doubly linked lists avoid this problem, since we can immediately retrieve the list predecessor of x.
		- Deletion is faster for sorted doubly linked lists than sorted arrays, because splicing out the deleted element from the list is more efficient than filling the hole by moving array elements.
		- The predecessor pointer problem again complicates deletion from singly linked sorted lists.
	- Search:
		- Sorting provides less benefit for linked lists than it did for arrays.
			- Binary search is no longer possible, as we can't access the median element without traversing all the elements before it.
		- What sorted lists *do* provide is quick termination of unsuccessful searches, for if we have not found n+1 by the time we hit n, we can deduce that n doesn't doesn't exist in the list (if in ascending order).
		- Still, searching takes linear time in the worst case.
	- Traversal Operations:
		- The predecessor pointer problem again complicates implementing *Predecessor* with singly linked lists.
		- The logical successor is equivalent to the node successor for both types of sorted lists, and hence can be implemented in constant time.
	- Minimum/Maximum:
		- The minimum element sits at the head of a sorted list, and so it easily retrieved.
		- The maximum element is at the tail of the list, which usually requires $\Theta (n)$ time to reach in either singly or doubly linked lists.
		- However, we can maintain a separate pointer to the list tail, provided we pay the maintenance costs for this pointer on every insertion and deletion.
			- The tail pointer can be updated in constant time on doubly linked lists:
				- On insertion, check whether last-> next still == NULL.
				- On deletion, set last to point to the list predecessor of last if the last element is deleted.
		- We have no efficient way to find this predecessor for singly linked lists.
			- However, the trick is to charge the cost to each deletion, which *already* takes linear time.
			- Adding an extra linear sweep to update the pointer does not harm the asymptotic complexity of Delete, while gaining us Maximum (and similarly Minimum) in constant time.

Also see [[3.1 - Contiguous vs. Linked Data Structures|3.1 - Contiguous vs. Linked Data Structures (Arrays vs Linked Lists)]].