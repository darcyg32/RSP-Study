#Chapter_3
- Data structures can be neatly classified as either *contiguous* or *linked*, depending on whether they are based on arrays or pointers.
- *Contiguously allocated structures*:
	- Are composed of single slabs of memory.
	- Include:
		- Arrays.
		- Matrices.
		- Heaps.
		- Hash tables.
- *Linked data structures*:
	- Are composed of distinct chunks of memory, bound together by *pointers*.
	- Include:
		- Lists.
		- Trees.
		- Graph adjacency lists.

### 3.1.1 - Arrays 
- The fundamental contiguously allocated data structure.
- They are structures of fixed-size data records such that each element can be efficiently located by its *index* or (equivalently) address.
- A good analogy likens an array to a street full of houses:
	- Each array element is equivalent to a house.
	- The index is equivalent to the house number.
	- Assuming all the houses are of equal size and numbered sequentially from 1 to *n*, we can compute the exact position of each house immediately from its address.
- Advantages of contiguously allocated arrays include:
	- *Constant-time access given the index*
		- As the index of each element maps directly to a particular memory address, we can access arbitrary data items instantly, provided we know the index.
	- *Space efficiency*
		- Arrays consist purely of data, so no space is wasted with links or other formatting information.
		- Further, end-of-record information is not needed, as arrays are built from fixed-size records.
	- *Memory locality*
		- Many programming tasks require iterating through all the elements of a data structure.
		- Arrays are good for this because they exhibit excellent memory locality.
		- Physical continuity between successive data accesses helps exploit the high-speed *cache memory* on modern computer architectures.
- The downside of arrays is that we cannot adjust their size in the middle of a program's execution.
	- Out program will fail as soon as we try to add the (*n*+1)-st customer, if we only allocated room for *n* records.
	- We can compensate by allocating extremely large arrays, but this can waste space, again restricting what our programs can do.
- However, we can efficiently enlarge arrays as needed, via *dynamic arrays*.
	- Suppose we start with an array of size 1, and double its size from *m* to *2m* whenever we run out of space.
		- This doubling process allocates a new contiguous array of size *2m*, copies the contents of the old array to the lower half of the new one, and then returns the space used by the old array to the storage allocation system.
	- The apparent waste in this procedure involves recopying the old contents on each expansion.
		- How much work do we really do?
			- It will take $\log_2 n$ (aka $\lg n$) doublings until the array gets to have *n* positions, plus one final doubling on the last insertion when $n=2^j$ for some *j*.
			- There are recopying operations after the first, second, fourth, eighth, ..., *n*th insertions.
			- The number of copy operations at the *i*th doubling will be $2^{i-1}$, so the total number of movements M will be:
			- $M=n+\sum^{\lg n}_{i=1}2^{i-1} = 1+2+4+...+\frac{n}{2}+n=\sum^{\lg n}_{i=1} \frac{n}{2^i} \leq n \sum^{\infty}_{i=0} \frac{1}{2^i} = 2n$
			- Thus, each of the *n* elements move only two times on average, and the total work of managing the dynamic array is the same *O(n)* as it would have been if a single array of sufficient size had been allocated in advance!
- The primary thing lost in using dynamic arrays however is the guarantee that each insertion takes constant time *in the worst case*.
	- Note that all accesses and *most* insertions will be fast, except for those relatively few insertions that trigger array doubling.
	- What we get instead is a promise that the *n*th element insertion will be completed quickly enough that the *total* effort expended so far will still be *O(n)*.
	- Such *amortized* guarantees arise frequently in the analysis of data structures.

### 3.1.2 - Pointers and Linked Structures 
- *Pointers*:
	- Are the connections that hold the pieces of linked structures together.
	- They represent the address of a location in memory.
	- A variable storing a pointer to a given data item can provide more freedom than storing a copy of the item itself.
	- For example, a phone number can be thought of as a pointer to its owner.
- Pointer syntax and power differ significantly across programming languages, so we begin with a quick review of pointers in C language.
	- A pointer p is assumed to give the address in memory where a particular chunk of data is located.
	- Pointers in C have types declared at compile time, denoting the data type of the items they can point to.
	- We use \*p to denote the item that is pointed to by a pointer p, and &x to denote the address of (i.e. pointer to) a particular variable x.
	- A special NULL pointer value is used to denote structure-terminating or unassigned pointers.
- All linked data structures share certain properties, as revealed by the following type declaration for linked lists (also described in Figure 3.1):
```C
typedef struct list {
	item_type item;    // data item
	struct list *next; // point to successor
} list;
```
![[Pasted image 20230921000918.png]]
- In particular:
	- Each node in our data structure (here list) contains one or more data fields (here item) that retain the data that we need to store.
	- Each node contains a pointer field to at least one other node (here next).
		- This means that much of the space used in linked data structures is devoted to pointers, not data.
	- Finally, we need a pointer to the head of the structure, so we know where to access it.
- The list here is the simplest linked structure.
	- The three basic operations supported by lists are searching, insertion, and deletion.
	- In *doubly linked lists*, each node points both to its predecessor and its successor element.
		- This simplifies certain operations at a cost of an extra pointer field per node.
###### Searching a List
- Searching for item x in a linked list can be done iteratively or recursively.
- The below implementation shows the recursive approach.
	- If x is in the list, it is either the first element of located in the rest of the list.
	- Eventually, the problem is reduced to searching in an empty list, which clearly cannot contain x.
```C
list *search_list(list *l, item_type x) {
	if (l == NULL) {
		return(NULL);
	}

	if (l->item == x) {
		return(l);
	} else {
		return(search_list(l->next, x));
	}
}
```
###### Insertion into a List
- Insertion into a linked list is a nice exercise in pointer manipulation, as shown below.
- Since we have no need to maintain the list in any order, we might as well insert each new item in the most convenient place.
- Insertion at the beginning of the list avoids any need to traverse the list, but does require use to update the pointer (denoted l) to the head of the data structure.
```C
void insert_list(list **l, item_type x) {
	list *p;    // temp pointer

	p = malloc(sizeof(list));
	p->item = x;
	p->next = *l;
	*l = p;
}
```
- Two C-isms to note.
	- First, the malloc function allocates a chunk of memory of sufficient size for a new node to contain x.
	- Second, the double star in \*\*l denotes that l is a *pointer to a pointer* to a list node.
		- Thus, the last line, `*l = p;` copies p to the place pointed to by l, which is the external variable maintaining access to the head of the list.
###### Deletion From a List
- Deletion from a list is somewhat more complicated.
- First, we must find a pointer to the *predecessor* of the item to be deleted.
- We do this recursively:
```C
list *item_ahead(list *, list *x) {
	if ((l == NULL) || (l->next == NULL)) {
		return(NULL);
	}
	if ((l->next) == x) {
		return(l)
	} else {
		return(item_ahead(l->next, x));
	}
}
```
- The predecessor is needed as it points to the node to be deleted, so its *next* pointer must be changed.
- The actual deletion operation is simple, once ruling out the case that the node to be deleted does not exist.
- Special care must be taken to reset the pointer to the head of the list (l) when the first element is deleted:
```C
void delete_list(list **l, list **x) {
	list *p;    // Item pointer
	list *pred; // predecessor pointer

	p = *l;
	pred = item_ahead(*l. *x);

	if (pred == NULL) {    // Splice out of list
		*l = p->next;
	} else {
		pred->next = (*x)->next;
	}
	
	free(*x);   // free memory used by node
}
```
- C language requires explicit deallocation of memory, so we must free the deleted node after we are finished with it, in order to return the memory to the system.
	- This leaves the incoming pointer as a *dangling reference* to a location that no longer exists, so care must be taken not to use this pointer again.
### Comparison
- The advantages of linked structures over static arrays include:
	- Overflow on linked structures never occurs, unless the memory is actually full.
	- Insertion and deletion are *simpler* than for static arrays.
	- With large records, moving pointers is easier and faster than moving the items themselves.
- Conversely, the relative advantages of arrays include:
	- Space efficiency: linked structures require extra memory for storing pointer fields.
	- Efficient random access to items in arrays.
	- Better memory locality and cache performance than random pointer jumping.
- **Take-Home Lesson:**
	- **Dynamic memory allocation provides us with flexibility on how and where we use our limited storage resources.**
- One final thought about these fundamental data structures is that both arrays and linked lists can be thought of as recursive objects:
	- *Lists*:
		- Chopping the first element off a linked list leaves a smaller linked list.
		- This same argument works for strings, since removing characters from a string leaves a string.
		- Lists are recursive objects.
	- *Arrays*:
		- Splitting the first k elements off an n element array gives two smaller arrays, of size k and n-k, respectively.
		- Arrays are recursive objects.
- This insight leads to simpler list processing, and efficient divide-and-conquer algorithms such as quicksort and binary search.