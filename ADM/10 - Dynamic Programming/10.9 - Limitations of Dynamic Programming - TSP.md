#Chapter_10 
- Dynamic programming doesn't always work.
	- It is important to see why it can fail, to help avoid traps leading to incorrect or inefficient algorithms.

- Our algorithmic poster child will once again be the traveling salesman problem, where we seek the shortest tour visiting all the cities in a graph.
	- We will limit attention here to an interesting special case:

- ***Problem:***
	- *Longest Simple Path*
- ***Input:*
	- A weighted graph $G=(V,E)$, with specified start and end vertices *s* and *t*.
- ***Output:***
	- What is the most expensive graph from *s* to *t* that does not visit any vertex more than once?

- This problem differs from TSP in two quite unimportant ways.
	- First, it asks for a path instead of a closed tour.
		- This difference isn't substantial: we get a closed tour simply by including the edge $(t,s)$.
	- Second, it asks for the most expensive path instead of the least expensive tour.
		- Again this difference isn't very significant: it encourages us to visit as many vertices as possible (ideally all), just as in TSP.
- The critical word in the problem statement is *simple*, meaning we are not allowed to visit any vertex more than once.

- For *unweighted graphs* (where each edge has cost 1), the longest possible simple path from *s* to *t* is of weight $n-1$.
	- Finding such *Hamiltonian paths* (if they exist) is an important graph problem, discussed in Section 19.5

### 10.9.1 - When is Dynamic Programming Correct?
- Dynamic programming algorithms are only as correct as the recurrence relations they are based on.
	- Suppose we define $LP[i,j]$ as the length of the longest simple path from *i* to *j*.
		- Note that the longest simple path from *i* to *j* has to visit some vertex *x* right before reaching *j*.
		- Thus, the last edge visited must be of the form $(x, j)$.
			- This suggests the following recurrence relation to compute the length of the longest path, where $c(x,j)$ is the cost/weight of edge $(x, j)$:
				- $LP[i,j]=\max_{x\in V, (x,j)\in E} LP[i, x] + c(x,j)$
	- This idea seems reasonable, but can you see the problem?

- First, this recurrence does nothing to enforce simplicity.
	- How do we know that vertex *j* has not appeared previously on the longest simple path from *i* to *x*?
		- If it did, then adding the edge $(x,j)$ will create a cycle.
	- To prevent this, we must define a recursive function that explicitly remembers where we have been.
		- Perhaps we could define $LP'[i,j,k]$ to denote  the length of the longest path from *i* to *j* avoiding vertex *k*?
			- This would be a step in the right direction, but still won't lead to a viable recurrence.

- The second problem concerns evaluation order.
	- What can you evaluate first?
		- Because there is no left-to-right or smaller-to-bigger ordering of the vertices on the graph, it is not clear what the *smaller* subproblems are.
		- Without such ordering, we get stuck in an infinite loop as soon as we try to do anything.

- Dynamic programming can be applied to any problem that obeys the *principle of optimality*.
	- Roughly stated, this means that partial solutions can be optimally extended given the *state* after the partial solution, instead of the specifics of the partial solution itself.
		- For example, in deciding whether to extend an approximate string matching by a substitution, insertion, or deletion, we did not need to know the sequence of operations that had been performed to date.
		- In fact, there may be several different edit sequences that achieve a cost of *C* on the first *p* characters of pattern *P* and *t* characters of string *T*.
	- Future decisions are made based on the *consequences* of previous decisions, not the actual decisions themselves.

- Problems do not satisfy the principle of optimality when the specifics of the operations matter, as opposed to just their cost.
	- Such would be the case with a special form of edit distance where we are not allowed to use combinations of operations in certain particular orders.
	- Properly formulated, however, many combinatorial problems respect the principle of optimality.

### 10.9.2 - When is Dynamic Programming Efficient?
- The running time of any dynamic programming algorithm is a function of two things:
	- The number of partial solutions we must keep track of; and
	- How long it takes to evaluate each partial solution.
- The first issue, namely the size of the state space, is usually the more pressing concern.

- In all of the examples we have seen, the partial solutions are completely described by specifying the possible stopping *places* in the input.
	- This is because the combinatorial objects being worked on (typically strings and numerical sequences) have an implicit order defined upon their elements.
		- This order cannot be scrambled without completely changing the problem.
	- Once the order is fixed, there are relatively few possible stopping places or *states*, so we get efficient algorithms.
- When the objects are not firmly ordered, however, we likely have an exponential number of possible partial solutions.

- Suppose the state of our partial longest simple path solution is the entire path *P* taken from the start to end vertex.
	- Thus, $LP[i,j,P_{ij}]$ denotes the cost of longest simple path from *i* to *j*, where $P_{ij}$ is the sequence of intermediate vertices between *i* and *j* on this path.
	- The following recurrence relation works correctly to compute this, where $P+x$ denotes appending *x* to the end of *P*:
		- $LP[i,j,P_{ij}]=\max_{j\notin P_{ix},(x,j)\in E, P_{ij}=P_{ix}+j}LP[i,x,P_{ix}]+x(x,j)$

- This formulation is correct, but how efficient is it?
	- The path $P_{ij}$ consists of an ordered sequence of up to $n-3$ vertices, so there can be up to $(n-3)!$ such paths!
- Indeed, this algorithm is really using combinatorial search (like [[9.1 - Backtracking|backtracking]]) to construct all the possible intermediate paths.
	- In fact, the max here is somewhat misleading, as there can only be one value of $P_{ij}$ to construct the state $LP[i,j,P_{ij}]$.

- We can do something better with this idea, however.
- Let $LP'[i,j,S_{ij}]$ denote the longest simple path from *i* to *j*, where $S_{ij}$ is the *set* of the intermediate vertices on this path.
	- Thus, if $S_{ij}=\{a,b,c,i,j\}$, there are exactly six paths consistent with $S_{ij}$: $iabcj, iacbj, ibacj, ibcaj, icabj$ and $icbaj$.
- This state space has at most $2^n$ elements, and is thus smaller than the enumeration of all the paths.
	- Further this function can be evalutaed using the following recurrence relation:
		- $LP'[i,j,S_{ij}]=\max_{j\notin S_{ix},(x,j)\in E, S_{ij}=S_{ix}\cup \{j\}}LP'[i,x,S_{ix}]+x(x,j)$
	- Where $S\cup \{x\}$ denotes unioning *S* with *x*.

- The longest simple path from *i* to *j* can then be found by maximizing over all possible intermediate vertex subsets:
	- $LP[i,j]=\max_S LP'[i,j,S]$.

- There are only $2^n$ subsets of *n* vertices, so this is a big improvement over enumerating all $n!$ tours.
	- Indeed, this method can be used to solve TSPs for up to thirty vertices or more, where $n=20$ would be impossible using the $O(n!)$ algorithm.

- Still, dynamic programming proves most efficient on well-ordered objects.

- ***Take-Home Lesson:***
	- ***Without an inherent left-to-right ordering on the objects, dynamic programming is usually doomed to require exponential space and time.*
