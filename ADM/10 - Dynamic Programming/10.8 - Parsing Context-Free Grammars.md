#Chapter_10 
- Compilers identify whether a particular program is a legal expression in a particular programming language, and reward you with syntax errors if it is not.
	- This requires a precise description of the language syntax, typically given by a *context-free grammar*, as shown in Figure 10.10(right).
	- Each *rule* or *production* of the grammar defines an interpretation for the named symbol on the left side of the rule as a sequence of symbols on the right side of the rule.
		- The right side can be a combination of *nonterminals* (themselves defined by rules) or *terminal* symbols defined simply as strings, such as *the*, *a*, *cat*, *milk*, and *drink*.

![[Pasted image 20231213214329.png]]

- *Parsing* a given text sequence *S* as per a given context-free grammar *G* is the algorithmic problem of constructing a *parse tree* of rule substitutions defining *S* as a single nonterminal symbol of *G*.
	- Figure 10.10(left) presents the parse tree of a simple sentence using our sample grammar.

- We assume that the sequence *S* has length *n* while the grammar *G* itself is of constant size.
	- This is fair, because the grammar defining a particular programming language (say C or Java) is of fixed length regardless of the size of the program we seek to compile.

- Further, we assume that the definitions of each rule are in *Chomsky normal form*, like the example in Figure 10.10.
	- This means that the right sides of every rule consists of either:
		- Exactly two nonterminals.
			- For example, $X\to YZ$.
		- Or exactly one terminal symbol.
			- For example, $X\to \alpha$
	- Any context-free grammar can easily and mechanically be transformed into Chomsky normal form by repeatedly shortening long right-hand sides at the cost of adding extra nonterminals and productions.
		- Thus, there is no loss of generality with this assumption.

- So how can we efficiently parse *S* using a context-free grammar where each interesting rule produces two nonterminals.
	- The key observation is that the rule applied at the root of the parse tree (say $X\to YZ$) splits *S* at some position *i* such that the left part ($S_1,...,S_i$) must be *generated* by nonterminal *Y*, and right part ($S_{i+1},...,S_n$) generated by *Z*.

- This suggests a dynamic programming algorithm, where we keep track of all nonterminals generated by each contiguous subsequence of *S*.
- Define $M[i,j,X]$ to be a Boolean function that is true only if subsequence $S_i,...,S_j$ is generated by nonterminal *X*.
	- This is true if there exists a production $X\to YZ$ and breaking point *k* between *i* and *j* such that the left part generates *Y* and the right part *Z*.
	- In other words, for $i<j$ we have:
		- $M[i,j,X]=\bigvee_{(X\to YZ)\in G}(\bigvee_{k=i}^{j-1}M[i,k,Y]\wedge M[k+1, j, Z])$
	- where $\lor$ denotes the logical *or* over all productions and split positions, and $\wedge$ denotes the logical *and* of two Boolean values.

- The terminal symbols define the boundary conditions of the recurrence.
	- In particular, $M[i,i,X]$ is true only if there exists a production $X\to \alpha$ such that $S_i=\alpha$

- What is the complexity of this algorithm?
	- The size of our state-space is $O(n^2)$, as there are $n(n+1)/2$ subsequences defined by $(i,j)$ pairs with $i\geq j$.
		- Multiplying this by the number of nonterminals, which is finite because the grammar was defined to be of constant size, has no impact on the Big Oh.
	- Evaluating $M[i,j,X]$ requires testing all intermediate values *k* where $i\leq k\leq j$, so it takes $O(n)$ in the worst case to evaluate all $O(n^2)$ cells.
		- This yields an $O(n^3)$ algorithm for parsing.

##### Stop and Think: Parsimonious Parserization
- ***Problem:***
	- Given a context-free grammar *G* and input sequence *S*, find the smallest number of character substitutions you must make to *S* so that the resulting sequence is accepted by *G*.

- ***Solution:*
	- This problem is just a very general version of [[10.2 - Approximate String Matching|edit distance]], addressed naturally by dynamic programming.
		- Parsing first sounded difficult, too, but fell to the same technique.
		- Indeed, we can solve the combined problem by generalizing the recurrence relation we used for simple parsing.
	- Define $M'[x,j,X]$ to be an *integer* function that reports the minimum number of changes to subsequence $S_i,...,S_j$ so it can be generated by nonterminal *X*.
		- This symbol will be generated by some production $X\to YZ$.
		- Some of the changes to *S* may be to the left of the breaking point and some to the right, but all we care about is minimizing the sum.
	- In other words, for $i<j$ we have:
		- $M'[i, j, X] =\min_{(X\to YZ)\in G}(\min^{j-1}_{k=i}M'[i, k, Y]+M'[k+1, j, Z])$
	- The boundary cases also change mildly.
		- If there exists a production $X\to \alpha$, the cost of matching at position *i* depends on the contents of $S_i$.
			- If $S_i=\alpha$, $M'[i,i] = 0$.
			- Otherwise, we can pay one substitution to change $S_i$ to $\alpha$, so $M'[i,i] = 1$ if $S_1\neq \alpha$.
			- If the grammar does not have a production of the form $X\to \alpha$, there is no way to substitute a single character string into something generating *X*, so $M'[i,i,X]=\infty$ for all *i*.


- ***Take-Home Lesson:***
	- ***For optimization problems on left-to-right objects, such as characters in a string, elements of a permutation, points around a polygon, or leaves in a search tree, dynamic programming likely leads to an efficient algorithm to find the optimal solution.*