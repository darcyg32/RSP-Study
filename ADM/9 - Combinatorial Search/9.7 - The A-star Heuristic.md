#Chapter_9 
- [[9.6 - Best-First Search|Best-first search]] can take a while, even if our partial cost function is a lower bound on the optimal tour, so we can stop as soon as we have a solution cheaper than the best unexplored partial solution.

- Consider the partial solutions we will encounter on a search for the optimal traveling salesman tour.
	- Costs increase with the number of edges in the partial solution, so partial solutions with few nodes will always look more promising than longer ones nearer to completion.
	- Even the most awful prefix path on $n/2$ nodes will largely be cheaper than the optimal solution on all *n* nodes, meaning that we must expand all partial solutions until their prefix cost is greater than the cost of the best full tour.
	- This will be horribly expensive to work through.

- The *A\** (A-star) *heuristic* is an elaboration on the branch-and-bound search presented earlier, where at each iteration we expanded the best (cheapest) partial solution that we have found so far.
	- The idea is to use a lower bound on the cost of all possible partial solution extensions that is stronger than just the cost of the current partial tour.
		- This will make promising partial solutions look more interesting than those that have the fewest vertices.

- How can we lower bound the cost of the full tour, which contains *n* edges, from a partial solution with *k* vertices (and thus $k-1$ edges)?
	- We know it will eventually get $n-k+1$ additional edges.
	- If `minlb` is a lower bound on the cost of any edge, specifically the distance between the two closest points, adding $(n-k+1)\times$`minlb` gives a cost lower bound that is much more realistic for the partial solution:

```C
double partial_solution_cost(tsp_solution* s, tsp_instance* t) {
	int i;                // Counter
	double cost = 0.0;    // Cost of solution

	for (i = 1; i < s->n; i++) {
		cost += distance(s, i, i + 1, t);
	}

	return cost;
}

double partial_solution_lb(tsp_solution* s, tsp_instance* t) {
	return partial_solution_cost(s, t) + (t->n - s->n + 1) * minlb;
}
```

- Figure 9.9 presents the number of full solution cost evaluations in finding the optimal TSP tour for several search variants.
	- Brute-force backtracking without pruning requires $(n-1)!$ such calls, but we do much better when we prune on partial costs, and even better when we prune using the full lower bound.
	- But branch and bound and A-star do even better here.

![[Pasted image 20231211103839.png]]

- Note that the number of full solutions encountered is a gross underestimate of the total work done on the search, which includes even partial solutions that got pruned just one move before the end of the tour.
	- But Figure 9.9 does capture the fact that best-first search might have to look at a substantially smaller part of the search tree than backtracking, even with the same pruning criteria.

- Best-first search is sort of like [[7.6 - Breadth-First Search|breadth-first search]].
	- A disadvantage of BFS over [[7.8 - Depth-First Search|DFS]] is the space required.
		- A [[9.1 - Backtracking|backtracking]] / DFS tree uses memory proportionate to the height of the tree, but a best-first / BFS tree requires maintaining all partial solutions, more akin to the width of the tree.

- The resulting size of the priority queue for best-first search is a real problem.
	- Consider the TSP experiments above.
		- For $n=11$, the queue size got to 202,063 compared to a stack size of just 11 for backtracking.
	- Space will kill you quicker than time.
	- To get an answer from a slow program you just have to be patient enough, but a program that crashes because of lack of memory will not give an answer no matter how long you wait.

- ***Take-Home Lesson:***
	- ***The promise of a given partial solution is not just its cost, but also includes the potential cost of the remainder of the solution.*
	- ***A tight solution cost estimate which is still a lower bound makes best-first search much more efficient.*

- The A\* heuristic proves useful in a variety of different problems, most notably finding [[8.3 - Shortest Paths|shortest paths]] from *s* to *t* in a graph.
	- Recall that Dijkstra's algorithm for shortest path starts from *s* and and with each iteration adds a new vertex to which it knows the shortest path.
	- When the graph describes a road network on the surface of the Earth, this known region should expand like a growing disk around *s*.

- But that means that half the growth is in a direction away from *t*, thus moving farther from the goal.
	- A best-first search, with the as-the-crow-flies straight line distance from each in-tree vertex *v* to *t* added to the in-tree distance from *s* to *v*, gives a lower bound on the driving distance from *s* to *t*, favoring growth in the right direction.
	- The existence of such heuristics for shortest path computations explains how online mapping services can supply you with the route home so quickly.