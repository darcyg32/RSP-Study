#Chapter_11 
- Reductions are an honorable way to generate new algorithms from old ones.
- Whenever we can translate the input for a problem we *want to solve* into input for a problem we *know how to solve*, we can compose the translation and the solution into an algorithm to deal with our problem.

- In this section, we look at several reductions that lead to efficient algorithms.
	- To solve problem *A*, we translate/reduce the *A* instance to an instance of *B*, and then solve this instance using an efficient algorithm for problem *B*.
		- The overall running time is the time needed to perform the reduction plus that to solve the *B* instance.

### 11.2.1 - Closest Pair
- The *closest-pair* problem asks us to find the pair of numbers within a set *S* that have the smallest distance between them.
	- For example, the closest pair in $S=\{10,4,8,3,12\}$ is $(3,4)$.
	- We can then make it a decision problem by asking if this value is less than some threshold:

- ***Input:*** A set *S* of *n* numbers, and threshold *t*.
- ***Output:*** Is there a pair $s_i,s_j\in S$ such that $|s_i-s_j|\leq t$?

- Finding the closest pair is a simple application of sorting, since the closest pair must be neighbours after sorting.
	- This gives the following algorithm:

CloseEnoughPair(*S*, *t*)
	Sort *S*.
	Is $\min_{1\leq i<n}|s_{i+1}-s_i| \leq t$?

- There are several things to note about this simple reduction:
	- The decision version captured what is interesting about the general problem, meaning it is no easier than finding the actual closest pair.
	- The complexity of this algorithm depends upon the complexity of sorting.
		- Use an $O(n\log n)$ algorithm to sort, and it takes $O(n\log n+n)$ to find the closest pair.
	- This reduction and the fact that there is an $\Omega(n\log n)$ lower bound on sorting *does not* prove that the close-enough pair problem must take $\Omega(n\log n)$ time in the worst case.
		- Perhaps this is just a slow algorithm for close-enough pair, and there is a faster approach that avoids sorting?
	- On the other hand, *if* we knew that a close-enough pair required $\Omega(n\log n)$ time to solve in the worst case, this reduction would suffice to prove that sorting couldn't be solved any faster than $\Omega(n\log n)$, because that would imply a faster algorithm for close-enough pair.

### 11.2.2 - Longest Increasing Subsequence
- Recall [[10.0 - Dynamic Programming - Introduction|Chapter 10]], where dynamic programming was used to solve a variety of problems, including string edit distance.
- To review:

- ***Problem:*** Edit Distance
- ***Input:*** Integer or character sequences *S* and *T*; penalty costs for each insertion ($c_{ins}$), deletion ($c_{del}$), and substitution ($c_{sub}$).
- ***Output:*** What is the cost of the least expensive sequence of operations that transforms *S* to *T*?

- It was shown that many other problems can be solved using edit distance.
	- But these algorithms can often be viewed as reductions.
- Consider:

- ***Problem:*** Longest Increasing Subsequence (LIS)
- ***Input:*** Integer or character sequence *S*.
- ***Output:*** What is the length of the longest sequence of positions $p_1,...,p_m$ such that $p_i<p_{i+1}$ and $S_{p_i}<S_{p_{i+1}}$?

- In [[10.3 - Longest Increasing Subsequence|Section 10.3]] I demonstrated that longest increasing subsequence can be solved as a special case of edit distance:

LongestIncreasingSubsequence(*S*)
	*T* = Sort(*S*)
	$c_{ins}=c_{del}=1$
	$c_{sub}=\infty$
	Return (|*S*| - EditDistance$(S,T,c_{ins},c_{del},c_{sub})/2$)

- Why does this work?
	- By constructing the second sequence *T* as the elements of *S* sorted in increasing order, we ensure that any common subsequence must be an increasing subsequence.
	- If we are never allowed to do any substitutions (because $s_{sub}=\infty$), the optimal alignment of *S* and *T* finds the longest common subsequence between them and removes everything else.
		- For example, transforming $S=cab$ to *abc* costs two, namely inserting and deleting the unmatched *c*.
	- The length of *S* minus half this cost gives the length of the LIS.

- What are the implications of this reduction?
	- The reduction takes $O(n\log n)$ time because of the cost of sorting.
	- Because edit distance takes time $O(|S|*|T|)$, this gives a quadratic algorithm to find the longest increasing subsequence of *S*.
- In fact, there exists a faster $O(n\log n)$ algorithm for LIS using clever data structures, while edit distance is known to be quadratic in the worst case.
	- Hence, our reduction gives us a simple but not optimal polynomial-time algorithm.

### 11.2.3 - Least Common Multiple
- The *least common multiple* (lcm) and *greatest common divisor* (gcd) problems arise often in working with integers.
- We say *b divides a* (written *b | a*) if there exists an integer *d* such that $a=bd$.
- Then:

- ***Problem:*** Least Common Multiple (lcm)
- ***Input:*** Two positive integers *x* and *y*.
- ***Output:*** Return the smallest positive integer *m* such that *m* is a multiple of *x* and also a multiple of *y*.

- ***Problem:*** Greatest Common Divisor (gcd)
- ***Input:*** Two positive integers *x* and *y*.
- ***Output:*** Return the largest integer *d* such that *d* divides both *x* and *y*.

- For example, $lcm(24,36)=72$ and $gcd(24,36)=12$. 
- Both problems can be solved easily after reducing *x* and *y* to their prime factorizations, but no efficient algorithm is known for factoring integers.
- Fortunately, Euclid's algorithm gives an efficient way to solve greatest common divisor without factoring.

- It is a recursive algorithm that rests on two observations.
	- If *(a | b)*, then $gcd(a,b)=b$
		- This should be pretty clear. If *b* divides *a*, then $a=bk$ for some integer *k*, and thus $gcd(bk, b)=b$.
	- If $a=bt+r$ for integers *t* and *r*, then $gcd(a,b)=gcd(b,r)$.

- Then, for $a\geq b$, Euclid's algorithm repeatedly replaces $(a, b)$ by $(b, a$ mod $b)$ until $b = 0$.
	- Its worst-case running time is $O(\log b)$.

- Since $x*y$ is a multiple of both *x* and *y*, $lcm(x, y) ≤ xy$.
	- The only way there can be a smaller common multiple is if there is some non-trivial factor shared between *x* and *y*.
- This observation, coupled with Euclid’s algorithm, provides an efficient way to compute least common multiple, namely:

LeastCommonMultiple(*x, y*) 
	Return ($xy/gcd(x, y)$).

- This reduction gives us a nice way to reuse Euclid's efforts for *lcm*.

### 11.2.4 - Convex Hull (\*)
- My final example of a reduction from an “easy” problem (meaning one that can be solved in polynomial time) involves finding convex hulls of point sets.

- A polygon is convex if the straight line segment drawn between any two points inside the polygon P lies completely within the polygon.
	- This is the case when P contains no notches or concavities, so convex polygons are nicely shaped.

- The convex hull provides a very useful way to provide structure to a point set.
	- Applications are presented in Section 20.2 (page 626).

- ***Problem:*** Convex Hull
- ***Input:*** A set S of n points in the plane.
- ***Output:*** Find the smallest convex polygon containing all the points of S.

- I will now show how to transform an instance of sorting (say $\{13, 5, 11, 17\}$) to an instance of the convex hull problem.
	- This means we must translate each number to a point in the plane.
		- We do so by mapping $x$ to $(x,x^2)$.

![[Pasted image 20231217183345.png]]

- Why? This maps each integer to a point on the parabola $y = x^2$, as shown in Figure 11.1.
	- Since the region above this parabola is convex, every point must be on the convex hull.
	- Furthermore, since neighboring points on the convex hull have neighboring *x* values, the convex hull returns the points sorted by the *x*-coordinate — that is, the original numbers.

- Creating and reading off these points takes $O(n)$ time:

Sort(*s*)
	For each $i\in S$:
		Create point $(i,i^2)$.
	Call subroutine convex-hull on this point set.
	From the left-most point in the hull:
		Read off the points from left to right.

- What does this mean? Recall the sorting lower bound of $\Omega(n\log n)$.
	- If we could compute convex hull in better than $n \log n$, this reduction would imply that we could sort faster than $\Omega(n \log n)$, which violates our lower bound.
		- Thus, convex hull must take $\Omega(n\log n)$ as well!
	- Observe that any $O(n\log n)$ convex hull algorithm also gives us a complicated but correct $O(n \log n)$ sorting algorithm when coupled with this reduction.