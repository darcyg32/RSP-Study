#Chapter_2 
- Coarse reasoning about an algorithm's running time is usually easy, given a precise description of the algorithm.
- What follows are several examples.

### 2.5.1 - Selection Sort
- Here we'll analyze the selection sort algorithm, which repeatedly identifies the smallest remaining unsorted element and puts it at the end of the sorted portion of the array.
- An animation of selection sort in action appears in Figure 2.5, and the code is shown below:

![[Pasted image 20231121182204.png]]

```C
void selection_sort(item_type s[], int n) {
	int i, j;    // Counters
	int min;     // Index of minimum

	for (i = 0 ; i < n ; i++) {
		min = i;
		for (j = i + 1 ; j < n ; j++) {
			if (s[j] < s[min]) {
				min = j;
			}
		}
		swap(&s[i], &s[min]);
	}
}
```

- The outer *for* loop goes around *n* times.
- The nested inner loop goes around $n-(i+1)$ times, where *i* is the index of the outer loop.
- The exact number of times the *if* statement is executed is given by:
						$T(n)=\sum_{i=0}^{n-1} \sum_{j=i+1}^{n-1} 1 = \sum_{i=0}^{n-1} n-i-1$

- What this sum is doing is adding up the non-negative integers in decreasing order status from $n-1$, that is,
						$T(n)=(n-1)+(n-2)+(n-3)+...+2+1$

- How can we reason about such a formula?
	- We must solve the summation formula using the techniques of [[2.6 - Summations|Section 2.6]] to get an exact value.
	- But with the Big Oh, we are only interested in the *order* of the expression.
	- One way to think about it is that we are adding up $n-1$ terms, whose average value is about $n/2$.
	- This yields $T(n)\approx (n-1)n/2=O(n^2)$
##### Proving the Theta
- Another way to think about this algorithm's running time is in terms of upper and lower bounds.
	- We have *n* terms at most, each of which is at most $n-1$.
	- Thus, $T(n)\leq n(n-1)=O(n^2)$. 
		- The Big Oh is an upper bound.
- The big $\Omega$ is a lower bound.
	- Looking at the sum again, we have $n/2$ terms each of which is bigger than $n/2$, followed by $n/2$ terms each greater than zero.
	- Thus, $T(n)\geq (n/2)*(n/2)+(n/2)*0=\Omega (n^2)$.
- Together with the Big Oh result, this tells us that the running time is $\Theta (n^2)$, meaning that the selection sort is quadratic.

- Generally speaking, turning a Big Oh worst-case analysis into a Big $\Theta$ involves identifying a bad input instance that forces the algorithm to perform as poorly as possible.
- But selection sort is distinctive among sorting algorithms in that it takes exactly the same time on all $n!$ possible input instances.
	- Since $T(n)=n(n-1)/2$ for all $n\geq 0$, $T(n)=\Theta (n^2)$.

### 2.5.2 - Insertion Sort
- $O(n^2)$
- $\Omega (n)$
- $\Theta (n^2)$
### 2.5.3 - String Matching
- $O(nm)$
- $\Omega (nm)$
- $\Theta (nm)$

### 2.5.4 - Matrix Multiplication
 $O(n^3)$
- $\Omega (n^3)$
- $\Theta (n^3)$