#Chapter_2 
- Machine-independent algorithm design depends upon a hypothetical computer called the *Random Access Machine*, or RAM.
- Under this model of computation, we are confronted with a computer where:
	- Each *simple* operation (+, \*, -, =, if, call) takes exactly one step.
	- Loops and subroutines are *not* considered simple operations.
		- Instead, they are the composition of many single-step operations.
			- It makes no sense for *sort* to be a single-step operation, since sorting 1,000,000 items will certainly take much longer than sorting 10 items.
		- The time it takes to run through a loop or execute a subprogram depends upon the number of loop iterations or the specific nature of the subprogram.
	- Each memory access takes exactly one step.
		- Furthermore, we have as much memory as we need.
		- The RAM model takes no notice of whether an item is in cache or on the disk.

- Under the RAM model, we measure run time by counting the number of steps an algorithm takes on a given problem instance.
	- If we assume that our RAM executes a given number of steps per second, this operation count converts naturally to the actual running time.

- The RAM is a simple model of how computers perform.
	- Although things like multiplication takes longer than addition, memory-access time variation, etc violate the assumptions of the RAM model, it proves an *excellent* model for understanding how an algorithm will perform on a real computer.
	- It balances capturing the essential behavior of computers while being simple to work with.
	- We use the RAM model because it is useful in practice.

- The RAM model is an abstraction that is generally very useful.
	- It is difficult to design an algorithm where the RAM model gives you substantially misleading results.
	- The robustness enables us to analyze algorithms in a machine-independent way.

- ***Take-Home Lesson*:
	- ***Algorithms can be understood and studied in a language- and machine-independent manner.*

### 2.1.1 - Best-Case, Worst-Case, and Average-Case Complexity
- Using the RAM model of computation, we can count how many steps our algorithm takes on any given input instance by executing it.
- However, to understand how good or bad an algorithm is in general, we must know how it works over *all* possible instances.

- To understand the notions of the best, worst, and average-case complexity, think about running an algorithm over all possible instances of data that can be fed to it.
	- For the problem of sorting, the set of possible input instances includes every possible arrangement of *n* keys, for all possible values of *n*.
- We can represent each input instance as a point on a graph (shown in Figure 2.1), where the *x*-axis represents the size of the input problem (for sorting, the number of items to sort), and the *y*-axis denotes the number of steps taken by the algorithm in this instance.

![[Pasted image 20231121105819.png]]

- These points naturally align themselves into columns, because only integers represent possible input sizes (e.g. it makes no sense to sort 10.57 items).
- We can define three interesting functions over the plot of these points:
	- *The worst-case complexity*:
		- The function defined by the maximum number of steps taken in any instance of size *n*. 
		- This represents the curve passing through the highest point in each column.
	- *The best-case complexity*:
		- The function defined by the minimum number of steps taken in any instance of size *n*.
		- This represents the curve passing through the lowest point in each column.
	- *The average-case complexity* / *expected time*:
		- The function defined by the average number of steps over all instances of size *n*.

- The worst-case complexity generally proves to be the most useful of these three measures in practice.
	- To illustrate why, the best case of bringing $*n* into a casino to gamble is walking out owning the place, but it is so unlikely that you shouldn't even think about it.
	- The worst case is that you will lose all $*n*, and is easy to calculate and very likely to happen.
	- The average case, that the typical bettor loses 87.32% of the money that they bring to the casino, is both difficult to establish and its meaning subject to debate.
		- What does *average* mean? What's an average person? Am I an average person?
- We avoid all these complexities and obtain a very useful result by considering the worst case.

- That said, average-case analysis for expected running time will prove very important with respect to *randomized algorithms*, which use random numbers to make decisions within the algorithm.

- ***Take-Home Lesson*:
	- ***Each of these time complexities defines a numerical function for any given algorithm, representing running time as a function of input size.*
	- ***These functions are just as well defined as any other numerical function.*
	- ***But time complexities are such complicated functions that we must simplify them for analysis using the [[2.2 - The Big Oh Notation|"Big Oh" notation]].***