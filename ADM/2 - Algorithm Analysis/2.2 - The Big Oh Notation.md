#Chapter_2 
- The [[2.1 - The RAM Model of Computation|best-case, worst-case, and average-case time complexities]] for any given algorithm are numerical functions over the size of possible problem instances.
- However, it is very difficult to work precisely with these functions, because they tend to:
	- *Have too many bumps*:
		- An algorithm such as binary search typically runs a bit faster for arrays of size exactly $n=2^k-1$ (where $k$ is an integer), because the array partitions work out nicely.
		- This detail it not particularly important, but it warns us that the *exact* time complexity function for any algorithm is liable to be very complicated, with lots of little up and down bumps as shown in Figure 2.2.
	- *Require too much detail to specify precisely*:
		- Counting the exact number of RAM instructions executed in the worst case requires the algorithm be specified to the detail of a complete computer program.
		- Furthermore, the precise answer depends upon uninteresting coding details (e.g. did the code use a case statement or nested ifs?).
		- Performing a precise worst-case analysis like $T(n)=12754n^2+4353n+834\lg_2 n+13546$ would clearly be very difficult work, but provides us little extra information that the observation that "the time grows quadratically with *n*."

![[Pasted image 20231121140324.png]]

- It proves to be much easier to talk in terms of simple upper and lower bounds of time-complexity functions using the Big Oh notation.
	- The Big Oh simplifies our analysis by ignoring levels of detail that do not impact our comparison of algorithms.

- The Big Oh notation ignores the difference between multiplicative constants.
	- The functions $f(n)=2n$ and $g(n)=n$ are identical in Big Oh analysis.
- This makes sense given our application.
	- Suppose a given algorithm in C language ran twice as fast as the same algorithm written in Java.
		- This multiplicative factor of two can tell us nothing about the algorithm itself, because both programs implement exactly the same algorithm.
- We should ignore such constant factors when comparing two factors.

- The formal definitions associated with Big Oh notation are as follows:
	- $f(n)=O(g(n))$ means $c*g(n)$ is an *upper bound* on $f(n)$.
		- Thus, there exists some constant *c* such that $f(n)\leq c*g(n)$ for every large enough *n*.
			- (That is, for all $n\geq n_0$, for some constant $n_0$).
	- $f(n)=\Omega (g(n))$ means $c*g(n)$ is a *lower bound* on $f(n)$.
		- Thus, there exists some constant *c* such that $f(n)\geq c*g(n)$ for every large enough *n*.
	- $f(n)=\Theta (g(n))$ means $c_1*g(n)$ is an *upper bound* on $f(n)$ and $c_2*g(n)$ is an *lower bound* on $f(n)$, for every large enough *n*.
		- Thus, there exists some constants $c_1$ and $c_2$ such that $f(n)\leq c_1*g(n)$ and $f(n)\geq c_2*g(n)$, for every large enough *n*.
			- This means that $g(n)$ provides a nice, tight bound on $f(n)$.

![[Pasted image 20231121140808.png]]

- These definitions are illustrated in Figure 2.3.
- Each of these definitions assumes there is a constant $n_0$ beyond which they are satisfied.
- We are not concerned about small values of *n*, anything to the left of $n_0$.
	- We don't really care whether one sorting algorithm sorts 6 items faster than another, but we do need to know which algorithm proves faster when sorting 10,000 or 1,000,000 items.
- The Big Oh notation enables us to ignore details and focus on the big picture.

- ***Take-Home Lesson*:
	- ***The Big Oh notation and worst-case analysis are tools that greatly simplify our ability to compare the efficiency of algorithms.*

- The Big Oh notation provides for a rough notion of equality when comparing functions.
	- It is somewhat jarring to see an expression like $n^2=O(n^3)$, but its meaning can always be resolved by going back to the definitions in terms of upper and lower bounds.
- It is perhaps most instructive to read the "=" here as meaning *one of the functions that are*.
	- Clearly, $n^2$ is one of the functions that are $O(n^3)$.