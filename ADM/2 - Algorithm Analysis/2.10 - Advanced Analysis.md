#Chapter_2 

### 2.10.1 - Esoteric Functions
- *Inverse Ackermann's function* $f(n)=\alpha (n)$:
	- This function arises in the detailed analysis of several algorithms, most notably the Union-Find data structure discussed in Section 8.1.3.
	- It is sufficient to think of this as talk for the slowest growing complexity function.
	- Unliked the constant function $f(n)=1$, $\alpha (n)$ eventually gets to infinity as $n\to\infty$, but so slowly that its value is smaller than 5 for any value of *n* that can be written in the physical universe.
- $f(n)=\log\log n$:
	- The "log log" function is the logarithm of the logarithm of *n*.
	- It might arise doing a binary search on an sorted array of only $\lg n$ items.
- $f(n)=\log n / \log\log n$:
	- This function grows a little slower than $\log n$, because it is divided by an even slower growing function.
	- Consider an *n*-leaf rooted tree of degree *d*.
		- The height *h* is given by $n=2^h \to h=\lg n$ by taking the logarithm of both sides of the equation.
		- Now consider the height of such a tree when the degree $d=\log n$.
			- Then, $n=(\log n)^h \to h=\log n / \log \log n$
- $f(n)=\log^2n$:
	- The product of two log functions.
	- It might arise if we wanted to count the bits looked at when doing a binary search on *n* items, each of which was an integer from 1 to (say) $n^2$.
		- Each such integer requires a $\lg(n^2)=2\lg n$ bit representation, and we look at $\lg n$ of them, for a total of $2\lg^2n$ bits.
	- The "log squared" function typically arises in the design of intricate nested data structures, where each node in (say) a binary tree represents another data structure, perhaps ordered on a different key.
- $f(n)=\sqrt n$:
	- Represents the class of "sublinear polynomials" since $\sqrt n = n^{1/2}$.
	- Such functions arise in building *d*-dimensional grids that contain *n* points.
		- A $\sqrt n * \sqrt n$ square has area *n*, and an $n^{1/3}*n^{1/3}*n^{1/3}$ cube has volume $n$.
		- In general, a *d*-dimensional hypercube of length $n^{1/d}$ on each side has volume $n$.
- $f(n)=n^{(1+\epsilon)}$:
	- Epsilon ($\epsilon$) denotes a constant that can be made arbitrarily small but never quite goes away.
	- Suppose I design an algorithm that runs in $2^c*n^{(1+1/c)}$ time, and I get to pick whichever $c$ I want.
		- The exponent keeps getting better the larger I make $c$.
		- However, I cannot make $c$ arbitrarily large before the $2^c$ term begins to dominate.
			- Instead, we report this algorithm as running in $O(n^{1+\epsilon})$, and leave the best value of $\epsilon$ to the beholder.

### 2.10.2 - Limits and Dominance Relations
- The dominance relation between functions is a consequence of the theory of limits.
- We say that $f(n)$ *dominates* $g(n)$ if $\lim_{n\to\infty}g(n)/f(n) = 0$.