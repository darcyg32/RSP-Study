#Chapter_2 
- Working with Big Oh requires simplifications of algebraic expressions.

### 2.4.1 - Adding Functions
- The sum of two functions is governed by the dominant one, namely:
	$f(n)+g(n) \to \Theta (max(f(n),g(n)))$

- This is very useful in simplifying expressions.
	- For example, it gives us that $n^3+n^2+n+1=\Theta (n^3)$.
	- The only part that patters is the dominant term.

- The intuition is as follows:
	- At least half the bulk of $f(n)+g(n)$ must come from the larger value.
	- The dominant function will, by definition, provide the larger value as $n\to \infty$.
	- Thus, dropping the smaller function from consideration reduces the value by at most a factor of 1/2, which is just a multiplicative constant.
		- For example, if $f(n)=O(n^2)$ and $g(n)=O(n^2)$, then $f(n)+g(n)=O(n^2)$ as well.

### 2.4.2 - Multiplying Factors
- Multiplication is like repeated addition.
- Consider multiplication by any constant $c>0$, be it 1.02 or 1,000,000.
	- Multiplying a function by a constant cannot affect its asymptotic behavior, because we can multiply the bounding constants in the Big Oh analysis to account for it.
- Thus:
	$O(c*f(n))\to O(f(n))$
	$\Omega (c*f(n))\to \Omega (f(n))$
	$\Theta (c*f(n))\to \Theta (f(n))$

- Of course, *c* must be strictly positive (i.e. $c>0$) to avoid any funny business, since we can wipe out even the fastest growing function by multiplying it by zero.

- On the other hand, when two functions in a product are increasing, both are important.
	- An $O(n!\log n)$ function dominates $n!$ by just as much as $\log n$ dominates 1.
- In general:
	$O(f(n)*g(n))\to O(f(n)*g(n))$
	$\Omega (f(n)*g(n))\to \Omega (f(n)*g(n))$
	$\Theta (f(n)*g(n))\to \Theta (f(n)*g(n))$
