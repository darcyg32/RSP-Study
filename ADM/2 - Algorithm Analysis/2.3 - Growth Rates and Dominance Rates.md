#Chapter_2 
- With [[2.2 - The Big Oh Notation|the Big Oh notation]], we discard the multiplicative constants.
	- Thus the functions $f(n)=0.001n^2$ and $g(n)=1000n^2$ are treated identically, even though $g(n)$ is a million times larger than $f(n)$ for all values of $n$.

- The reason we are content with such coarse Big Oh analysis is provided by Figure 2.4, which shows the growth rate of several common time analysis functions.
	- In particular, it shows how long algorithms that use $f(n)$ operations take to run on a fast computer, where each operation costs one nanosecond ($10^{-9}$ seconds).

![[Pasted image 20231121143749.png]]

- The following conclusions can be drawn from this table:
	- All such algorithms take roughly the same time for $n=10$
	- Any algorithm with $n!$ running time becomes useless for $n\geq 20$.
	- Algorithms whose running time is $2^n$ have a greater operating range, but become impractical for $n>40$.
	- Quadratic-time algorithms, whose running time is $n^2$, remain usable up to about $n=10,000$, but quickly deteriorate with larger inputs. They are likely to be hopeless for $n>1,000,000$.
	- Linear-time and $n\lg n$ algorithms remain practical on inputs of one billion items.
	- An $O(\lg n)$ algorithm hardly sweats for any imaginable value of *n*.

- Overall, even ignoring constant factors, we get an excellent idea of whether a given algorithm is appropriate for a problem of a given size.

### 2.3.1 - Dominance Relations
- The Big Oh notation groups functions into a set of classes, such that all the functions within a particular class are essentially equivalent.
	- Functions $f(n)=0.34n$ and $g(n)=234,234n$ belong in the same class, namely those that are order $\Theta (n)$.
- When two functions $f$ and $g$ belong to different classes, they are *different* with respect to our notation, meaning either $f(n)=O(g(n))$ or $g(n)=O(f(n))$, but not both.

- We say that a faster growing function *dominates* a slower growing one, just as a faster growing company eventually comes to dominate the laggard.
	- When $f$ and $g$ belong to different classes (i.e. $f(n)\neq \Theta(g(n))$), we say $g$ *dominates* $f$ when $f(n)=O(g(n)).
		- This is sometimes written as $g \gg f$.

- The good news is that only a few different function classes tend to occur in the course of basic algorithm analysis.
	- These suffice to cover almost all the algorithms we will discuss in this text, and are listed in increasing dominance:

- *Constant functions*, $f(n)=1$:
	- Such functions might measure the cost of adding two numbers, printing out “Hello World”, or the growth realized by functions such as f(n) = min(n, 100).
	- In the big picture, there is no dependence on the parameter n.
- *Logarithmic Functions*, $f(n)=\log n$:
	- Logarithmic time complexity shows up in algorithms such as binary search.
	- Such functions grow quite slowly as *n* gets big, but faster than the constant function (which isn't growing at all).
	- Logarithms will be discussed in Section 2.7.
- *Linear Functions*, $f(n)=n$:
	- Such functions measure the cost of looking at each item once (or twice, or ten times) in an n-element array, say to identify the biggest item, the smallest item, or compute the average value.
- *Superlinear Functions*, $f(n)=n\lg n$:
	- This important class of functions arises in such algorithms as quicksort and mergesort. 
	- They grow just a little faster than linear, but enough so to rise to a higher dominance class.
- *Quadratic Functions*, $f(n)=n^2$:
	- Such functions measure the cost of looking at most or all pairs of items in an n-element universe.
	- These arise in algorithms such as insertion sort and selection sort.
- *Cubic Functions*, $f(n)=n^3$:
	- Such functions enumerate all triples of items in an n-element universe.
	- These also arise in certain dynamic programming algorithms, to be developed in Chapter 10.
- *Exponential Functions*, $f(n)=c^n$ *for a given constant* $c>1$:
	- Functions like $2^n$ arise when enumerating all subsets of *n* items.
- *Factorial Functions*, $f(n)=n!$:
	- Functions like $n!$ arise when generating all permutations or orderings of *n* items.

- The intricacies of dominance relations will be further discussed in Section 2.10.2.
- However, all you really need to understand is that:
$n!\gg 2^n\gg n^3\gg n^2\gg n\log n\gg n\gg \log n\gg 1$

- ***Take-Home Lesson***:
	- ***Although esoteric functions arise in advanced algorithm analysis, a small set of time complexities suffice for most algorithms we will see in this book***