#Chapter_2 
- A *logarithm* is simply an inverse exponential function.
- Saying $b^x=y$ is equivalent to saying that $x=\log_by$.
	- Further, this is the same as saying $b^{\log_by}=y$.

- Exponential functions grow at a distressingly fast rate. Thus, inverse exponential functions (logarithms) grow refreshingly slowly.
- Logarithms arise in any process where things are repeatedly halved.

### 2.7.1 - Logarithms and Binary Search
- Binary search is a good example of an $O(\log n)$ algorithm.
- After just one comparison you can discard one half of the items you are trying to search.
- The number of steps the algorithm takes equals the number of times we can halve *n* until only one name is left.
	- By definition, this is exactly $\log_2n$.

### 2.7.2 - Logarithms and Trees
- What is the height *h* of a rooted binary tree with *n* leaf nodes?
	- Note that the number of leaves double every time we increase the height by 1.
	- To account for *n* leaves, $n=2^h$, which implies that $h=log_2n$.

- What if we generalize to trees with *d* children, where $d=2$ for the case of binary trees?
	- The number of possible leaves multiplies by $d$ every time we increase the heigh by 1, so to account for *n* leaves, $n=d^h$, which implies that $h=log_dn$.

### 2.7.3 - Logarithms and Bits
- How many bits *w* do we need to represent any one of *n* different possibilities?
	- The key observation is that there must be at least *n* different bit patterns of length *w*.
	- Since the number of different bit patterns doubles as you add each bit, we need at least *w* bits where $2^w=n$.
		- In other words, we need $w=log_2n$ bits.

### 2.7.4 - Logarithms and Multiplication
- Recall that $log_a(xy)=log_a(x)+log_a(y)$; that is, the log of a product is the sum of all the logs.
	- A direct consequence of this is $log_an^b=b*log_an$.

- How can we compute $a^b$ for any *a* and *b* using the *exp(x)* and *ln(x)* functions on your calculator, where $\exp(x)=e^x$ and $\ln(x)=\log_e(x)$?
	- We know $a^b=\exp(\ln(a^b))=\exp(b\ln(a))$, so the problem is reduced to one multiplication plus one call to each of these functions.

### 2.7.5 - Fast Exponentiation
- Suppose we need to *exactly* compute the value of $a^n$ for some reasonable large *n*.
	- Observe that $n=n/2 + n/2$.
		- If *n* is even, then $a^n=(a^{n/2})^2$.
		- If *n* is odd, then $a^n=a(a^{n/2})^2$.

- In either case, we halve halved the size of our exponent at the cost of, at most, two multiplications, so $O(\lg n)$ multiplications suffice to compute the final value.
![[Pasted image 20231122094328.png]]

### 2.7.6 - Logarithms and Summations
- The Harmonic numbers arise as a special case of a sum of a power of integers, namely $H(n)=S(n,-1)$.
						 $H(n)=\sum_{i=1}^{n}1/i=\Theta(\log n)$.

- The Harmonic numbers usually explain "where the log comes from" when one magically pops out from algebraic manipulation.


See [[2.8 - Properties of Logarithms]].