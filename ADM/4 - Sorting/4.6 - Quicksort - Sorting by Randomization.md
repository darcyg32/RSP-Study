#Chapter_4 
- Suppose we select an arbitrary item *p* from the *n* items we seek to sort.
- *Quicksort* (shown in Figure 4.5) separates the $n-1$ other items into two piles:
	- A low pile containing all the elements that are $< p$; and
	- A high pile containing all the elements that are $\geq p$.
- Low and high denote the array positions into which we place the respective piles, leaving a single slot between them for *p*.

![[Pasted image 20231119093615.png]]

- Such partitioning buys us two things.
	- First, the pivot element *p* ends up in the exact array position it will occupy in the final sorted order.
	- Second, after partitioning, no element flips to the other side in the final sorted order.
		- ***Thus, we can now sort the elements to the left and right of the pivot independently!***
- This gives us a recursive sorting algorithm, since we can use the partitioning approach to sort each subproblem.
- The algorithm must be correct, because each element ultimately ends up in the proper position:
```C
void quicksort(item_type s[], int l, int h) {
	int p;    // Index of partition

	if (l < h) {
		p = partition(s, l, h);
		quicksort(s, l, p - 1);
		quicksort(s, p + 1, h);
	}
}
```

- We can partition the array in one linear scan for a particular pivot element by maintaining three sections of the array:
	- Less than the pivot (to the left of `firsthigh`);
	- Greater than or equal to the pivot (between `firsthight` and `i`); and
	- Unexplored (to the right of `i`), as implemented below.

```C
int partition(item_type s[], int l, int h) {
	int i;            // Counter
	int p;            // Pivot element index
	int firsthigh;    // Divider position for pivot element

	p = h;            // Select last element as pivot
	firsthigh = l;

	for (i = l ; i < h ; i++) {
		if (s[i] < s[p]) {
			swap(&s[i], &s[firsthigh]);
			firsthigh++;
		}
	}

	swap(&s[p], &s[firsthigh]);

	return(firsthigh);
}
```

- Since the partitioning step consists of at most *n* swaps, it takes time linear with the number of keys.

- But how long does the entire quicksort take?
	- As with mergesort, quicksort builds a recursion tree of nested subranges of the *n*-element array.
	- As with mergesort, quicksort spends linear time processing (now `partition`-ing instead of `merge`-ing) the elements in each subarray on each level.
	- As with mergesort, quicksort runs in $O(n*h)$ time, where *h* is the height of the recursion tree.
	- 
- The difficulty is that the height of the tree depends upon where the pivot element ends up in each partition.
	- If we get very lucky and *happen* to repeatedly pick the median element as our pivot, the subproblems are always half the size of those at the previous level.
	- The height represents the number of times we can halve *n* until we get down to 1, meaning $h = \lg n$.
		- This happy situation is shown in Figure 4.6(Left), and corresponds to the best case of quicksort.
- Now suppose we consistently get unlucky, and our pivot element *always* splits the array as unequally as possible.
	- This implies that the pivot element is always the biggest or smallest element in the sub-array.
	- After this pivot settles into its position, we will be left with one subproblem of size $n-1$.
	- After doing linear work we have reduced the size of our problem by just one element, as shown in Figure 4.6(Right).
	- It takes a tree of height $n-1$ to chop our array down to one element per level, for a worst case time of $\Theta (n^2)$.

![[Pasted image 20231119103957.png]]

- Thus, the worst case for quicksort is worse than [[4.3 - Heapsort - Fast Sorting via Data Structures|heapsort]] or [[4.5 - Mergesort - Sorting by Divide and Conquer|mergesort]].
- To justify its name, quicksort had better be good in the average case.
	- Understanding why requires some intuition about random sampling.

### 4.6.1 - Intuition - The Expected Case for Quicksort
- The expected performance of quicksort depends upon the height of the partition tree constructed by pivot elements at each step.
	- Mergesort splits the keys into two equal halves, sorts both of them recursively, and then merges the halves in linear time - and hence runs in $O(n\log n)$ time.
		- Thus, whenever our pivot element is near the centre of the sorted array (meaning the pivot is close to the median element), we get the same good split realizing the same running time as mergesort.

- I will give an intuitive explanation of why quicksort runs in $O(n\log n)$ time in the average case.
- How likely is it that a randomly selected pivot is a good one?
	- The best possible selection for the pivot would be the median key, because exactly half of elements would end up left, and half the elements right, of the pivot.
		- However, we only have a probability of $1/n$ that a randomly selected pivot is the median, which is quite small.
	
- Suppose we say a key is a *good enough* pivot if it lies in the centre half of the sorted space of keys - those ranked in from $n/4$ to $3n/4$ in the space of all keys to be sorted.
	- Such *good enough* pivot elements are quite plentiful, since half the elements lie closer to the middle than to one of the two ends (see Figure 4.7).
	- Thus, on each selection we will pick a *good enough* pivot with probability $1/2$.
	- We will make good progress towards sorting whenever we pick a good enough pivot.
![[Pasted image 20231119104100.png]]

- The worst possible good enough pivot leaves the bigger of the two partitions with $3n/4$ items.
	- This happens also to be the expected size of the larger partition left after picking a random pivot *p*, at the median between the worst possible pivot ($p = 1$ or $p = n$, leaving a partition of size $n - 1$), and the best possible pivot ($p = n/2$, leaving two partitions of size $n/2$).
- So what is the height $h_g$ of a quicksort partition tree constructed repeatedly from the expected pivot value?
	- The deepest path through this tree passes through partitions of size $n, (3/4)n, (3/4)^2n, ...,$ down to 1.
	- How many times can we multiply $n$ by $3/4$ until it gets down to one?
		- $(3/4)^{h_g}n=1\Longrightarrow n=(4/3)^{h_g}$.
		- So $h_g = \log_{4/3}n$.

- On average, random quicksort partition trees (and by analogy, [[3.4 - Binary Search Trees|binary search trees]] under random insertion) are very good.
- More careful analysis shows the average height after *n* insertions is approximately $2\ln n$.
	- Since $2\ln n \approx 1.386\lg n$, this is only 39% taller than a perfectly balanced binary tree.
- Since quicksort does $O(n)$ work partitioning on each level, the average time is $O(n\log n)$.
- If we are *extremely* unlucky, and our randomly selected elements are always among the largest or smallest elements in the array, quicksort turns into selection sort and runs in $O(n^2)$.
	- But the odds against this are vanishingly small.

### 4.6.2 - Randomized Algorithms
- There is an important subtlety about the expected case $O(n\log n)$ running time for quicksort.
	- Our quicksort implementation above selected the last element in each sub-array as the pivot.
		- Suppose this program were given a sorted array as input.
		- Then at each step it would pick the worst possible pivot, and run in quadratic time.

- For any deterministic method of pivot selection, there exists a worst-case input instance which will doom us to quadratic time.
- The analysis presented above made no claim stronger than:
	- "Quicksort runs in $\Theta(n\log n)$ time, with high probability, *if* you give it randomly ordered data to sort."

- But now suppose we add an initial step to our algorithm where we randomly permute the order of the *n*elements before we try to sort them.
	- Such a permutation can be constructed in $O(n)$ time.
- This may seem like wasted effort, but it provides the guarantee that we can expect $\Theta (n\log n)$ running time *whatever* the initial input was.
	- The worst case performance still can happen, but it now depends only upon how unlucky we are.
- There is no longer a well-defined "worst-case" input.
- We can now claim that:
	- "Randomized quicksort runs in $\Theta (n\log n)$ time on *any* input, with high probability."
- Alternatively, we could get the same guarantee by selecting a random element to be the pivot at each step.

- *Randomization* is a powerful tool to improve algorithms with bad worst-case but good average-case complexity.
	- It can be used to make algorithms more robust to boundary cases and more efficient on highly structures input instances that confound heuristic decisions (such as sorted input to quicksort).
	- It often lends itself to simple algorithms that provide expected-time performance guarantees, which are otherwise obtainable only using complicated deterministic algorithms.
- Randomized algorithms will be the topic of Chapter 6.

- Proper analysis of randomized algorithms requires some knowledge of probability theory, and will be deferred to Chapter 6.
- However, some of the basic approaches to designing efficient randomized algorithms are readily explainable:
	- *Random Sampling:
		- Want to get an idea of the median value of *n* things, but don't have either the time or space to look at them all?
			- Select a small random sample of the input and find the median of those.
			- The result should be representative for the full set.
		- This is the idea behind opinion polling, where we sample a small number of people as a proxy for the full population.
			- Biases creep in unless you take a truly *random* sample, as opposed to the first *x* people you happen to see.
			- To avoid bias, actual polling agencies typically dial random phone numbers and hope someone answers.
	- *Randomized Hashing*:
		- We have claimed that hashing can be used to implement dictionary search in $O(1)$ "expected time".
			- However, for any hash function there is a given worst-case set of keys that all get hashed to the same bucket.
			- But now suppose we randomly select our hash function from a large family of good ones as the first step of our algorithm.
			- We get the same type of improved guarantee that we did with randomized quicksort.
	- *Randomized Search*:
		- Randomization can also be used to drive search techniques such as simulated annealing, as will be discussed in Section 12.6.3.

##### Stop and Think: Nuts and Bolts
- ***Problem***:
	- The *nuts and bolts* problem is defined as follows:
		- You are given a collection of *n* bolts of different widths, and *n* corresponding nuts.
		- You can test whether a given nut and bolt fit together, from which you learn whether the nut is too large, too small, or an exact match for the bolt.
		- The differences in size between pairs of nuts or bolts are too small to see by eye, so you cannot compare the sizes of two nuts or two bolts directly.
		- You are asked to match each bolt to each nut as efficiently as possible.
	- Give an $O(n^2)$ algorithm to solve the nuts and bolts problem.
	- Then give a randomized $O(n\log n)$ expected-time algorithm for the same problem.
- ***Solution***:
	- The brute force algorithm involves starting with the first bolt and comparing it to each nut until a match is found.
		- In the worst case, this will require *n* comparisons.
		- Repeating this for each successive bolt on all remaining nuts yields an algorithm with a quadratic number of comparisons.
	- But what if we pick a random bolt and try it?
		- On average, we would expect to get about halfway through the set of nuts before we found the match, so this randomized algorithm would do half the work on average as the worst case.
		- That counts as some kind of improvement, although not an asymptotic one.
	- Randomized quicksort achieves the desired expected-case running time, so a natural idea is to emulate it on the nuts and bolts problem.
		- The fundamental step in quicksort is partitioning elements around a pivot.
		- Can we partition nuts and bolts around a randomly selected bolt *b*?
			- Certainly we can partition the nuts into those of size less than *b* and greater than *b*, but decomposing the problem into two halves requires partitioning the bolts as well, and we cannot compare bolt against bolt.
			- But once we find the matching nut to *b*, we can use it to partition the bolts accordingly.
		- In $2n-2$ comparisons, we partition the nuts and bolts, and the remaining analysis follows directly from randomized quicksort

### 4.6.3 - Is Quicksort Really Quick?
- There is a clear, asymptotic difference between a $\Theta (n\log n)$ algorithm and one that runs in $\Theta (n^2)$.
	- Only the most obstinate reader would doubt the claim that mergesort, heapsort, and quicksort will all outperform insertion sort or selection sort on large enough instances.

- But how can we compare two $\Theta (n\log n)$ algorithms to decide which is faster?
- How can we prove that quicksort is really quick?
	- Unfortunately, the RAM model and Big Oh analysis provide to coarse a set of tools to make that type of distinction.
	- When faced with algorithms of the same asymptotic complexity, implementation details and system quirks such as cache performance and memory size often prove to be the decisive factor.

- What we can say is that experiments show that when quicksort is implemented well, it is typically two to three times faster than mergesort or heapsort.
	- The primary reason is that the operations in the innermost loop are simpler.