#Chapter_4 
- To sort names for a class roster or the telephone book, we could first partition them according to the first letter of the last name.
	- This will create 26 different piles (or buckets) of names.
- Observe that any name in the *J* pile must occur after all names in the *I* pile, and before any name in the *K* pile.
	- Therefore, we can proceed to sort each pile individually and just concatenate the sorted piles together at the end.
- Assuming the names are distributed evenly among the buckets, the resulting 26 sorting problems should each be substantially smaller than the original problem.
	- By now further partitioning each pile based on the *second* letter of each name, we can generate smaller and smaller piles.
		- The set of names will be completely sorted as soon as every bucket contains only a single name.
- Such an algorithm is commonly called *bucketsort* or *distribution sort*.

- Bucketing is a very effective idea whenever we are confident that the distribution of data will be roughly uniform.
	- It is the idea that underlies [[3.7 - Hashing|hash tables]], *kd*-trees, and a variety of other practical data structures.
- The downside of such techniques is that the performance can be terrible when the data distribution is not what we expected.
	- Although data structures such as balanced binary trees can offer guaranteed worst-case behaviour for any input distribution, no such promise exists for heuristic data structures on unexpected input distributions.

- Non-uniform distributions do occur in real life.
- Consider Americans with the uncommon last name of Shifflett.
	- As of time of writing, the Manhattan telephone directory (with over one million names) contained exactly 5 Shiffletts. So how many should there be in a small city of 50,000 people?
	- There are *two and a half pages* of Shiffletts in the Charlottesville, Virginia telephone book.
	- The Shifflett clan is a fixture of the region, but it would play havoc with any distribution sort program, as refining buckets from *S* to *Sh* to *Shi* to ... to *Shifflett* results in no significant partitioning.

- ***Take-Home Lesson:***
	- *Sorting can be used to illustrate many algorithm design paradigms. Data structure techniques, divide and conquer, randomization, and incremental construction all lead to efficient sorting algorithms.*

### 4.7.1 Lower Bounds for Sorting
- One last issue on the complexity of sorting.

- We have seen several sorting algorithms that run in worst-case $O(n \log n)$ time, but none that are linear.
	- To sort *n* items certainly requires looking at all of them, so any sorting algorithm must be $\Omega (n)$ in the worst case.
	- Might sorting be possible in linear time?

- The answer is no, presuming that your algorithm is based on comparing pairs of elements.
- An $\Omega (n\log n)$ lower bound can be shown by observing that any sorting algorithm must behave differently during execution on each of the *n!* possible permutations of *n* keys.
	- If an algorithm did *exactly* the same thing with two different input permutations, there is no way that both of them could correctly come out sorted.
		- The outcome of each pairwise comparison governs the run-time behaviour of any comparison-based sorting algorithm.
- We can think of the set of all possible executions for such an algorithm as a tree with *n!* leaves, each of which correspond to one input permutation, and each root-to-leaf path describes the comparisons performed to sort the given input.
	- The minimum height tree corresponds to the fastest possible algorithm, and it happens that $\lg (n!)=\Theta (n\log n)$.

- Figure 4.9 presents the decision tree for insertion sort and on three elements.
- To interpret it, simulate what insertion sort does on the input $a=(3,1,2)$.
	- Because $a_1\geq a_2$, these elements must be swapped to produce a sorted order.
	- Insertion sort the compares the end of the sorted array (the original input $a_1$) against $a_3$.
		- If $a_1\geq a_3$, the final test of $a_3$ against the head of the sorted part (original input $a_2$) decides whether to put $a_2$ first or second in sorted order.
![[Pasted image 20231119152918.png]]

- This lower bound is important for several reasons.
	- First, the idea can be extended to give lower bounds for many applications of sorting, including element uniqueness, finding the mode, and constructing convex hulls.
	- Sorting has one of the few non-trivial lower bounds among algorithmic problems.
- We will present a different approach to arguing that fast algorithms are unlikely to exist in Chapter 11.

- Note that hashing-based algorithms do not perform such element comparisons, putting them outside the scope of this lower bound.
	- But hashing-based algorithms can get unlucky, and with worst-case luck the running time of any randomized algorithm for one of these problems will be $\Omega (n\log n)$,.