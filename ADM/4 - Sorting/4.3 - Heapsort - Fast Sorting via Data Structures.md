#Chapter_4 
- Sorting is great for studying algorithm design paradigms, since many useful techniques lead to interesting sorting algorithms.
- The next several sections will introduce algorithmic design techniques motivated by particular sorting algorithms.

- We start with data structure design, because one of the most dramatic algorithmic improvements via appropriate data structures occurs in sorting.
- Selection sort is a simple-to-code algorithm that repeatedly extracts the smallest remaining element from the unsorted part of the set:
```
SelectionSort(A)
	for i = 1 to n, do
		Sort[i] = Find-Minimum from A
		Delete-Minimum from A
	Return(Sort)
```
- A C language implementation of selection sort appeared back in Section 2.5.1.
	- There, we partitioned the input array into sorted and unsorted regions.
	- To find the smallest item, we performed a linear sweep through the unsorted portion of the array.
	- The smallest item is then swapped with the *i*th item in the array before moving on to the next iteration.
- Selection sort performs *n* iterations, where the average iteration takes *n*/2 steps, for a total of $O(n^2)$ time.

- But what if we improve the data structure?
- It takes $O(1)$ time to remove a particular item from an unsorted array after it has been located, but $O(n)$ time to find the smallest item.
	- These are exactly the operations supported by priority queues.
- So what happens if we replace the data structure with a better priority queue implementation, either a heap or a balanced binary tree?
	- The operations within the loop now take $O(\log n)$ time each, instead of $O(n)$.
	- Using such a priority queue implementation speeds up selection sort from $O(n^2)$ to $O(n\log n)$.
- The name typically given to this algorithm, *heapsort*, obscures the fact that the algorithm is nothing but an implementation of selection sort using the right data structure.

### 4.3.1 - Heaps
- Heaps are a simple and elegant data structure for efficiently supporting the priority queue operations *insert* and *extract-min*.
- Heaps work by maintaining a partial order on the set of elements that is weaker than the sorted order (so it can be efficient to maintain), yet stronger than random order (so the minimum element can be quickly identified).

- Power in any hierarchically structured organisation is reflected by a tree, where each node in the tree represents a person, and edge (*x, y*) implies that *x* directly supervises (or dominates) *y*.
	- The person at the root sits at the "top of the heap".

- In this spirit, a *heap-labeled tree* is defined to be a binary tree such that the key of each node *dominates* the keys of its children.
- In a *min-heap*, a node dominates its children by having a smaller tree than they do, while in a *max-heap*, parent nodes dominate by being bigger.
	- Figure 4.2(L) presents a min-heap ordered tree of a specific set of years.
![[Pasted image 20231115171943.png]]

- The most natural implementation of this binary tree would store each key in a node with pointers to its two children.
	- But as with binary search trees, the memory used by pointes can easily outweigh the size of the keys, which is the data we are really interested in.
- The *heap* is a slick data structure that enables us to represent binary trees without using any pointers.
	- We store data as an array of keys, and use the position of the keys to *implicitly* play the role of the pointers.
- We store the root of the tree in the first position of the array, and its left and right children in the second and third positions, respectively.
	- In general, we store the $2^{l-1}$ keys of the *l*th level of a complete binary tree from left to right in positions $2^{l-1}$ to $2^l-1$, as shown in Figure 4.2(R).
	- We assume that the array starts with index 1 to simplify matters.

```C
typedef struct {
	item_type q[PQ_SIZE+1];    // Body of queue
	int n;                     // Number of queue elements
} priority queue;
```
- What is especially nice about this representation is that the positions of the parent and children of the key at position *k* are readily determined.
	- The *left* child of *k* sits in position $2k$ and the right child in $2k+1$, while the parent of *k* holds court in position $[k/2]$.
	- Thus, we can move around the tree without any pointers.
```C
int pq_parent(int n) {
	if (n == 1) {
		return(-1);
	}
	return((int) n/2);    // Implicitly take floor(n/2)
}

int pq_young_child(int n) {
	return(2 * n);
}
```

- This approach means that we can store any binary tree in an array without pointers.
- But what is the catch?
	- Suppose our height *h* tree was sparse, meaning that the number of nodes $n \ll 2^h -1$.
	- All missing internal nodes still take up space in our structure, since we must represent a full binary tree to maintain the positional mapping between parents and children.

- Space efficiency thus demands that we not allow holes in our tree - meaning that each level be packed as much as it can be. Then only the last level may be incomplete.
- By packing the elements of the last level as far left as possible, we can represent an *n*-key tree using the first *n* elements of the array.
	- If we did not enforce these structural constraints, we might need an array of size $2^n - 1$ to store *n* elements.

- With heaps, all but the last level are filled, so the height *h* of an *n* element heap is logarithmic; because:
		$\sum_{i=0}^{h-1} 2^i = 2^h - 1 \geq n$
	- implying that $h=[\lg (n+1)]$.

- This implicit representation of binary trees saves memory, but is less flexible than using pointers.
	- We cannot store arbitrary tree topologies without wasting large amounts of space.
	- We cannot move subtrees around by just changing a single pointer, only by explicitly moving all the elements in the subtree.
- This loss of flexibility explains why we cannot use this idea to represent binary search trees, but it works just fine for heaps.

##### Stop and Think: Who's Where in the Heap?
- Problem:
	- How can we efficiently search for a particular key *k* in a heap?
- Solution:
	- We can't.
	- Binary search does not work because a heap is not a binary search tree.
	- We know almost nothing about the relative order of the *n*/2 leaf elements in a heap. Certainly nothing that lets us avoid doing linear search through them.

### 4.3.2 Constructing Heaps
- Heaps can be constructed incrementally, by inserting each new element into the left-most open spot in the array, namely the (*n* + 1)th position of a previously *n*-element heap.
	- This ensures the desired balanced shape of the heap-labeled tree, but does not maintain the dominance ordering of the keys.
		- The new key might be less than its parent in a min-heap, or greater than its parent in a max-heap.

- The solution is to swap any such dissatisfied element with its parent.
	- The old parent is now happy, because it is properly dominated.
	- The other child of the old parent is still happy, because it is now dominated by an element even more extreme than before.
	- The new element is now happier, but may still dominate its new parent. So we recur at a higher level, *bubbling up* the new key to its proper position in the hierarchy.
- Since we replace the root of a subtree by a larger one at each step, we preserve the heap order elsewhere.

```C
void pq_insert(priority_queue *q, item_type x) {
	if (q->n >= PQ_SIZE) {
		printf("Warning: priority queue overflow!\n");
	} else {
		q->n = (q->n) + 1;
		q->q[q->n] = x;
		bubble_up(q, q->n);
	}
}

void bubble_up(priority_queue *q, int p) {
	if (pq_parent(p) == -1) {
		return;    // At root of heap, no parent
	}

	if (q->q[pq_parent(p)] > q->q[p]) {
		pq_swap(q, p, pq_parent(p));
		bubble_up(q, pq_parent(p));
	}
}
```

- This swap process takes constant time at each level.
- Since the height of an *n*-element heap is $[\lg n]$, each insertion takes at most $O(\log n)$ time.
- A heap of *n* elements can thus be constructed in $O(n\log n)$ time through *n* such insertions:
```C
void pq_init(priority_queue *q) {
	q->n = 0;
}

void make_heap(priority_queue *q, item_type s[], int n) {
	int i;    // Counter

	pq_init(q);
	for (i = 0 ; i < n ; i++) {
		pq_insert(q, s[i]);
	}
}
```

### 4.3.3 Extracting the Minimum
- The remaining priority queue operations are identifying and deleting the dominant element.
	- Identification is easy, since the top of the heap sits in the first position of the array.
- Removing the top element leaves a hold in the array.
	- This can be filled by moving the element from the *right-most* leaf (sitting in the *n*th position of the array) into the first position.
- The shape of the tree has been restored but (as after insertion) the labeling of the root may no longer satisfy the heap property.
	- Indeed, this new root may be dominated by both of its children.

- The root of this min-heap should be the smallest of three elements, namely the current root and its two children.
	- If the current root is dominant, the heap order has been restored.
	- If not, the dominant child should be swapped with the root and the problem pushed down to the next level.

- This dissatisfied element *bubbles down* the heap until it dominates all its children, perhaps by becoming a leaf node and ceasing to have any.
- This percolate-down operation is also called *heapify*, because it merges two heaps (the subtrees below the original root) with the new key.

```C
item_type extract_min(priority_queue *q) {
	int min = -1;    // Min value

	if (q->n <= 0) {
		printf("Warning: Empty priority queue.\n");
	} else {
		min = q->q[1];

		q->q[1] = q->q[q->n];
		q->n = q->n - 1;
		bubble_down(q, 1);
	}
	return(min);
}

void bubble_down(priority_queue *q, int p) {
	int c;            // Child index
	int i;            // Counter
	int min_index;    // Index of lightest child

	c = pq_young_child(p);
	min_index = p;

	for (i = 0 ; i <= 1 ; i++) {
		if ((c + i) <= q->n) {
			if (q->q[min_index] > q->q[c + i]) {
				min_index = c + i;
			}
		}
	}

	if (min_index != p) {
		pq_swap(q, p, min_index);
		bubble_down(q, min_index);
	}
}
```

- We will reach a new leaf after $[\lg n]$ `bubble_down` steps, each constant time.
	- Thus, root deletion is completed in $O(\log n)$ time.

- Repeatedly exchanging the maximum element with the last element an calling heapify yields an $O(n \log n)$ sorting algorithm, named *heapsort*.
```C
void heapsort_(item_type s[], int n) {
	int i;            // Counter
	priority_queue q; // Heap for heapsort

	make_heap(&q, s, n);

	for (i = 0 ; i < n ; i++) {
		s[i] = extract_min(&q);
	}
}
```

- Heapsort is a great sorting algorithm.
	- It is simple to program.
	- It runs in worst-case $O(n \log n)$ time, which is the best that can be expected from any sorting algorithm.
	- It is an *in-place* sort, meaning it uses no extra memory over the array containing the elements to be sorted.
		- However, the implementation above is *not* in-place, as it creates the priority queue in `q`, not `s`. But each newly extracted element fits perfectly in the slot freed up by the shrinking heap, leaving behind a sorted array.
- Although other algorithms prove slightly faster in practice, you won't go wrong using heapsort for sorting data that sits in the computer's main memory.

### 4.3.4 - Faster Heap Construction (\*)
- As we have seen, a heap can be constructed on *n* elements by incremental insertion in $O(n\log n)$ time.
	- Surprisingly, heaps can be constructed even faster, by using our `bubble_down` procedure and some clever analysis.

- Suppose we pack the *n* keys destined for our heap into the first *n* elements of our priority-queue array.
	- The shape of our heap will be right, but the dominance order will be all messed up. How can we restore it?

- Consider the array in reverse order, starting from the last (*n*th) position.
	- It represents a leaf of the tree and so dominates its non-existent children.
		- The same is the case for the last *n*/2 positions in the array, because all are leaves.
	- If we continue to walk backwards through the array, we will eventually encounter an internal node with children.
		- This element may not dominate its children, but its children represent well-formed (if small) heaps.

- This is exactly the situation the `bubble_down` procedure was designed to handle, restoring the heap order of an arbitrary root element sitting on top of two sub-heaps.
- Thus, we can create a heap by performing *n*/2 non-trivial calls to the `bubble_down` procedure:
```C
void make_heap_fast(priority_queue *q, item_type s[], int n) {
	int i;    // Counter

	q->n = n;
	for (i = 0 ; i < n ; i++) {
		q->q[i + 1] = s[i];
	}

	for (i = q->n/2 ; i >= 1 ; i--) {
		bubble_down(q, i);
	}
}
```

- Multiplying the number of calls to `bubble_down` (*n*) times an upper bound on the cost of each operation ($O(\log n)$) gives us a running time analysis of $O(n\log n)$.
	- This would make it no faster than the incremental insertion algorithm described above.
- But note that is is indeed an *upper bound*, because only the last insertion will actually take $\lg n$ steps.
	- Recall that `bubble_down` takes time proportional to the height of the heaps it is merging.
		- Most of these heaps are extremely small.
	- In a full binary tree on *n* nodes, there are *n*/2 nodes that area leaves (i.e. height 0), *n*/4 nodes that are height 1, *n*/8 nodes that are height 2, and so on.
- In general, there are at most \[$n/2^{h+1}$] nodes of height *h*, so the cost of building a heap is: $\sum^{\lg n}_{h=0} [n/2^{h+1}] h \leq n \sum^{\lg n}_{h=0} h/2^h \leq 2n$
- Since this sum is not quite a geometric series, we can't apply the usual identity to get the sum, but rest assured that the puny contribution of the numerator (*h*) is crushed by the denominator $(2^h)$.
	- The series quickly converges to linear.

##### Stop and Think: Where in the Heap?
- *Problem*:
	- Given an array-based heap on *n* elements and a real number *x*, efficiently determine whether the *k*th smallest element in the heap is greater than or equal to *x*.
	- Your algorithm should be $O(k)$ in the worst case, independent of the size of the heap.
	- (Hint: You do not need to find the *k*th smallest element; you only need to determine its relationship to *x*).

- *Solution*:
	- There are at least two different ideas that lead to correct but inefficient algorithms for this problem:
		- Call extract-min *k* times, and test whether all of these are less than *x*.
			- This explicitly sorts the first *k* elements and so gives us more information than the desired answer, but it takes $O(k\log n)$ time to do so.
		- The *k*th smallest element cannot be deeper than the *k*th level of the heap, since the path from it to the root must go through elements of decreasing value.
			- We can thus look at all the elements on the first *k* levels of the heap, and count how many of them are less than *x*, stopping when we either find *k* of them or run out of elements.
			- This is correct, but takes $O(min(n,2^k))$ time, since the top *k* elements have $2^k - 1$ elements.
		- An $O(k)$ solution can look at only *k* elements smaller than *x*, plus at most $O(k)$ elements greater than *x*.
		- Consider the following recursive procedure, called at the root with $i=1$ and *count* = *k*:
```C
int heap_compare(priority_queue *q, int i, int count, int x) {
	if (count <= 0 || i > q->n) {
		return(count);
	}

	if (q->q[i] < x) {
		count = heap_compare(q, pq_young_child(i), count - 1, x);
		count = heap_compare(q, pq_young_child(i) + 1, count, x);
	}

	return(count);
}
```
- If the root of the min-heap is $\geq$ *x*, then no elements in the heap can be less than *x*, as by definition the root must be the smallest element.
	- This procedure searches the children of all nodes of weight smaller than *x* until either:
		- We have found *k* of them, when it returns 0; or
		- They are exhausted, when it returns a value greater than zero.
	- Thus, it will find enough small elements if they exist.
- But how long does it take?
	- The only nodes whose children we look at are those < *x*, and there are at most *k* of these in total.
	- Each have visited at most two children, so we visit at most $2k+1$ children, for a total time of $O(k)$.

### 4.3.5 - Sorting by Incremental Insertion
- Now consider a different approach to sorting via efficient data structures.
- Select the next element from the unsorted set, and put it into it's proper position in the proper set:
```C
for (i = 1 ; i < n ; i++) {
	j = 1;
	while (j > 0 && s[j] < s[j - 1]) {
		swap(&s[j], $s[j - 1]);
		j = j - 1;
	}
}
```
- Although insertion sort takes $O(n^2)$ in the worst case, it performs considerably better if the data is almost sorted, since few iterations of the inner loop suffice to sift it into the proper position.
- Insertion sort is perhaps the simplest example of the *incremental insertion* technique, where we build up a complicated structure on *n* items by first building it on *n - 1* items, and then making the necessary changes to add the last item.

- Note that faster sorting algorithms based on incremental insertion follow from more efficient data structures.
- Insertion into a balanced search tree takes $O(\log n)$ per operation, or a total of $O(n\log n)$ to construct the tree.
	- A in-order traversal reads through the elements in sorted order to complete the job in linear time.