#Chapter_4
- I will review several sorting algorithms and their complexities over the course of this chapter.
	- But the punch line is this: clever sorting algorithms exist that run in $O(n \log n)$ time.
	- This is a *big* improvement over naive $O(n^2)$ sorting algorithms, for large values of *n*.

- Consider the number of steps done by two different sorting algorithms for reasonable values of *n*:
![[Pasted image 20231114153727.png]]
- You might survive using a quadratic-time algorithm even if $n=10,000$, but the slow algorithm clearly gets ridiculous once $n\geq 100,000$. 

- Many important problems can be reduced to sorting, so we can use our clever $O(n \log n)$ algorithms to do work that might otherwise seem to require a quadratic algorithm.
- An important algorithm design technique is to use sorting as a basic building block, because many other problems become easy once a set of items is sorted.

- Consider the following applications:
	- *Searching*:
		- Binary search tests whether an item is in a dictionary in $O(\log n)$ time, provided the keys are all sorted.
		- Search preprocessing is perhaps the single most important application of sorting.
	- *Closest Pair*:
		- Given a set of *n* numbers, how do you find the pair of numbers that have the smallest difference between them?
		- Once the numbers are sorted, the closest pair of numbers must lie next to each other somewhere in sorted order.
		- Thus, a linear-time scan through the sorted list completes the job, for a total of $O(n \log n)$ time including the sorting.
	- *Element Uniqueness*:
		- Are there any duplicates in a given set of *n* items?
		- This is a special case of the closest-pair problem, where now we ask if there is a pair separated by a gap of zero.
		- An efficient algorithm sorts the numbers and then does a linear scan checking all adjacent pairs.
	- *Finding the Mode*:
		- Given a set of *n* items, which element occurs the largest number of times in the set?
			- If the items are sorted, we can sweep from left to right and count the number of occurrences of each element, since all identical items will be lumped together after sorting.
		- To find out how often an arbitrary element *k* occurs, look up *k* using binary search in a sorted array of keys.
			- By walking to the left of this point until the first element is not *k*, then doing the same to the right, we can find this count in $O(\log n+c)$ time, where *c* is the number of occurrences of *k*.
		- Even better, the number of instances of *k* can be found in $O(\log n)$ time by using binary search to look for the positions of both $k-\epsilon$ and $k+\epsilon$ (where $\epsilon$ is suitably small), and then taking the difference of these positions.
	- *Selection*:
		- What is the *k*th largest item in an array?
		- If the keys are placed in sorted order, the *k*th largest item can be found in constant time as it must sit in the *k*th position of the array.
		- In particular, the median element appears in the (*n*/2)th position in sorted order.
	- *Convex Hulls*:
		- What is the polygon of smallest perimeter that contains a given set of *n* points in two dimensions?
			- The convex hull is like a rubber band stretched around the points in the plane and then released.
			- It shrinks to just enclose the points, as shown in Figure 4.1(L).
			- The convex hull gives a nice representation of the shape of the point set and is an important building block for more sophisticated geometric algorithms.
		- But how can we use sorting to construct the convex hull?
			- Once you have the point sorted by *x*-coordinate, the points can be inserted from left to right into the hull.
			- Since the right-most point is always on the boundary, we know that i must appear in the hull.
			- Adding this new right-most point may cause others to be deleted, but we can quickly identify these points because they lie inside the polygon formed by adding a new point.
			- See the example in Figure 4.1(R). These points will be neighbours of the previous point we inserted, so they will be easy to find and delete.
			- The total time is linear after the sorting has been done.![[Pasted image 20231114155741.png]]

- While a few of these problems (namely median and selection) can be solved in linear time using more sophisticated algorithms, sorting provides quick and easy solutions to all of these problems.
- It is a rare application where the running time of sorting proves to be the bottleneck, especially a bottleneck that could have otherwise been removed using more clever algorithmics.
- Never be afraid to spend time sorting, provided you use an efficient sorting routine.

- ***Take-Home Lesson*: Sorting lies at the heart of many algorithms. Sorting the data is one of the first things any algorithm designer should try in the quest for efficiency.**

##### Stop and Think: Finding the Intersection
- ***Problem*:**
	- Give an efficient algorithm to determine whether two sets (of size *m* and *n*, respectively) are disjoint. Analyze the worst-case complexity in terms of *m* and *n*, considering the case where $m \ll n$.

- ***Solution*:**
	- At least three algorithms come to mind, all of which are variants of sorting and searching:
		- *First sort the big set*:
			- The big set can be sorted in $O(n\log n)$ time.
			- We can now do a binary search with each of the *m* elements in the second, looking to see if it exists in the big set.
			- The total time will be $O((n+m)\log n)$.
		- *First sort the small set*:
			- The small set can be sorted in $O(m\log m)$ time.
			- We can now do a binary search with each of the the *n* elements in the big set, looking to see if it exists in the small one.
			- The total time will be $O((n+m)\log m)$.
		- *Sort both sets*:
			- Observe that once the two sets are sorted, we no longer have to do a binary search to detect a common element.
			- We can compare the smallest element of the two sorted sets, and discard the smaller one if they are not identical.
			- By repeating this idea recursively on the now smaller sets, we can test for duplication in linear time after sorting.
			- The total cost is $O(n\log n + m\log m + n + m)$.
	- So which of these is the fastest method?
		- Clearly small-set sorting trumps big-set sorting, since $\log m < \log n$ when $m < n$.
		- Similarly, $(n+m)\log m$ must be asymptotically smaller than $n \log n$, since $n+m < 2n$ when $m < n$.
		- Thus, sorting the small set of the best of these options.
			- Note that this is linear when *m*is constant in size.
	- Note that *expected* linear time can be achieved by hashing.
		- Build a hash table containing the elements of both sets, and then explicitly check whether collisions in the same bucket are in fact identical elements.
		- In practice, this may be the best solution.

##### Stop and Think: Making a Hash of the Problem?
- ***Problem:***
	- Fast sorting is a wonderful thing. But which of these tasks can be done as fast or faster (in expected time) using hashing instead of sorting)?
		- *Searching*?
		- *Closest pair*?
		- *Element uniqueness*?
		- *Finding the mode*?
		- *Finding the median*?
		- *Convex hull*?

- ***Solution:***
	- Hashing can solve some of these problems efficiently, but is inappropriate for others. Let's consider them one by one:
		- *Searching*:
			- Hash tables are a great answer here, enabling you to search for items in constant expected time, as opposed to $O(\log n)$ with binary search.
		- *Closest Pair*:
			- Hash tables as so far defined cannot help at all.
			- Normal hash functions scatter keys around the table, so a pair of similar numerical values are unlikely to end up in the same bucket for comparison.
			- Bucketing values by numerical ranges will ensure that the closest pair lie within the same bucket, or at worst neighboring buckets.
			- But we cannot also force only a small number of items to lie in this bucket, as will be discussed with respect to bucketsort in Section 4.7.
		- *Element Uniqueness*:
			- Hashing is even faster than sorting for this problem.
			- Build a hash table using chaining, and then compare each of the (expected constant) pairs of items within a bucket.
			- If no bucket contains a duplicate pair, then all the elements must be unique.
			- The table construction and sweeping can be completed in linear expected time.
		- *Finding the Mode*:
			- Hashing leads to a linear expected-time algorithm here.
			- Each bucket should contain a small number of *distinct* elements, but may have many duplicates.
			- We start from the first element in a bucket and count/delete all copies of it, repeating this sweep the expected constant number of passes until the bucket is empty.
		- *Finding the Median*:
			- Hashing does not help us, I am afraid.
			- The median might be in any bucket of our table, and we have no way to judge how many items lie before or after it in sorted order.
		- *Convex Hull*:
			- Sure, we can build a hash table on points just as well as any other data type.
			- But it isn't clear what good that does us for this problem: certainly it can't help us order the points by *x*-coordinate.