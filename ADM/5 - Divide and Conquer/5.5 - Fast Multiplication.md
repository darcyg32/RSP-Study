#Chapter_5 
- One way to multiply two *n*-digit base-10 numbers *A* and *B* involve adding up *B* copies of *A*, which gives an $O(n*10^n)$ time algorithm.
- Another way is to multiply long numbers on a digit-by-digit basis, like $9256 * 5367 = 9256 * 7 + 9256 * 60 + 9256 * 300 + 9256 * 5000 = 13, 787, 823$. This algorithm gives an $O(n^2)$ time complexity.
- This section will provide an even faster algorithm.

- Suppose each number has $n=2m$ digits.
- Observe that we can split each number into two pieces each of *m* digits, such that the product of the full numbers can easily be constructed from the products of the pieces, as follows.
	- Let $w=10^{m+1}$ (for the current number), and represent $A=a_0 + a_1w$ and $B=b_0 + b_1w$, where $a_i$ and $b_i$ are the pieces of each respective number.
		- Then: $A*B=(a_0+a_1w)*(b_0+b_1w)=a_0b_0+a_0b_1w+a_1b_0w+a_1b_1w^2$.

- This procedure reduces the problem of multiplying two *n*-digit numbers to four products of $(n/2)$-digit numbers.
	- Recall that multiplication by *w* doesn't count: it is simply padding the product with zeros.
	- We also have to add together these four products once computed, which is $O(n)$ work.

- Let $T(n)$ denote the amount of time it takes to multiply two *n*-digit numbers.
	- Assuming we use the same algorithm recursively on each of the smaller products, the running time of this algorithm is given by the recurrence: $T(n)=4T(n/2)+O(n)$.
- Using the master theorem (case 1), we see that this algorithm runs in $O(n^2)$ time, exactly the same as the digit-by-digit method.
	- We divided, but we did not conquer.

- *Karatsuba's Algorithm* is an alternative recurrence for multiplication, which yields a better running time.
- Suppose we compute the following *three* products:
	- $q_0=a_0b_0$
	- $q_1=(a_0+a_1)(b_0+b_1)$
	- $q_2=a_1b_1$
- Note that  $A * B = (a_0 + a_1w) * (b_0 + b_1w) = a_0b_0 + a_0b_1w + a_1b_0w + a_1b_1w^2$ 
				 $= q_0+(q_1-q_0-q_2)w+q_2w^2$

- So now we have computed the full product with just three half-length multiplications and a constant number of additions.
	- Again, the *w* terms don't count as multiplications: recall that they are really just zero shifts.

- The time complexity of this algorithm is therefore governed by the recurrence $T(n)=3T(n/2)+O(n)$.
	- Since $n=O(n^{\log_23})$, this is solved by the first case of the master theorem, and $T(n)=\Theta(n^{\log_23})=\Theta(n^{1.585})$.
	- This is a substantial improvement.

- This approach of defining a recurrence that uses fewer multiplications but more additions also lurks behind fast algorithms for matrix multiplication.
	- The nested-loops algorithm for matrix multiplication discussed in [[2.5 - Reasoning About Efficiency|Section 2.5.4]] takes $O(n^3)$ for two *n x n* matrices, because we compute the dot product of *n* terms for each of the $n^2$ elements in the product matrix.
	- However, Strassen discovered a divide-and-conquer algorithm that manipulates the products of seven *n/2 x n/2* matrix products to yield the product of two *n x n* matrices.
	- This yields a time-complexity recurrence $T(n)=7T(n/2)+O(n^2)$.
		- Because $\log_27\approx 2.81$, $O(n^{\log_27})$ dominates $O(n^2)$, so Case 1 of the master theorem applies and $T(n)=\Theta(n^{2.81})$.
	- This algorithm has been repeatedly "improved" by increasingly complicated recurrences, and the current best is $O(n^{2.3727})$.