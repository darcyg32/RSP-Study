#Chapter_5 
- Divide-and-conquer recurrences of the form $T(n)=aT(n/b)+f(n)$ are generally easy to solve, because the solutions typically fall into one of three distinct cases:
	- If $f(n)=O(n^{\log_ba-\epsilon})$ for some constant $\epsilon>0$, then $T(n)=\Theta(n^{\log_ba})$.
		- That is, if $f(n)=O(n^c)$ for some constant $c<\log_ba$, then $T(n)=\Theta(n^{\log_ba})$.
	- If $f(n)=\Theta(n^{\log_ba})$, then $T(n)=\Theta(n^{\log_ba}\lg n)$.
	- If $f(n)=\Omega(n^{\log_ba+\epsilon})$ for some constant $\epsilon>0$, and if $a*f(n/b)\leq c*f(n)$ for some $c<1$, then $T(n)=\Theta(f(n))$.
		- That is, if $f(n)=\Omega(n^c)$ for some constant $c>\log_ba$, and if $a*f(n/b)\leq k*f(n)$ for some constant $k<1$, then $T(n)=\Theta(f(n))$.

- The issue is identifying which case of this so-called *master theorem* holds for your given recurrence.
	- Case 1 holds for heap construction and matrix multiplication.
	- Case 2 holds for mergesort.
	- Case 3 generally arises with clumsier algorithms, where the cost of combining the subproblems dominates everything.

- The master theorem can be though of as a black-box piece of machinery, invoked as needed and left with its mystery intact. However, after a little study it becomes apparent why the master theorem works.

![[Pasted image 20231125120047.png]]

- Figure 5.2 shows the recursion tree associated with a typical $T(n)=aT(n/b)+f(n)$ divide-and-conquer algorithm.
	- Each problem of size *n* is decomposed into *a* problems of size *n/b*.
	- Each subproblem of size *k* takes $O(f(k))$ time to deal with internally, between partitioning and merging.
	- The total time for the algorithm is the sum of these internal evaluation costs, plus the overhead of building the recursion tree.
	- The height of this tree is $h=\log_bn$.
	- The number of leaf nodes is $a^h=a^{\log_bn}$, which happens to simplify to $n^{\log_ba}$, with some algebraic manipulation.

- The three cases of the master theorem correspond to three different costs, each of which might be dominant as a function of *a*, *b*, and $f(n)$:
	- *Case 1: Too many leaves*:
		- If the number of leaf nodes outweigh the overall internal evaluation cost, the total running time is $O(n^{\log_ba})$.
	- *Case 2: Equal work per level*:
		- As we move down the tree, each problem gets smaller but there are more of them to solve.
		- If the sums of the internal evaluation costs at each level are equal, the total running time is the cost per level ($n^{\log_ba}$) times the number of levels ($\log_bn$), for a total running time of $O(n^{\log_ba}\lg n)$.
	- *Case 3: Too expensive a root*:
		- If the internal evaluation grows very rapidly with *n*, then the cost of the root evaluation may dominate everything.
		- Then the total running time is $O(f(n))$.

- Once you accept the master theorem, you can easily analyze any divide-and-conquer algorithm, given only the recurrence associated with it.
- We use this approach on several problems (see [[5.5 - Fast Multiplication]], [[5.6 - Largest Subrange and Closest Pair]])