#Chapter_5 
- The mother of all divide-and-conquer algorithms is *binary search*, which is a fast algorithm for searching in a sorted array of keys *S*.
	- To search for key *q*, we compare *q* to the middle key $S[n/2]$.
		- If *q* appears before $S[n/2]$, it must reside in the left half of *S*.
		- If not, it must reside in the right half of *S*.
	- By repeating this process recursively on the correct half, we locate the key in a total of $[\lg n]$ comparisons, a big win over the $n/2$ comparisons expected using sequential search.

```C
int binary_search(item_type s[], item_type key, int low, int high) {
	int middle;    // Index of middle element

	if (low > high) {
		return -1;
	}

	middle = (low + high) / 2;

	if (s[middle] == key) {
		return middle;
	}

	if (s[middle > key]) {
		return binary_search(s[], key, low, middle - 1);
	} else {
		return binary_search(s[], key, middle + 1, high);
	}
}
```

### 5.1.1 - Counting Occurrences
- Several interesting algorithms are variants of binary search.

- Suppose that we want to count the number of times a given key *k* occurs in a given sorted array.
	- Because sorting groups all the copies of *k* into a contiguous block, the problem reduces to finding that block and then measuring its size.

- The binary search routine presented above enables us to find the index of an element *x* of the correct block in $O(\lg n)$ time.
	- A natural way to identify the boundaries of the block is to sequentially test elements to the left of *x* until we find one that differs from the search key, and then repeat this search to the right of *x*.
		- The difference between the indices of these boundaries (plus one) gives the number of occurrences of *k*.
- This algorithm runs in $O(\lg n + s)$ time, where *s* is the number of occurrences of the key.
	- But this can be as bad as $\Theta(n)$ if the entire array consists of identical keys.

- A faster algorithm results by modifying binary search to find the *boundary*of the block containing *k*, instead of *k* itself.
	- Suppose we delete the equality test `if (s[middle] == key) return middle;` from the implementation above and return the index `high` instead of -1 on each unsuccessful search.
		- *All* searches will not be unsuccessful, since there is no equality test.
			- The search will proceed to the right half whenever a key is compared to an identical array element, eventually terminating at the right boundary.
			- Repeating the search after reversing the direction of the binary comparison will lead us to the left boundary.
	- Each search takes $O(\lg n)$ time, so we can count the occurrences in logarithmic time regardless of the size of the block.

- By modifying our binary search routine to return `(low+high)/2` instead of -1 on an unsuccessful search, we obtain the location between two array elements where the key *k* should have been.
	- This variant suggests another way to solve our length of run problem.
		- We search for the positions of keys $k-\epsilon$ and $k+\epsilon$, where $\epsilon$ is a tiny enough constant that both searches are guaranteed to fail with no intervening keys.
		- Again, doing two binary searches takes $O(\log n)$ time.

### 5.1.2 - One-Sided Binary Search
- Now suppose we have an array *A* consisting of a run of 0's, following by an unbounded run of 1's, and would like to identify the exact point of transition between them.

- Binary search on the array would find the transition point in $[\lg n]$ tests, if we had a bound *n* on the number of elements in the array.

- But in the absence of such a bound, we can test repeatedly at larger intervals $(A[1], A[2], A[4], A[8], A[16], ...)$, until we find a non-zero value.
	- Now we have a window containing the target and can proceed with binary search.

- This *one-sided binary search* finds the transition point *p* using at most $2[\lg p]$ comparisons, regardless of how large the array actually is.
- One-sided binary search is useful whenever we are looking for a key that lies close to our current position.

### 5.1.3 - Square and Other Roots
- The square root of *n* is the positive number *r* such that $r^2=n$.

- First, observe that the square root of $n\geq 1$ must be at least 1 and at most *n*.
	- Let $l=1$ and $r=n$.
	- Consider the midpoint of this interval, $m=(l+r)/2$.
	- How does $m^2$ compare to *n*?
		- If $n\geq m^2$, then the square root must be greater than *m*, so the algorithm repeats with $l=m$.
		- If $n<m^2$, then the square root must be less than *m*, so the algorithm repeats with $r=m$.
	- Either way, we have halved the interval using only one comparison.
	- Therefore, after $[\lg n]$ rounds, we will have identified the square root to within $\pm 1/2$.

- This *bisection method*, as it is called in numerical analysis, can also be applied to the more general problem of finding the roots of an equation.
	- We say that *x* is a *root* of a function $f$ if $f(x)=0$.
	- Suppose that we start with values *l* and *r* such that $f(l)>0$ and $f(l)<0$.
		- If $f$ is a continuous function, there must exist a root between *l* and *r*.
		- Depending upon the sign of $f(m=(l+r)/2)$, we can cut the window containing the root in half with each test, and stop as soon as our estimate becomes sufficiently accurate.

- Root-finding algorithms converging faster than binary search are known for both of these problems.
	- Instead of always testing the midpoint of the interval, these algorithms interpolate to find a test point closer to the actual root.
- Still, binary search is simple, robust, and works as well as possible without additional information on the nature of the function to be computed.