#Chapter_5
- One of the most powerful techniques for solving problems is to break them down into smaller, more easily solved pieces.
	- Smaller problems are less overwhelming, and permit us to focus on details that are lost when we are studying the whole thing.
- A recursive algorithm starts to become apparent whenever we can break the problem into smaller instances of the same type of problem.

- Two important algorithm design paradigms are based on breaking problems down into smaller problems.
	- In Chapter 10, we will see dynamic programming, which typically removes one element from the problem, solves the smaller problem, and then adds back the element to the solution of this smaller problem in the proper way.
	- *Divide and conquer* instead splits the problem into (say) halves, solves each half, then stitches the pieces back together to form a full solution.

- Thus, to use divide and conquer as an algorithm design technique, we must divide the problem into two smaller subproblems, solve each of them recursively, and then meld the two partial solutions into one solution to the full problem.
	- Whenever the merging takes less time than solving the two subproblems, we get an efficient algorithm.
- Mergesort, discussed in [[4.5 - Mergesort - Sorting by Divide and Conquer|Section 4.5]], is the classic example of a divide-and-conquer algorithm.
	- It takes only linear time to merge two sorted lists of $n/2$ elements, each of which was obtained in $O(n\lg n)$ time.

- Divide and conquer is a design technique with many important algorithms to its credit, including mergesort, the fast Fourier transform, and Strassen's matrix multiplication algorithm.
	- Beyond binary search and its many variants, however, I find it to be a difficult design technique to apply in practice.
- Our ability to analyze divide and conquer algorithms rests on our proficiency in solving the recurrence relations governing the cost of such recursive algorithms, so we will introduce techniques for solving recurrences here.