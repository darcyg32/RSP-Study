#Chapter_5 
- The *convolution* of two arrays (or vectors) *A* and *B* is a new vector *C* such that $C[k]=\sum_{j=0}^{m-1}A[j]*B[k-j]$.

- If we assume that *A* and *B* are of length *m* and *n* respectively, and indexed starting from 0, the natural range on *C* is from $C[0]$ to $C[n+m-2]$.
	- The values of all out-of-range elements of *A* and *B* are interpreted as zero, so they do not contribute to any product.

- An example of convolution is *polynomial multiplication*.
- Recall the problem of multiplying two polynomials, for example:
$(3x^2 + 2x + 6) * (4x^2 + 3x + 2) = (3 · 4)x^4 + (3 · 3+2 · 4)x^3 + (3 · 2+2 · 3+6 · 4)x^2 + (2 · 2+6 · 3)x^1 + (6 · 2)x^0$
- Let $A[i]$ and $B[i]$ denote the coefficients of $x^i$ in each of the polynomials.
	- Then multiplication is a convolution, because the coefficient of the $x^k$ term in the product polynomial is given by the convolution $C[k]$ above.
	- This coefficient is the sum of the products of all terms which have exponent pairs adding to *k*: for example, $x^5=x^4*x^1=x^3*x^2$.

- The obvious way to implement convolution is by computing the *m* term dot product $C[k]$ for each $0\le k\le n+m-2$.
	- This is two nested loops, running in $\Theta(nm)$ time.
		- The inner loop does not always involve *m* iterations because of boundary conditions.
		- Simpler loop bounds could have been employed if *A* and *B* were flanked by ranges of zeros.

```C
for (i = 0; i < n + m - 1; i++) {
	for (j = max(0, i - (n - 1)); j <= min(m - 1, i); j++) {
		c[i]=c[i] + a[j] * b[i - j];
	}
}
```

- Convolution multiplies every possible pair of elements from *A* and *B*, and hence it seems like we *should* require quadratic time to get these $n+m-1$ numbers.
	- But there exists a clever divide-and-conquer algorithm that runs in $O(n\log n)$ time, assuming that $n\ge m$.

### 5.9.1 - Applications of Convolution
- Going from $O(n^2)$ to $O(n\log n)$ is as big a win for convolution as it was for sorting.
- Taking advantage of it requires recognizing when you are doing a convolution operation.

- Convolutions often arise when you are trying all possible ways of doing things that add up to *k*, for a large range of values of *k*, or when sliding a mask or pattern *A* over a sequence *B* and calculating at each position.

- Important examples of convolution operations include:
	- *Integer Multiplication*:
		- We can interpret integers as polynomials in any base *b*.
			- For example, $632=6*b^2+3*b^1+2*b^0$, where $b=10$.
			- Polynomial multiplication behaves like integer multiplication without carrying.
		- There are two different ways we can use fast polynomial multiplication to deal with integers.
			- First, we can explicitly perform the carrying operation on the product polynomial, adding $[C[i]/b]$ to $C[i+1]$, and then replacing $C[i]$ with $C[i]$ (mod *b*).
			- Alternatively, we could compute the product polynomial and then evaluate it at *b* to get the integer product *A x B*.
		- With fast convolution, either way gives us an even faster multiplication algorithm than Karatsuba, running in $O(n \log n)$ time.
	- *Cross-Correlation*:
		- For two time series *A* and *B*, the cross-correlation function measures the similarity as a function of the shift or displacement of one relative to the other.
			- Perhaps people buy a product on average *k* days after seeing an advertisement for it. Then there should be high correlation between sales and advertising expenditures lagged by *k* days.
		- This cross-correlation function $C[k]$ can be computed: $C[k]=\sum_j A[j]B[j+k]$.
			- Note that the dot product here is computed over backward shifts of *B* instead of forward shifts, as in the original definition of a convolution.
				- But we can still use fast convolution to compute this: simply input the reversed sequence $B^R$ instead of *B*.
	- *Moving Average Filters*:
		- Often we are tasked with smoothing time series data by averaging over a window.
			- Perhaps we want $C[i-1]=0.25B[i-1]+0.5B[i]+0.25B[i+1]$ over all positions *i*.
				- This is just another convolution, where *A* is the vector of weights within the window around $B[i]$.
	- *String Matching*:
		- We are given a text string *S* and a pattern string *P*, and seek to identify all locations in *S* where *P* may be found. For *S = abaababa* and *P = aba*, we can find *P* in *S* starting at positions 0, 3, and 5.
		- The $O(mn)$ algorithm described in [[2.5 - Reasoning About Efficiency|Section 2.5.3]] works by sliding the length-*m* pattern over each of the *n* possible starting points in the text.
		- This sliding window approach is suggestive of being a convolution with the reversed pattern $P^R$, as shown in Figure 5.5. Can we solve string matching in $O(n \log n)$ by using fast convolution?
			- The answer is yes! Suppose our strings have an alphabet of size α. We can represent each character by a binary vector of length α having exactly one non-zero bit. Say *a* = 10 and *b* = 01 for the alphabet *{a, b}*. Then we can encode the strings *S* and *P* above as:
				- $S = 1001101001100110$
				- $P = 100110$
			- The dot product over a window will be *m* on an even-numbered position of *s* iff p starts at that position in the text. 
			- So fast convolution can identify all locations of *p* in *s* in $O(n log n)$ time.

- ***Take-Home Lesson:***
	- ***Learn to recognize possible convolutions.*** 
	- ***A magical $\Theta(n \log n)$ algorithm instead of $O(n^2)$ is your reward for seeing this.***