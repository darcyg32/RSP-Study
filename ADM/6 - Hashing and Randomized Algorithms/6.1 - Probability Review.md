#Chapter_6 
- This is not a thorough review.
- It is limited to a few basic definitions and properties we will use.

### 6.1.1 - Probability
- Probability theory provides a formal framework for reasoning about the likelihood of events.
- Because it is a formal discipline, there is a thicket of associated definitions to instantiate exactly what we are reasoning about.
	- An *experiment*:
		- Is a procedure that yields one of a set of possible outcomes.
		- As out ongoing example, consider the experiment of tossing two six-sided dice, one red and one blue, with each face bearing a distinct integer {1, ... 6}.
	- A *sample space S*:
		- Is the set of possible outcomes of an experiment.
		- In our dice example, there are 36 possible outcomes.
	- An *event E*:
		- Is a specified subset of the outcomes of an experiment.
		- The even that the sum of the dice equals 7 or 11 is the subset $E=\{(1, 6),(2, 5),(3, 4),(4, 3),(5, 2),(6, 1),(5, 6),(6, 5)\}$.
	- The *probability of an outcome s*:
		- Denoted $p(s)$, is a number with the two properties:
			- For each outcome *s* in sample space *S*, $0\leq p(s) \leq 1$.
			- The sum of probabilities of all outcomes adds to one: $\sum_{s\in S}p(s)=1$.
		- If we assume two distinct fair dice, the probability $p(s)=(1/6)*(1/6)=1/36$ for all outcomes $s\in S$.
	- The *probability of an event E*:
		- Is the sum of the probabilities of the outcomes of the event.
			- Thus, $P(E)=\sum_{s\in E}p(s)$.
		- An alternative formulation is in terms of the *complement* of the event $\bar{E}$, the case where *E* does not occur. Then
			- $P(E)=1-P(\bar{E})$
		- This is useful, because often it is easier to analyze $P(\bar{E})$ than $P(E)$ directly.
	- A *random variable V*:
		- Is a numerical function on the outcomes of a probability space.
		- The function "sum the values of two dice" ($V(a,b)=a+b$) produces an integer result between 2 and 12.
			- This implies a probability distribution of the possible values of the random variable.
			- The probability $P(V(s)=7)=1/6$, while $P(V(s)=12)=1/36$.
	- The *expected value*:
		- Of a random variable *V* defined on a sample space *S*, *E(V)*, is defined $E(V)=\sum_{s\in S}p(s)*V(s)$

### 6.1.2 - Compound Events and Independence
![[Pasted image 20231126104348.png]]
- We will be interested in complex events computed from simpler events *A* and *B* on the same set of outcomes.
	- Perhaps event *A* is that at least one of two dice be an even number, while event *B* denotes rolling a total of either 7 or 11.
	- Note that there exist outcomes of *A* that are not outcomes of *B*, specifically: $A - B = \{(1, 2),(1, 4),(2, 1),(2, 2),(2, 3),(2, 4),(2, 6),(3, 2),(3, 6),(4, 1)\}$                                                  $\{(4, 2),(4, 4),(4, 5),(4, 6),(5, 4),(6, 2),(6, 3),(6, 4),(6, 6)\}$

- This is the *set difference* operation.
	- Observe here that $B-A=\{\}$, because every pair adding to 7 or 11 must contain one odd and one even number.

- The outcomes in common between both events *A* and *B* are called the *intersection*, denoted $A\cap B$.
	- This can be written as: $A\cap B=A-(S-B)$.

- Outcomes that appear in either *A* or *B* are called the *union*, denoted $A\cup B$.

- The probability of the union and intersection are related by the formula: $P(A\cup B)=P(A)+P(B)-P(A\cap B)$.

- With the complement operation $\bar{A}=S-A$, we get a rich language for combining events, shown in Figure 6.1.
	- We can readily compute the probability of any of these sets by summing the probabilities of the outcomes in the defined sets.

- The events *A* and *B* are said to be *independent* if $P(A\cap B)=P(A)*P(B)$.
	- This means that there is no special structure of outcomes shared between events *A* and *B*.
		- For example, assuming that half the students in a class are female, and half the students in the class are above average, we would expect that a quarter of the students are both female and above average if the events are independent.

- Probability theorists love independent events, because it simplifies their calculations.
	- For example, if $A_i$ denotes the event of getting an even number on the *i*th dice throw, then the probability of obtaining all events in a throw of two dice is $P(A_1\cap A_2) = P(A_1)P(A_2)=(1/2)(1/2)=1/4$.
	- Then, the probability of *A*, that at least one of the two dice is even, is $P(A) = P(A_1 \cup A_2) = P(A_1) + P(A_2) − P(A_1 \cap A_2)=1/2+1/2 - 1/4=3/4$.

- The fact that independence often doesn't hold explains much of the subtlety and difficulty of probabilistic analysis.
	- The probability of getting *n* heads when tossing *n* independent coins is $1/2^n$. 
	- But it would be 1/2 if the coins were perfectly correlated, since the only possibilities would be all heads or all tails.
	- This computation would become very hard if there were complex dependencies between the outcomes of the *i*th and *j*th coins.

- Randomized algorithms are typically designed around samples drawn independently at random, so that we can safely multiply probabilities to understand compound events.

### 6.1.3 - Conditional Probability
- Presuming that $P(B)>0$, the *conditional probability* of *A* given *B*, $P(A|B)$ is defined as follows: $P(A|B)=\frac{P(A\cap B)}{P(B)}$.

- In particular, if events *A* and *B* are independent, then $P(A|B)=\frac{P(A\cap B)}{P(B)}=\frac{P(A)P(B)}{P(B)}=P(A)$, and *B* has absolutely no impact on the likelihood of *A*.
	- Conditional probability becomes interesting only when the two events have dependence on each other.

- Recall the dice-rolling events from Section 6.1.2, namely:
	- Event *A*: at least one of two dice is an even number.
	- Event *B*: the sum of the two dice is either 7 or 11.

- Observe that $P(A|B)=1$, because *any* roll summing to an odd value must consist of one even and one odd number. Thus, $A\cap B=B$.
	- For $P(B|A)$, note that $P(A\cap B)=P(B)=8/36$ and $P(A)=27/36$, so $P(B|A)=8/27$.

- Our primary tool to compute conditional probabilities will be *Bayes' theorem*, which reverses the direction of the dependencies:
	- $P(B|A)=\frac{P(A|B)P(B)}{P(A)}$

- Often it proves easier to compute probabilities in one direction than another, as in this problem.
	- By Bayes' theorem, $P(B|A) = (1 * 8/36)/(27/36) = 8/27$, exactly what we got before.

### 6.1.4 - Probability Distributions
- Random variables are numerical functions where the values are associated with probabilities of occurrence.
	- In our example where $V(s)$ is the sum of two tossed dice, the function produces an integer between 2 and 12.

- The probability of a particular value $V(s)=X$ is the sum of the probabilities of all the outcomes whose components add up to *X*.

- Such random variables can be represented by their *probability density function*, or *pdf*.
	- This is a graph where the *x*-axis represents the values the random variable can take on, and the *y*-axis denotes the probability of each given value.
- Figure 6.2 (left) presents the pdf of the sum of two fair dice.
	- Observe that the peak at $X=7$ corresponds to the most probable dice total, with a probability of 1/6.

![[Pasted image 20231126110506.png]]

### 6.1.5 - Mean and Variance
- There are two main types of summary statistics, which together tell us an enormous amount about a probability distribution of a data set:
	- *Central Tendency Measures*:
		- Which capture the center around which the random samples or data points are distributed.
	- *Variation* or *Variability Measures*:
		- Which describe the spread, that is, how far the random samples or data points can lie from the center.

- The primary centrality measure is the *mean*.
	- The mean of a random variable *V*, denoted *E(V)* and also known as the expected value, is given by $E(V)=\sum_{s\in S}V(s)p(s)$.
		- When the elementary events are all of equal probability, the *mean* or *average*, is computed as $\bar{X}=\frac{1}{n}\sum^{n}_{i=1}x_i$.

- The most common measure of variability is the *standard deviation* $\sigma$.
	- The standard deviation of a random variable *V* is given by $\sigma = \sqrt{E(V-E(V))^2}$.
	- For a data set, the standard deviation is computed from the sum of squared differences between the individual elements and the mean: $\sigma = \sqrt{\frac{\sum^{n}_{i=1}(x_i-\bar{X})^2}{n-1}}$.

- A related statistic, the *variance* $V=\sigma^2$, is the square of the standard deviation.
	- Sometimes it is more convenient to talk about variance than standard deviation, because the term is ten characters shorter. But they measure the same thing.

### 6.1.6 - Tossing Coins
- You probably have a fair degree of intuition about the distribution of the number of heads and tails when you toss a fair coin 10,000 times.
- You know that the expected number of heads in $n$ tosses, each with probability $p = 1/2$ of heads, is $pn$, or 5,000 for this example.
- You likely know that the distribution for $h$ heads out of $n$ is a binomial distribution, where $P(X-h)=\frac{\binom{n}{n}}{\sum^{n}_{x=0}\binom{n}{x}}=\frac{\binom{n}{h}}{2^n}$, and that is it is a bell-shaped symmetrical distribution about the mean.

- But you may not appreciate just how narrow this distribution is, as shown in Figure 6.3.
	- Sure, anywhere from 0 to *n* heads can result from *n* fair coin tosses.
	- But they won’t: the number of heads we get will almost always be within a few standard deviations of the mean, where the standard deviation $\sigma$ for the binomial distribution is given by $\sigma = \sqrt{np(1-p)} = \Theta(\sqrt{n})$.

![[Pasted image 20231126112625.png]]

- Indeed, for any probability distribution, at least $1-(1/k^2)$ of the mass of the distribution lies within $\pm k\sigma$ of the mean $\mu$.
- Typically, $\sigma$ is small relative to $\mu$ for the distribution arising in the analysis of randomized algorithms and processes.

- ***Take-Home Lesson:***
	- ***Students often ask me “what happens” when randomized quicksort runs in $\Theta(n^2)$.***
		- ***The answer is that nothing happens, in exactly the same way nothing happens when you buy a lottery ticket: you almost certainly just lose.*** 
	- ***With a randomized quicksort you almost certainly just win: the probability distribution is so tight that you nearly always run in time very close to expectation.***

##### Stop and Think: Random Walks on a Path
![[Pasted image 20231126113151.png]]
- ***Problem:***
	- Random walks on graphs are important processes to understand. 
		- The expected covering time (the number of moves until we have visited all vertices) differs depending upon the topology of the graph (see Figure 6.4). What is it for a path?
	- Suppose we start at the left end of an m-vertex path. 
		- We repeatedly flip a fair coin, moving one step right on heads and one step left on tails (staying where we are if we were to fall off the path). 
	- How many coin flips do we expect it will take, as a function of m, until we get to the right end of the path?

- ***Solution***:
	- To get to the right end of the path, we need $m − 1$ more heads than tails after *n* coin flips, assuming we don’t bother to flip when on the left-most vertex where we can only move right.
	- We expect about half of the flips to be heads, with a standard deviation of $\sigma = \Theta(\sqrt{n})$.
		- This $\sigma$ describes the spread of the difference in the number of the heads and tails we are likely to have.
	- We must flip enough times for $\sigma$ to be on the order of *m*, so:
	- $m=\Theta(\sqrt{n})\to n=\Theta(m^2)$