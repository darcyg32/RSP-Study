#Chapter_6 
- Recall the problem of detecting duplicate documents faced by search engines like Google.
	- They seek to build an index of all the *unique* documents on the web.
	- Identical copies of the same document often exist on many different websites.
	- Whenever Google crawls a new link, they need to establish whether what they found is not a previously encountered document worth adding to the index.

- Perhaps the most natural solution here is to build a hash table of the documents.
- Should a freshly crawled document hash to an empty bucket, we know it must be new.
- But when there is a collision, it does not necessarily mean we have seen this document before.
	- To be sure, we must explicitly compare the new document against all other documents in the bucket, to detect spurious collisions between *a* and *b*, where $h(a)=s$ and $h(b)=s$, but $a\ne b$.
	- This is what was discussed in [[3.7 - Hashing|Section 3.7.2]].

- But in this application, spurious collisions are not really a tragedy: they only mean that Google will fail to index a new document it has found.
	- This can be an acceptable risk, provided the probability of it happening is low enough.

- Removing the need to explicitly resolve collisions has big benefits in making the table smaller.
	- By reducing each bucket from a pointer link to a single bit (is this bucket occupied or not?), we reduce the space by a factor of 64 on typical machines.
	- Some of this space can then be taken back to make the hash table larger, thus reducing the probability of collisions in the first place.

- Now suppose we build such a bit-vector hash table, with a capacity of *n* bits.
	- If we have distinct bits corresponding to *m* documents occupied, the probability that a new document will spuriously hash to one of these bits is $p=m/n$.
		- Thus, even if the table is only 5% full, there is still a $p=0.05$ probability that we will falsely discard a new discovery, which is much higher than is acceptable.

- Much better is to employ a *Bloom filter*, which is also just a bit-vector hash table.
- But instead of each document corresponding to a single position in the table, a Bloom filter hashes each key *k* times, using *k* different hash functions.
	- When we insert document *s* into our Bloom filter, we set all the bits corresponding to $h_1(s),h_2(s),...,h_k(s)$ to be 1, meaning occupied.
- To test whether a query document *s* is present in the data structure, we must test that all *k* of these bits equal 1.
	- For a document to be falsely convicted of already being in the filter, it must be unlucky enough that all *k* of these bits were set in hashes of previous documents, as in the example of Figure 6.6.

![[Pasted image 20231127135944.png]]

- What are the chances of this?
- Hashes of *m* documents in such a Bloom filter will occupy at most *km* bits, so the probability of a single collision rises to $p_1=km/n$, which is *k* times greater than the single hash case.
- But *all k* bits must collide with those of our query document, which only happens with probability $p_k=(p_1)^k=(km/n)^k$.
	- This becomes a peculiar expression, because a probability raised to the *k*th power quickly becomes smaller with increasing *k*, yet here the probability being raised simultaneously increases with *k*.
	- To find *k* that minimizes $p_k$, we could take the derivative and set it to zero.

- Figure 6.7 graphs this error probability ($(km/n)^2$) as a function of load ($m/n$), with a separate line for each *k* from 1 to 5.
	- It is clear that using a large number of hash functions (increased *k*) reduces false positive error substantially over a conventional hash table (the blue line, $k=1$), at least for small loads.
	- But observe that the error rate associated with larger *k* increases rapidly with load, so for any given load there is always a point where adding more hash functions becomes counter-productive.

![[Pasted image 20231127171157.png]]

- For a 5% load, the error rate for a simple hash table of $k=1$ will be 51.2 times larger than a Bloom filter with $k=5$ $(9.77\times 10^{-4})$, even though they use exactly the same amount of memory.
- A Bloom filter is an excellent data structure for maintaining an index, provided you can live with occasionally saying yes when the answer is no.