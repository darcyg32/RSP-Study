#Chapter_6 
- Hashing lets you quickly test whether a specific word *w*in document $D_1$ also occurs in document $D_2$: build a hash table on the words of $D_2$ and then hunt for $h(w)$ in this table *T*.
	- For simplicity and efficiency we can remove duplicate words from each document, so each contains only one entry for each vocabulary word used.
- By looking up all the vocabulary words $w_i\in D_1$ in *T*, we can get a count of the intersection, and compute the *Jaccard similarity* $J(D_1, D_2)$ of the two documents, where $J(D_1,D_2)=\frac{|D_1\cap D_2|}{|D_1\cup D_2|}$.
	- This similarity measure ranges from 0 to 1, sort of like a probability that the two documents are similar.

- But what if you want to test whether two documents are similar without looking at all the words?
	- If we are doing this repeatedly, on a web scale, efficiency matters.
	- Suppose we are allowed to look at only *one* word per document to make a decision. Which word should we pick?

- A first thought might be to pick the most frequent word in the original document, but it is likely to be "the" and tell you very little about similarity.
- Picking the most representative word, perhaps according to the TD-IDF statistic, would be better.
	- But it still makes assumptions about the distribution of words, which may be unwarranted.

- The key idea is to synchronize, so we pick the same word out of the two documents while looking at the documents separately.
- *Minwise hashing* is a clever but simple idea to do this.
	- We compute the hash code $h(w_i)$ of every word in $D_1$, and select the code with smallest value among all the hash codes. Then we do the same thing with the words in $D_2$, using the same hash function.

![[Pasted image 20231127205845.png]]

- What this does is it gives us a way to pick the same random word in both documents, as shown in Figure 6.10.
- Suppose the vocabularies of each document were identical.
	- Then the word with the minimum hash code will be the same in both $D_1$ and $D_2$, and we get a match.
	- In contrast, suppose we had picked completely random words from each document. Then the probability of picking the same word would be only $1/v$, where *v* is the vocabulary size.
- Now suppose that $D_1$ and $D_2$ do not have identical vocabularies.
	- The probability that the minhash word appears in both documents depends on the number of words in common, that is, the intersection of the vocabularies, and also on the total vocabulary size of the documents.
	- In fact, this probability is exactly the Jaccard similarity described above.

- Sampling a larger number of words, say, the *k* smallest hash values in each document, and reporting the size of intersection over *k* gives us a better estimate of Jaccard similarity.
	- But the alert reader may ask "why bother".
	- It takes time linear in the size of $D_1$ and $D_2$ to compute all the hash values just to find the minhash values, yet this is the same running time that it would take to compute the exact size of intersection using hashing!

- The value of minhash comes in building indexes for similarity search and clustering over large corpora of documents.
- Suppose we have *N* documents, each with an average of *m* vocabulary words in them.
- We want to build an index to help us determine which of these is most similar to a new query document *Q*.
	- Hashing all words in all documents gives us a table of size $O(Nm)$.
	- Storing $k\ll m$ minwise hash values from each document will be much smaller at $O(Nk)$, but the documents at the intersection of the buckets associated with the *k* minwise hashes of *Q* are likely to contain the most similar documents - particularly if the Jaccard similarity is high.

##### Stop and Think: Estimating Population Size
- Problem:
	- Suppose we will receive a stream *S* of *n* numbers one by one.
	- This stream will contain many duplicates, possibly even a single number repeated *n* times.
	- How can we estimate the number of distinct values in *S* using only a constant amount of memory?

- Solution:
	- If space was not an issue, the natural solution would be to build a dictionary data structure on the distinct elements from the stream, each with an associated count of how often it was occurred.
		- For the next element we see in the stream, we add one to the count if it exists in the dictionary, or insert it if it is not found.
		- But we only have enough space to store a constant number of elements. What can we do?
	- Minwise hashing comes to the rescue.
		- Suppose we hash each new element *s* of *S* as it comes in, and only save $h(s)$ if it is smaller than the previous minhash.
	- Why is this interesting?
		- Suppose the range of possible hash values is between 0 and $M-1$, and we select *k* values in this range uniformly at random.
		- What is the expected minimum of these *k* values?
			- If $k=1$, the expected value will obviously be $M/2$.
			- For general *k*, we can say that if our *k* values were equally spaced in the interval, the minhash should be $\frac{M}{k+1}$.
	- The punch line is that *M* divided by the minhash value gives an excellent estimate of the number of distinct values we have seen.
		- This method will not be fooled by repeated values in the stream, since repeated occurrences will yield precisely the same value every time we evaluate the hash function.