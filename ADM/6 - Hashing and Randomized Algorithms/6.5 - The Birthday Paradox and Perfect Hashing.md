#Chapter_6 
- Hash tables are an excellent data structure in practice for the standard dictionary operations of insert, delete, and search.
- However, the $\Theta(n)$ worst-case search time for hashing is an annoyance, no matter how rare it is.
- Is there a way we can guarantee worst-case constant time search?

- *Perfect hashing* offers us this possibility for *static* dictionaries.
	- Here we are given all possible keys in one batch, and are not allowed to later insert or delete items.
	- We can thus build the data structure once and use it repeatedly for search/membership testing.
	- This is a fairly common use case, so why pay for the flexibility of dynamic data structures when you don't need them?

- One idea for how this might work would be to try a given hash function $h(x)$ on our set of *n* keys *S*, and hope it creates a hash table with no collisions, that is, $h(x)\ne h(y)$ for all pairs of distinct $x,y\in S$.
	- It should be clear that our chances of getting lucky improve as we increase the size of our table relative to *n*: the more empty slots there are available for the next key, the more likely we find one.

- How large a hash table *m* do we need before we can expect zero collisions among *n* keys?
- Suppose we start from an empty table, and repeatedly insert keys.
	- For the $(i+1)$th insertion, the probability that we hit one of the $m-i$ still-open slots in the table is $(m-i)/m$.
	- For a perfect hash, *all n* inserts must succeed, so $P($no collision$)=\prod^{n-1}_{i=0}(\frac{m-i}{m})=\frac{m!}{m^n((m-n)!)}$

- What happens when you evaluate this is famously called the *birthday paradox*:
- How many people do you need in a room before it is likely that at least two of them share a birthday?
	- Here the table size is $m=365$.
	- The probability of no collisions drops below $1/2$ for $n=23$ and is less than 3% when $n\geq50$, as shown in Figure 6.8.
		- In other words, odds are we have a collision when only 23 people are in the room.
	- Solving this asymptotically, we begin to expect collisions when $n=\Theta(\sqrt{m})$, or equivalently when $m=\Theta(n^2)$.

![[Pasted image 20231127173123.png]]

- But quadratic space seems like an awful large penalty to pay for constant time access to *n* elements.
- Instead, we create a two-level hash table.
	- First, we hash the *n* keys of set *S* into a table with *n* slots.
	- We expect collisions, but unless we are very unlucky, all the lists will be short enough.

- Let $l_i$ be the length of the *i*th list in this table.
	- Because of collisions, many lists be of length longer than 1.
- Our definition of short enough is that *n* items are distributed around the table such that the sum of squares of the list lengths is linear, that is, $N=\sum^{n}_{i=1}l_i^2=\Theta(n)$.

- Suppose that it happened that all elements were in lists of length *l*, meaning that we have $n/l$ empty lists.
	- The sum of squares of the list lengths is $N=(n/l)l^2=nl$, which is linear because *l* is a constant.
- We can even get away with a fixed number of lists of length $\sqrt{n}$ and still use linear space.

- In fact, it can be shown that $N\leq4n$ with high probability.
	- So if this isn't true on *S* for the first hash function we try, we can just do another.
	- Pretty soon we will find one with short-enough list lengths that we can use.

- We will use an array of length *N* for our second-level table, allocating $l_i^2$ space for the elements of the *i*th bucket.
	- Note that this is big enough relative to the number of elements to avoid the birthday paradox - odds are we will have no collision in any given hash function. And if we do, simply try another hash function until all elements end up in unique places.

- The complete scheme is illustrated in Figure 6.9.
- The contents of the *i*th entry in the first-level hash table include the starting and ending positions for the $l_i^2$ entries in the second-level table corresponding to the list.
	- It also contains a description (or identifier) of the hash function that we will use for this region of the second-level table.

![[Pasted image 20231127174301.png]]

- Lookup for an item *s* starts by calling hash function $h_1(s)$ to compute the right spot in the first table, where we will find the appropriate start/stop position and hash function parameters.
	- From this, we can compute $start + (h_2(s) ($mod $(stop-start)))$, the location where item *s* will appear in the second table.
	- Thus, search can always be performed in $\Theta(1)$ time, using linear storage space between the two tables.

- Perfect hashing is a very useful data structure in practice, ideal for whenever you will be making large numbers of queries to a static dictionary.
	- There is a lot of fiddling you can do with this basic scheme to minimize space demands and construction/search cost, such as working harder to find second-level hash functions with fewer holes in the table.
	- Indeed, *minimum perfect hashing* guarantees constant time access with zero empty hash table slots, resulting in an *n*-element second hash table for *n* keys.