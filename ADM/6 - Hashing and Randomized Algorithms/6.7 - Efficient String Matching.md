#Chapter_6 
- Strings are sequences of characters where the order of the characters matters: the string *ALGORITHM* is different than *LOGARITHM*.

- The primary data structure for representing strings is an array of characters.
	- This allows us constant-time access to the *i*th character of the string.
- Some auxiliary information must be maintained to mark the end of the string:
	- Either a special end-of-string character or (perhaps more usefully) a count *n* of the characters in the string.

- The most fundamental operation on text strings is substring search, namely:
	- ***Problem***: Substring Pattern Matching
	- ***Input***: A text string *t* and a pattern string *p*.
	- ***Output***: Does *t* contain the pattern *p* as a substring, and if so where?

- The simplest algorithm to search for the presence of pattern string *p* in text *t* overlays the pattern string at every position in the text, and checks whether every pattern character matches the corresponding text character.
	- As demonstrated in [[2.5 - Reasoning About Efficiency|Section 2.5.3]], this runs in $O(nm)$ time, where $n=|t|$ and $m=|p|$.
	- This quadratic bound is worst case. More complicated, worst-case linear-time search algorithms do exist.

- But here I give a linear *expected-time* algorithm for string matching, called the Rabin-Karp algorithm.
	- It is based on hashing.
- Suppose we compute a given hash function on both the pattern string *p* and the *m*-character substring starting from the *i*th position of *t*.
	- If these two strings are identical, clearly the resulting hash values must be the same.
	- If these two strings are different, the hash values will *almost certainly* be different.
		- These false positives should be so rare that we can easily spend the $O(m)$ time it takes to explicitly check the identity of the two strings whenever the hash values agree.

- This reduces string matching to $n-m+2$ hash value computations (the $n-m+1$ windows of *t*, plus one hash of *p*), plus what *should be* a very small number of $O(m)$ time verification steps.
	- The catch is that it takes $O(m)$ time to compute a hash function on an *m*-character string, and $O(n)$ such computations seems to leave us with an $O(nm)$ algorithm again.

- But let's look more closely at our previously defined (in [[3.7 - Hashing|Section 3.7]]) hash function, applied to the *m* characters starting from the *j*th position of string *S*:
	- $H(S,j)=\sum^{m-1}_{i=0}\alpha^{m-(i+1)}\times char(s_{i+j})$

- What changes if we now try to compute $H(S, j+1)$ -- the hash of the next window of *m* characters?
	- Note that $m-1$ characters are the same in both windows, although this differs by one in the number of times they are multiplied by $\alpha$.
	- A little algebra reveals that: $H(S,j+1)=\alpha(H(S,j)-\alpha^{m-1}char(s_j))+char(s_{j+m})$.

- This means that once we know the hash value from the *j*th position, we can find the hash value from the $(j+1)$th position for the cost of two multiplications, one addition, and one subtraction.
	- This can be done in constant time (the value of $\alpha^{m-1}$ can be computed once and used for all hash value computations).
	- This math works even if we compute $H(S,j)$ mod *M*, where *M* is a reasonably large prime number.
		- This keeps the size of our hash values small (at most *M*), even when the pattern string is long.

- Rabin-Karp is a good example of a randomized algorithm (if we pick *M* in some random way).
	- We get no guarantee that the algorithm runs in $O(n+m)$ time, because we may get unlucky and have the hash values frequently collide with spurious matches.
	- Still, the odds are heavily in our favor, if the hash function returns values uniformly from 0 to $M-1$, the probability of a false collision should be $1/M$.
		- This is quite reasonable: 
			- If $M\approx n$, there should only be one false collision per string;
			- If $M\approx n^k$ for $k\geq 2$, the odds are great we will never see any false collisions.

![[Pasted image 20231128133316.png]]