#Chapter_6 
- Balls and bin problems are classics of probability theory.
- We are given *x* identical balls to toss at random *y* labeled bins.
	- We are interested in the resulting distribution of balls.
	- How many bins are expected to contain a given number of balls?

- Hashing can be though of as a ball and bin process.
- Suppose we hash *n* balls/keys into *n* bins/buckets.
	- We expect an average of one ball per bin, but note that this will be true *regardless* of how good or bad the hash function is.

- A good hash function should behave like a random number generator, selecting integers/bin IDs with equal probability from 1 to *n*.
	- But what happens when we draw *n* such integers from a uniform distribution?
	- The ideal would be for each of the *n* items (balls) to be assigned to a different bin, so that each bucket contains exactly one item to search. But is that really what happens?

- I ran a simulation, and got the following results, for hash table sizes from one million to one hundred million items:
![[Pasted image 20231127092022.png]]

- We see that 36.78% of the buckets are empty in all three cases. That can't be a coincidence.
- The first bucket will be empty only iff each of the *n*balls gets assigned to one of the other $n - 1$ buckets.
	- The probability *p* of missing for each particular ball is $p=(n-1)/p$, which approaches 1 as *n* gets large.
	- But we must miss for *all n* balls, the probability of which is $p^n$.

- What happens when we multiply a large number of large probabilities?
	- $P(|B_1|=0)=(\frac{n-1}{n})^2\to \frac{1}{e}=0.367879$
- Thus, 36.78% of the buckets in a large hash table will be empty.
	- As it turns out, exactly the same fraction of buckets is expected to contain one element.

- If so many buckets are empty, others must be unusually full.
	- The fullest bucket gets fuller in the table as *n* increases, from 8 to 9 to 11.
	- In fact, the expected value of the longest list if $O(\log n / \log \log n)$, which grows slowly but is not a constant.
		- Thus, the worst-case access time for hashing is not *actually* $O(1)$.

- ***Take-Home Lesson***:
	- ***Precise analysis of random process requires formal probability theory, algebraic skills, and careful asymptotics.*** 
	- ***We will gloss over such issues in this chapter, but you should appreciate that they are out there.***

### 6.2.1 - The Coupon Collector's Problem
- As a final hashing warmup, let's keep tossing balls into these *n* bins until none of them are empty, that is, until we have at least one ball in each bin.
	- How many tosses do we expect this should take?
	- As shown in Figure 6.5, it may require considerably more than *n* tosses until every bin is occupied.

![[Pasted image 20231127093719.png]]

- We can split such a sequence of balls into *n* runs, where run $r_i$ consists of the balls we toss after we have filled *i* buckets until the next time we hit an empty bucket.
- The expected number of balls to fill all *n* slots $E(n)$ will be the sum of the expected lengths of all runs.
	- If you are flipping a coin with probability *p* of coming up heads, the expected number of flips until you get your first head is $1/p$; this is a property of the *geometric* distribution.
- After filling *i* buckets, the probability that our next toss will hit an empty bucket is $p=(n-i)/n$.

- Putting this together, we get the following $E(n)=\sum^{n-1}_{i=0}|r_1|=\sum^{n-1}_{i=0} \frac{n}{n-i}=n\sum^{n-1}_{i=0}\frac{1}{n-i}=nH_n\approx n\ln n$

- The trick here is to remember that the *harmonic number* $H_n=\sum^{n}_{i=1} \frac{1}{i}$, and that $H(n)\approx \ln n$.

##### Stop and Think: Covering Time for $K_n$
- ***Problem:***
	- Suppose we start on vertex 1 of a complete *n*-vertex graph (see Figure 6.4).
	- We take a random walk on this graph, at each step going to a randomly selected neighbour of our current position.
	- What is the expected number of steps until we have visited all vertices of the graph?

- ***Solution:***
	- This is exactly the same question as the previous *stop and think*, but with a different graph and thus with a possibly different answer.
	- Indeed, the random process here of independently generating random integers from 1 to *n* looks essentially the same as the coupon collector's problem.
		- This suggests that the expected length of the covering walk is $\Theta(n\log n)$.
	- The only hitch in this argument is that the random walk model does not permit us to stay at the same vertex for two successive steps, unless the graph has edges from a vertex to itself (self-loops).
		- A graph without such self-loops should have a *slightly* faster covering time, since a repeat visit does not make progress in any way, but not enough time to change the asymptotics.
	- The probability of discovering one of $n-i$ untouched vertices in the next step changes from $(n-i)/n$ to $(n-i)/(n-1)$, reducing the total covering time analysis from $nH_n$ to $(n-1)H_n$/
		- But these are asymptotically the same.
	- Covering the complete graph takes $\Theta(n\log n)$ steps, much faster then the covering time of the path.