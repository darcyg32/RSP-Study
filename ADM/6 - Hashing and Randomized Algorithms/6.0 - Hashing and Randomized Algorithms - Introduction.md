#Chapter_6
- Most of the algorithms discussed in previous chapters were designed to optimize worst-case performance:
	- They are guaranteed to return optimal solutions for every problem instance within a specified running time.

- This is great, when we can do it. But relaxing the demand for either *always* correct or *always* efficient can lead to useful algorithms that still have performance guarantees.
- Randomized algorithms are not merely heuristics:
	- Any bad performance is due to getting unlucky on coin flips, rather than adversarial input data.

- We classify randomized algorithms into two types, depending upon whether they guarantee correctness or efficiency:
	- *Las Vegas algorithms*:
		- Guarantee correctness, and are usually (but not always) efficient.
		- Quicksort is an excellent example of a Las Vegas algorithm.
	- *Monte Carlo algorithms*:
		- Are provable efficient, and usually (but not always) produce the correct answer or something close to it.
		- Representative of this class are random sampling methods discussed in Section 12.6.1, where we return the best solution found in the course of (say) 1,000,000 random samples.
- We will see several examples of both types of algorithm in this chapter.

- One blessing of randomized algorithms is that they tend to be very simple to describe and implement.
	- Eliminating the need to worry about rare or unlikely situations make it possible to avoid complicated data structures and other contortions.
	- These clean randomized algorithms are often intuitively appealing, and relatively easy to design.

- However, randomized algorithms are frequently quite difficult to analyze rigorously.
	- [[6.1 - Probability Review|Probability theory]] is required, and probabilistic analysis often involves algebraic manipulation of long chains of inequalities that looks frightening, and relies on tricks and experience.