#Chapter_6 
- Recall that a hash function $h(s)$ maps keys *s* to integers in a range from 0 to $m-1$, ideally uniformly over this interval.
	- Because good hash functions scatter keys around this integer range in a manner similar to that of a random number generator, we can analyze hashing by treating the values as drawn from tosses of an *m*-sided die.

- But just because we can analyze hashing in terms of probabilities doesn't make it a randomized algorithm.
- As discussed so far, hashing is completely deterministic, involving no random numbers.
	- Indeed, hashing *must* be deterministic, because we need $h(x)$ to produce exactly the same result whenever called with a given *x*, or else we can never hope to find *x* in a hash table.

- One reason we like randomized algorithms is that they make the worst case input instance go away: bad performance should be a result of extremely bad luck, rather than some joker giving us data that makes us do bad things.
- But it is easy (in principle) to construct a worst-case example for any hash function $h$.
	- Suppose we take an arbitrary set *S* of *nm* distinct keys, and hash each $s\in S$.
	- Because the range of this function only has *m* elements, there must be many collisions.
	- Since the average number of items per bucket is $nm/m=n$, it follows from the pigeonhole principle that there must be a bucket with at least *n*items in it.
	- The *n* items in this bucket, taken by themselves, will prove a nightmare for hash function *h*.

- How can we make such a worst-case input go away?
	- We are protected if we pick our *hash function* at random from a large set of possibilities, because we can only construct such a bad example by knowing the exact hash function we will be dealing with.

- So how can we construct a family of random hash functions?
- Recall that typically $h(x)=f(x) ($mod $m)$, where $f(x)$ turns the key into a huge value, and taking the remainder mod *m* reduces it to the desired range.
	- Our desired range is typically determined by application and memory constraints, so we would no want to select *m* at random.
	- But what about first reducing with a large integer *p*?

- Observe that in general, $f(x) ($mod $m \ne (f(x)$ mod $p) ($ mod $m)$.
- For example: $21347895537127 ($mod $17) = 8$ 
				$\neq (21347895537127 ($mod $2342343)) ($mod $17) = 12$.

- Thus, we can select *p* at random to define the hash function $h(x) = ((f(x)$ mod $p)$ mod $m)$, and things will work out just find provided:
	- $f(x)$ is large relative to *p*;
	- *p* is large relative to *m*; and,
	- *m* is relatively prime to *p* (their GCD is 1 (e.g. 15 and 28)).

- This ability to select random hash functions means we can now use hashing to provide legitimate hashing guarantees, thus making the worst-case input go away.
	- It also lets us build powerful algorithms involving multiple hash functions, such as Bloom filters, discussed in [[6.4 - Bloom Filters|Section 6.4]].