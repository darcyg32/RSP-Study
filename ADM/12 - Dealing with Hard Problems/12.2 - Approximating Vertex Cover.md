#Chapter_12 
- Recall the vertex cover problem, where we seek a small subset *S* of the vertices of a given graph *G* such that for every edge *(x, y)* in *G*, at least one of *x* or *y* is in *S*.
	- As we have seen, finding the minimum vertex cover of a graph is NP-complete.
- However, a very simple procedure will always find a cover that is at most twice as large as the optimal cover.
	- It repeatedly selects an uncovered edge, and picks *both* of its vertices for the cover:

VertexCover($G=(V,E)$)
	Wile ($E\neq \emptyset$), do:
		Select an arbitrary edge $(u, v)\in E$
		Add both *u* and *v* to the vertex cover.
		Delete all edges from *E* that are incident to either *u* or *v*.

- It should be apparent that this procedure always produces a vertex cover, since each edge is deleted only after an incident vertex has been added to the cover.
- More interesting is the claim that the best vertex cover must use at least half as many vertices as this one.
	- Why? Consider only the *k* edges selected by the algorithm that constitute a matching in the graph.
	- No two of these matching edges can share a vertex, so *any* cover of just these *k*edges must include at least one vertex per edge, which makes it at least half the size of this *2k*-vertex greedy cover.

- There are several interesting things to notice about this algorithm:
	- *Although the procedure is simple, it is not stupid:*
		- Many seemingly smarter heuristics can give a far worse performance in the worst case.
			- For example, why not modify the above procedure to select only one of the vertices for the cover, instead of both? After all, the selected edge will be equally well covered by only one vertex.
			- But consider the star-shaped graph of Figure 12.1. The original heuristic will produce a two-vertex cover, while the single-vertex heuristic *might* return a cover as large as $n-1$ vertices, should we get unlucky and repeatedly select the leaf instead of the center as the cover vertex we retain.![[Pasted image 20231219124006.png]]
	- *Greedy isn't always the answer*:
		- Perhaps the most natural heuristic for vertex cover would repeatedly select (and then delete) the vertex with highest remaining degree for the vertex cover.
			- After all, this vertex will cover the largest number of possible edges.
		- However, in the case of ties or near ties, this heuristic can go seriously astray.
			- In the worst case, it can yield a cover that is $\Theta (\lg n)$ times optimal, as shown by the example of Figure 12.2.![[Pasted image 20231219124355.png]]
	- *Making a heuristic more complicated does not necessarily make it better*:
		- It is easy to complicate heuristics by adding more special cases or details.
			- For example, the procedure above did not specify which edge should be selected next for the matching.
			- It might seem reasonable to pick the edge whose endpoints have the highest total degree.
				- However, this does not improve the worst-case bound, and just makes it more difficult to analyze.
	- *A post-processing cleanup step can't hurt*:
		- The flip side of designing simple heuristics is that they can often be modified to yield better-in-practice solutions without weakening the approximation bound.
			- For example, a post-processing step that deletes any unnecessary vertex from the cover can only improve things in practice, even though it won't help the worst-case bound.
			- And it is fair to repeat the process multiple times with different starting edges and take the best of the resulting runs.

- The important property of approximation algorithms is relating the size of the solution produced to a lower bound on the optimal solution.
- Instead of thinking about how well we might do, we must think about the worst case. That is, how badly the algorithm might perform.

##### Stop and Think: Leaving Behind a Vertex Cover
- ***Problem*:
	- Suppose we do a depth-first search of graph *G*, naturally building a depth-first search tree *T* in the process.
	- A leaf node in a tree is any non-root vertex of degree 1.
	- Delete every leaf node from *T*.
	- Show that:
		- The set of all non-leaf nodes of *T* form a vertex cover of graph *G*; and
		- That this vertex cover is of size at most twice that of the minimum vertex cover.

- ***Solution*:
	- Why must the set of all non-leaf nodes in the DFS tree *T* form a vertex cover?
		- Recall that the magic property of DFS is that it partitions all edges into tree edges and back edges.
		- If a vertex *v* is a leaf of *T*, then there is a single tree edge $(x,v)$ containing it, which will be covered by taking non-leaf vertex *x*.
		- If there are other edges containing *v*, they must be back edges going to ancestors of *v*, all of which were selected to be in the cover.
		- So all edges will be covered by the set of non-leaves.
	- But why is the set of non-leaves at most twice the size of the optimal cover?
		- Start from any leaf *v* and walk up the tree to the root.
		- Suppose this path is of length *k* edges, meaning $k+1$ vertices leaf-to-root.
		- This heuristic will select the *k* non-leaf vertices for the cover.
		- But the best possible cover for this path requires $k/2$ vertices, so we are always within a factor of at most two times optimal.

### 12.2.1 - A Randomized Vertex Cover Heuristic
- Although we proved that our original vertex cover heuristic of selecting arbitrary uncovered edges and adding *both* vertices to the cover yields a factor two approximation algorithm, it feels wrong to grow the cover by two vertices when either one would equally cover the given edge.
	- However, the star-shaped heuristic example of Figure 12.1 shows that if we repeatedly pick the wrong (meaning non-center) vertex for each edge, we could end up with a cover of size $n-1$ instead of 1.

- Such a horrible performance requires making the wrong decision $n-1$ times in a row, which implies either a special talent or horrendous luck.
- We can make it a matter of luck by choosing the vertex at random:

VertexCover($G=(V,E)$)
	While ($E\neq\emptyset$) do:
		Select an arbitrary edge $(u,v)\in E$
		Randomly pick either *u* or *v*, and add it to the vertex cover.
		Delete all edges from *E* that are incident to the selected vertex.

- At the end of this procedure, we will end up with a vertex cover, but how well does its expected size compare to a particular minimum size cover *C*?
- Observe that with each edge $(u,v)$ we select, at least one of the two endpoints must appear in the optimal cover *C*.
	- Thus, at least half the time we get lucky and pick the "right" vertex.
- At the end of this procedure we will have picked a set $C'\subset C$ of cover vertices plus a set *D* of vertices from $V-C$ for our cover.
	- We know that $|C'|$ always must be less than or equal to $|C|$.
	- Further, the expected size of *D* is equal to that of $C'$.
- Thus, in expectation $|C'|+|D|\leq 2|C|$, and we get a solution whose size is expected to be at most twice that of optimal.

- Randomization is a very powerful tool for developing approximation algorithms.
	- Its role is to make bad special cases go away by making it very unlikely that they will occur.
- The careful analysis of such probabilities often requires sophisticated efforts, but the heuristics themselves are generally very simple and easy to implement.