#Chapter_8 
- A *spanning tree* of a connected graph $G=(V,E)$ is a subset of edges from *E* forming a tree connecting all vertices of *V*.
- For edge-weighted graphs, we are particularly interested in the *minimum spanning tree* - the spanning tree whose sum of edge weights is as small as possible.

- Minimum spanning trees are the answer whenever we need to connect a set of points (representing cities, homes, junctions, or other locations) cheaply using the smallest amount of roadway, wire, or pipe.

- *Any* tree is the smallest possible connected graph in terms of number of edges, but the minimum spanning tree is the smallest connected graph in terms of edge weight.
- In geometric problems, the point set $p_1,...,p_n$ defines a complete graph, with edge $(v_i,v_j)$ assigned a weight equal to the distance from $p_i$ to $p_j$.
	- An example of a geometric minimum spanning tree is illustrated in Figure 8.1.

![[Pasted image 20231205182148.png]]

- A minimum spanning tree minimizes the total edge weight over all possible spanning trees.
- However, there can be more than one minimum spanning tree of a given graph.
	- Indeed, all spanning trees of an unweighted graph *G* are minimum spanning trees, since each contains exactly $n-1$ equal-weight edges.
	- Such a spanning tree can be found using either BFS or DFS.

- Finding a minimum spanning tree is more difficult for general weighted graphs, but two different algorithms are presented below, both demonstrating the optimality of specific greedy heuristics.

### 8.1.1 - Prim's Algorithm
- Prim's minimum spanning tree algorithm starts from one vertex and grows the rest of the tree one edge at a time until all vertices are included.

- Greedy algorithms make the decision of what to do next by selecting the best local option from all available choices without regard to the global structure.
- Since we seek the tree of minimum weight, the natural greedy algorithm for MST repeatedly selects the smallest weight edge that will enlarge the number of vertices in the tree.

Prim-MST(*G*)
	Select an arbitrary vertex *s* to start the tree $T_{prim}$ from.
	While there are still non-tree vertices
		Find the minimum-weight edge between a tree and a non-tree vertex.
		Add the selected edge and vertex to the tree $T_{min}$.

- Prim's algorithm clearly creates a spanning tree, because no cycle can be introduced by adding edges between tree and non-tree vertices.
- But why should it be of minimum weight over *all* spanning trees?

- We use proof by contradiction.
- Suppose that there existed a graph *G* for which Prim's algorithm did not return a minimum spanning tree.
- Since we are building the tree incrementally, this means that there must have been some particular instant where we went wrong.
	- Before we inserted edge $(x, y)$, $T_{prim}$ consisted of a set of edges that was a subtree of some minimum spanning tree $T_{min}$, but choosing edge $(x, y)$ fatally took us away from any possible minimum spanning tree (see Figure 8.2(a)).
- But how could we have gone wrong? There must be a path *p* from *x* to *y* in $T_{min}$, as shown in Figure 8.2(b).
	- This path must use an edge $(v_1,v_2)$, where $v_1$ is already in $T_{prim}$, but $v_2$ is not.
	- This edge $(v_1,v_2)$ must have weight at least that of $(x,y)$, or else Prim's algorithm would have selected it before $(x,y)$ when it had the chance.
- Inserting $(x,y)$ and deleting $(v_1,v_2)$ from $T_{min}$  leaves a spanning tree no larger than before, meaning that Prim's algorithm could not have made a fatal mistake in selecting edge $(x,y)$.
- Therefore, by contradiction, Prim's algorithm must construct a minimum spanning tree.

![[Pasted image 20231205183502.png]]
##### Implementation
- Prim's algorithm grows the minimum spanning tree in stages, starting from a given vertex.
	- At each iteration, we add one new vertex into the spanning tree.
	- A greedy algorithm suffices for correctness: we always add the lowest-weight edge linking a vertex in the tree to a vertex on the outside.

- The simplest implementation of this idea would assign to each vertex a Boolean variable denoting whether it is already in the tree (the array `intree` in the code below), and then search all edges at each iteration to find the minimum-weight edge with exactly one `intree` vertex.

- Our implementation is somewhat smarter.
- It keeps track of the cheapest edge linking every non-tree vertex to the tree.
	- The cheapest such edge over all remaining non-tree vertices gets added in the next iteration.
- We must update the costs of getting to the non-tree vertices after each iteration.
	- However, since the most recently inserted vertex is the only change in the tree, all possible edge-weight updates must come from its outgoing edges:

```C
int prim(graph* g, int start) {
	int i;                     // Counter
	edgenode* p;               // Temp pointer
	bool intree[MAXV + 1];     // Is the vertex in the tree yet?
	int distance[MAXV + 1];    // Cost of adding to tree
	int v;                     // Current vertex to process
	int w;                     // Candidate next vertex
	int dist;                  // Cheapest cost to enlarge tree
	int weight = 0;            // Tree weight

	// Initialization
	for (i = 1; i <= g->nvertices; i++) {
		intree[i] = false;     // No vertices in tree initially
		distance[i] = MAXINT;  // Set distance to infinity for all vertices
		parent[i] = -1;        // Initialize parent of all vertices as undefined
	}

	distance[start] = 0;       // Distance to the start vertex is set to 0
	v = start;                 // Start with the given vertex

	// Loop until all vertices are included in the tree
	while (!intree[v]) {
		intree[v] = true;      // Add the current vertex to the tree
		if (v != start) {
			printf("edge (%d, %d) in tree \n", parent[v], v);
			weight += dist;    // Accumulate the weight of the tree
		}
		p = g->edges[v];

		// Update distance and parents for adjacent vertices to the added vertex
		while (p != NULL) {
			w = p->y; // w is an adjacent vertex to v
			if ((distance[w] > p->weight) && (!intree[w])) {
				// The distance to the tree is now the weight between edge w, v
				distance[w] = p->weight;
				parent[w] = v;
			}
			p = p->next;
		}

		// Find the next vertex to add to the tree,
		dist = MAXINT;
		for (i = 1; i <= g->nvertices; i++) {
			if ((!intree[i]) && (dist > distance[i])) {
				dist = distance[i];
				v = i;
			}
		}
	}

	return weight;
}	
```
##### Analysis
- Prim's algorithm is correct, but its efficiency depends on which data structures are used to implement it.
- In the pseudocode, Prim's algorithm makes *n* iterations sweeping through all *m* edges on each iteration, yielding an $O(nm)$ algorithm.
- But our implementation avoids the need to test all *m* edges on each pass.
	- It only considers the $\le n$ cheapest known edges represented in the `parent` array, and the $\le n$ edges out of a new tree vertex *v* to update `parent`.
	- By maintaining a Boolean flag along with each vertex to denote whether it is in the tree, we test whether the current edge joins a tree with a non-tree vertex in constant time.

- The result is an $O(n^2)$ implementation of Prim's algorithm, and a good illustration of the power of data structures to speed up algorithms.
	- In fact, more sophisticated priority-queue data structures lead to an $O(m+n\lg n)$ implementation, by making it faster to find the minimum-cost edge to expand the tree at each iteration.

- The minimum spanning tree itself can be reconstructed in two different ways.
	- The simplest method would be to augment this procedure with statements that print the edges as they are found, and totals the weight of all selected edges to get the cost.
	- Alternatively, the tree topology is encoded by the `parent` array, so it completely describes edges in the minimum spanning tree.

### 8.1.2 - Kruskal's Algorithm
- Kruskal's algorithm is an alternative approach to finding the minimum spanning tree that proves more efficient on sparse graphs.
	- Like Prim's, Kruskal's algorithm is greedy.
	- Unlike Prim's it does not start with a particular vertex.
	- As shown in Figure 8.3, Kruskal might produce a different spanning tree than Prim's algorithm, although both will have the same weight.

![[Pasted image 20231206101159.png]]

- Kruskal's algorithm builds up connected components of vertices, culminating in the complete minimum spanning tree.
	- Initially, each vertex forms its own separate component in the tree-to-be.
	- The algorithm repeatedly considers the lightest remaining edge and tests whether its two endpoints lie within the same connected component.
		- If so, this edge will be discarded, because adding it would create a cycle.
	- If the endpoints lie in different components, we insert the edge and merge the two components into one.
	- Since each connected component always is a tree, we never need to explicitly test for cycles.

Kruskal-MST(*G*)
	Put the edges into a priority queue ordered by increasing weight.
	*count* = 0
	while (*count < n - 1*)
		get next edge (*v, w*)
		if (component(*v*) $\neq$ component(*w*))
			increment *count*
			add (*v, w*) to $T_{kruskal}$
			merge component(*v*) and component(*w*)

- This algorithm adds $n-1$ edges without creating a cycle, so it must create a spanning tree for any connected graph. But why does this have to be a *minimum* spanning tree?
- Suppose it isn't.
	- As with the correctness proof of Prim's algorithm, there must be a particular graph *G* on which it fails.
	- In particular, there must be an edge $(x, y)$ in *G* whose insertion first prevented $T_{kruskal}$ from being a minimum spanning tree $T_{kruskal}$.
	- Inserting this edge (*x, y*) into $T_{min}$ will create a cycle with the path from *x* to *y*, as shown in Figure 8.4.
	- Since *x* and *y* were in different components at the time of inserting (*x, y*), at least one edge (say $(v_1, v_2)$) on this path must have been evaluated by Kruskal's algorithm at a later time than (*x, y*).
	- But this means that $weight(v_1,v_2)\geq weight(x,y)$, so exchanging the two edges yields a tree of weight at most $T_{min}$.
	- Thus, we could not have made a fatal mistake in selecting (*x, y*), and the correctness follows.

![[Pasted image 20231206103001.png]]

- What is the time complexity of Kruskal's algorithm?
	- Sorting the *m* edges takes $O(m\lg m)$ time.
	- The `while` loop makes at most *m* iterations, each testing the connectivity of two trees plus an edge.
	- In the most simple-minded implementation, this can be done by BFS or DFS in the sparse partial tree graph with at most *n* edges and *n* vertices, thus yielding an $O(nm)$ algorithm.
- However, a faster implementation results if we can implement the component test in faster than $O(n)$ time.
	- In fact, a clever data structure called a *union-find*, can support such queries in $O(\lg n)$ time, and it is discussed in Section 8.1.3.
	- With this data structure, Kruskal's algorithm runs in $O(m\lg m)$ time, which is faster than Prim's for sparse graphs.
##### Implementation
- The implementation of the main routine follows directly from the pseudocode:
```C
int kruskal(graph* g) {
	int i;                    // Counter
	union_find s;             // Union-find data structure
	edge_pair e[MAXV + 1];    // Array of edges data structre
	int weight = 0;           // Cost of the minimum spanning tree

	union_find_init(&s, g->nvertices);

	// Convert the graph edges into an array format
	to_edge_array(g, e);
	
	// Sort the edges in increasing order based on their weights
	qsort(&e, g->nedges, sizeof(edge_pair), &weight_compare);

	// Iterate through the sorted edges
	for (i = 0; i < g->nedges; i++) {
		// Check if adding the current edge does not create a cycle in the spanning tree
		if (!same_component(&s, e[i].x, e[i].y)) {
			printf("edge (%d,%d) in MST\n", e[i].x, e[i].y);
			// Update total weight of MST
			weight += e[i].weight;
			// Merge the components of the current edge in the union-find ds
			union_sets(&s, e[i].x, e[i].y);
		}
	}

	return weight;
}
```

### 8.1.3 - The Union-Find Data Structure
- A *set partition* parcels out the elements of some universal set (say the integers 1 to *n*) into a collection of disjoint subsets, where each element is in exactly one subset.
	- Set partitions naturally arise in graph problems such as connected components (each vertex is in exactly one connected component) and vertex coloring (a vertex may be white or black in a bipartite graph, but not both or neither).
	- Section 17.6 presents algorithms for generating set partitions and related objects.

- The connected components in a graph can be represented as a set partition.
- For Kruskal's algorithm to run efficiently, we need a data structure that efficiently supports the following operations:
	- $same\_component(v_1, v_2)$:
		- Do vertices $v_1$ and $v_2$ occur in the same connected component of the current graph?
	- $merge\_component(C_1, C_2)$:
		- Merge the given pair of connected components into one component in response to the insertion of an edge between them.

- The two obvious data structures for this task each support only one of these operations efficiently.
	- Explicitly labeling each element with its component number enables the *same_component* test to be performed in constant time, but updating the component numbers after a merger would require linear time.
	- Alternatively, we can treat the merge components operation as inserting an edge in the graph, but then we must run a full graph traversal to identify the connected components on demand.

- The *union-find* data structure represents each subset as a "backwards" tree, with pointers from a node to its parent.
	- Each node of this tree contains a set element, and the *name* of the set is taken from a key at the root, as shown in Figure 8.5.
	- For reasons that will become clear, we also keep track of the number of elements in the subtree rooted in each vertex *w*.

![[Pasted image 20231206105440.png]]

```C
typedef struct {
	int p[SET_SIZE + 1];    // Parent element
	int size[SET_SIZE + 1]; // Number of elements in subtree i
	int n;                  // Number of elements in set
} union_find;
```

- We implement our desired component operations in terms of two simpler operations, *union* and *find*:
	- *Find(i)*:
		- Find the root of the tree containing element *i*, by walking up the parent pointers until there is nowhere to go.
		- Return the label of the root.
	- *Union(i, j)*:
		- Link the root of one of the trees (say containing *i*) to the root of the tree containing the other (say *j*).
		- So *find(i)* now equals *find(j)*.

- We seek to minimize the time it takes to execute the *worst possible* sequence of unions and finds.
	- Tree structures can be very unbalanced, so we must limit the height of our trees.
	- Our most obvious means of control is the choice of which of the two component roots becomes the root of the merged component on each *union*.
- To minimize the tree height, it is better to make the smaller tree the subtree of the bigger one.
	- Why? The heights of all the nodes in the root subtree stay the same, but the height of the nodes merged into this tree all increase by one.
	- Thus, merging in the smaller tree leaves the height unchanged on the larger set of vertices.
##### Implementation
- The implementation details are as follows:
```C
void union_find_init(union_find* s, int n) {
	int i;    // Counter

	for (i = 1; i <= n; i++) {
		s->p[i] = i;
		s->size[i] = 1;
	}

	s->n = n;
}

int find(union_find* s, int x) {
	if (s->p[x] == x) {
		return x;
	}
	return find(s, s->p[x]);
}

void union_sets(union_find* s, int s1, int s2) {
	int r1, r2;    // Roots of sets

	r1 = find(s, s1);
	r2 = find(s, s2);

	if (r1 == r2) {
		return;    // Already in same set
	}

	if (s->size[r1] >= s->size[r2]) {
		s->size[r1] += s->size[r2];
		s->p[r2] = r1;
	} else {
		s->size[r2] += s->size[r1];
		s->p[r1] = r2;
	}
}

bool same_component(union_find* s, int s1, int s2) {
	return (find(s, s1) == find (s, s2));
}
```
##### Analysis
- On each iteration, the tree with fewer nodes becomes the child, but how tall can such a tree get as a function of the number of nodes in it?
	- Consider the smallest possible tree of height *h*.
		- Single-node trees have height 1.
		- The smallest tree of height 2 has two nodes: it is made from the union of two single-node trees.
		- Merging in more single-node trees won't further increase the height, because they just become children of the rooted tree of height 2.
		- Only when we merge two height 2 trees together can we get a tree of height 3, now with at least 4 nodes.
	- See the pattern?
		- We must double the number of nodes in the tree to get an extra unit of height.
		- How many doublings can we do before we use up all *n* nodes?
			- At most, $\lg n$ doublings can be performed.
		- Thus, we can do both unions and finds in $O(\log n)$, fast enough to make Kruskal's algorithm efficient on sparse graphs.
- Union-find can actually be done even faster, as discussed in Section 15.5.

### 8.1.4 - Variations on Minimum Spanning Trees
- The algorithms that construct minimum spanning trees can also be used to solve several closely related problems:
	- *Maximum Spanning Trees*:
		- The *maximum spanning tree* of any graph can be found by simply negating the weights of all edges and running Prim's or Kruskal's algorithm.
			- The most negative spanning tree in the negated graph is the maximum spanning tree in the original.
	- *Minimum Product Spanning Trees*:
		- Suppose we seek the spanning tree that minimizes the product of edge weights, assuming all edge weights are positive.
		- Since $\lg(a*b)=\lg(a)+\lg(b)$, the minimum spanning tree on a graph whose edge weights are replaced with their logarithms gives the minimum product spanning tree on the original graph.
	- *Minimum Bottleneck Spanning Tree*:
		- Sometimes we seek a spanning tree that minimizes the maximum edge weight over all possible trees.
			- In fact, every minimum spanning tree has this property.
			- The proof follows directly from the correctness of Kruskal's algorithm.
		- Such bottleneck spanning trees have interesting applications when the edge weights are interpreted as costs, capacities, or strengths.
			- A less efficient but conceptually simpler way to solve such problems might be to delete all "heavy" edges from the graph and ask whether the result is still connected.
			- These kinds of tests can bed one with BFS or DFS.

- The minimum spanning tree of a graph is unique if all *m* edge weights in the graph are distinct.
	- Otherwise, the order in which Prim's/Kruskal's algorithm breaks ties determines which minimum spanning tree is returned.

- There are two important variants of a minimum spanning tree that are *not* solvable with the techniques presented in this section:
	- *Steiner Tree*:
		- Suppose we want to wire a bunch of houses together, but have the freedom to add extra intermediate vertices to serve as a shared junction.
		- This problem is known as a *minimum Steiner tree*, and is discussed in Section 19.10.
	- *Low-degree Spanning Tree*:
		- Alternatively, what if we want to find the minimum spanning tree where the highest degree of a node in the tree is small?
		- The lowest max-degree tree possible would be a simple path, consisting of $n-2$ nodes of degree 2 and two endpoints of degree 1.
		- Such a path that visits every vertex once is called a *Hamiltonian path*, and is discussed in Section 19.5.